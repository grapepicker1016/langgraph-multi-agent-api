{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install sqlalchemy==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# llm = ChatOpenAI(base_url='http://localhost:11434/v1', api_key='ollama', model='mistral:7b-instruct-v0.2-q8_0', temperature=0.5)\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "llm = OllamaFunctions(model='mistral:7b-instruct-v0.2-q6_K')\n",
    "# llm = OllamaFunctions(model='calebfahlgren/natural-functions:Q8_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from langsmith import trace\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "\n",
    "@tool\n",
    "def scrape_webpages(urls: List[str]) -> str:\n",
    "    \"\"\"Use requests and bs4 to scrape the provided web pages for detailed information.\"\"\"\n",
    "    loader = WebBaseLoader(urls)\n",
    "    docs = loader.load()\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document name=\"{doc.metadata.get(\"title\", \"\")}\">\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document writing team tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Dict, Optional\n",
    "\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "_TEMP_DIRECTORY = TemporaryDirectory()\n",
    "WORKING_DIRECTORY = Path(_TEMP_DIRECTORY.name)\n",
    "\n",
    "\n",
    "@tool\n",
    "def create_outline(\n",
    "    points: Annotated[List[str], \"List of main points or sections.\"],\n",
    "    file_name: Annotated[str, \"File path to save the outline.\"],\n",
    ") -> Annotated[str, \"Path of the saved outline file.\"]:\n",
    "    \"\"\"Create and save an outline.\"\"\"\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "        for i, point in enumerate(points):\n",
    "            file.write(f\"{i + 1}. {point}\\n\")\n",
    "    return f\"Outline saved to {file_name}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def read_document(\n",
    "    file_name: Annotated[str, \"File path to save the document.\"],\n",
    "    start: Annotated[Optional[int], \"The start line. Default is 0\"] = None,\n",
    "    end: Annotated[Optional[int], \"The end line. Default is None\"] = None,\n",
    ") -> str:\n",
    "    \"\"\"Read the specified document.\"\"\"\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    if start is not None:\n",
    "        start = 0\n",
    "    return \"\\n\".join(lines[start:end])\n",
    "\n",
    "\n",
    "@tool\n",
    "def write_document(\n",
    "    content: Annotated[str, \"Text content to be written into the document.\"],\n",
    "    file_name: Annotated[str, \"File path to save the document.\"],\n",
    ") -> Annotated[str, \"Path of the saved document file.\"]:\n",
    "    \"\"\"Create and save a text document.\"\"\"\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "        file.write(content)\n",
    "    return f\"Document saved to {file_name}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def edit_document(\n",
    "    file_name: Annotated[str, \"Path of the document to be edited.\"],\n",
    "    inserts: Annotated[\n",
    "        Dict[int, str],\n",
    "        \"Dictionary where key is the line number (1-indexed) and value is the text to be inserted at that line.\",\n",
    "    ],\n",
    ") -> Annotated[str, \"Path of the edited document file.\"]:\n",
    "    \"\"\"Edit a document by inserting text at specific line numbers.\"\"\"\n",
    "\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    sorted_inserts = sorted(inserts.items())\n",
    "\n",
    "    for line_number, text in sorted_inserts:\n",
    "        if 1 <= line_number <= len(lines) + 1:\n",
    "            lines.insert(line_number - 1, text + \"\\n\")\n",
    "        else:\n",
    "            return f\"Error: Line number {line_number} is out of range.\"\n",
    "\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "    return f\"Document edited and saved to {file_name}\"\n",
    "\n",
    "\n",
    "# Warning: This executes code locally, which can be unsafe when not sandboxed\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "\n",
    "@tool\n",
    "def python_repl(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"]\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    return f\"Succesfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, List, Optional, TypedDict, Union\n",
    "\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.tools import BaseTool\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "\n",
    "def create_agent(\n",
    "    llm: ChatOllama,\n",
    "    tools: list,\n",
    "    system_prompt: str,\n",
    ") -> str:\n",
    "    \"\"\"Create a function-calling agent and add it to the graph.\"\"\"\n",
    "    system_prompt += \"\\nWork autonomously according to your specialty, using the tools available to you.\"\n",
    "    \" Do not ask for clarification.\"\n",
    "    \" Your other team members (and other teams) will collaborate with you with their own specialties.\"\n",
    "    \" You are chosen for a reason! You are one of the following team members: {team_members}.\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                system_prompt,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    return executor\n",
    "\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}\n",
    "\n",
    "\n",
    "def create_team_supervisor(llm: ChatOllama, system_prompt, members) -> str:\n",
    "    \"\"\"An LLM-based router.\"\"\"\n",
    "    options = [\"FINISH\"] + members\n",
    "    function_def = {\n",
    "        \"name\": \"route\",\n",
    "        \"description\": \"Select the next role.\",\n",
    "        \"parameters\": {\n",
    "            \"title\": \"routeSchema\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"next\": {\n",
    "                    \"title\": \"Next\",\n",
    "                    \"anyOf\": [\n",
    "                        {\"enum\": options},\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"next\"],\n",
    "        },\n",
    "    }\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"system\",\n",
    "                \"Given the conversation above, who should act next?\"\n",
    "                \" Or should we FINISH? Select one of: {options}\",\n",
    "            ),\n",
    "        ]\n",
    "    ).partial(options=str(options), team_members=\", \".join(members))\n",
    "    return (\n",
    "        prompt\n",
    "        | llm.bind(functions=[function_def], function_call={\"name\": \"route\"})\n",
    "        # | JsonOutputFunctionsParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Agent Teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "import functools\n",
    "\n",
    "\n",
    "# Research team graph state\n",
    "class ResearchTeamState(TypedDict):\n",
    "    # A message is added after each team member finishes\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    # The team members are tracked so they are aware of\n",
    "    # the others' skill-sets\n",
    "    team_members: List[str]\n",
    "    # Used to route work. The supervisor calls a function\n",
    "    # that will update this every time it makes a decision\n",
    "    next: str\n",
    "\n",
    "\n",
    "llm = OllamaFunctions(model='mistral:7b-instruct-v0.2-q6_K')\n",
    "\n",
    "search_agent = create_agent(\n",
    "    llm,\n",
    "    [tavily_tool],\n",
    "    \"You are a research assistant who can search for up-to-date info using the tavily search engine.\",\n",
    ")\n",
    "search_node = functools.partial(agent_node, agent=search_agent, name=\"Search\")\n",
    "\n",
    "research_agent = create_agent(\n",
    "    llm,\n",
    "    [scrape_webpages],\n",
    "    \"You are a research assistant who can scrape specified urls for more detailed information using the scrape_webpages function.\",\n",
    ")\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name=\"Web Scraper\")\n",
    "\n",
    "supervisor_agent = create_team_supervisor(\n",
    "    llm,\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  Search, Web Scraper. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\",\n",
    "    [\"Search\", \"Web Scraper\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_graph = StateGraph(ResearchTeamState)\n",
    "research_graph.add_node(\"Search\", search_node)\n",
    "research_graph.add_node(\"Web Scraper\", research_node)\n",
    "research_graph.add_node(\"supervisor\", supervisor_agent)\n",
    "\n",
    "# Define the control flow\n",
    "research_graph.add_edge(\"Search\", \"supervisor\")\n",
    "research_graph.add_edge(\"Web Scraper\", \"supervisor\")\n",
    "research_graph.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda x: x[\"next\"],\n",
    "    {\"Search\": \"Search\", \"Web Scraper\": \"Web Scraper\", \"FINISH\": END},\n",
    ")\n",
    "\n",
    "\n",
    "research_graph.set_entry_point(\"supervisor\")\n",
    "chain = research_graph.compile()\n",
    "\n",
    "\n",
    "# The following functions interoperate between the top level graph state\n",
    "# and the state of the research sub-graph\n",
    "# this makes it so that the states of each graph don't get intermixed\n",
    "def enter_chain(message: str):\n",
    "    results = {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "research_chain = enter_chain | chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Writing Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Document writing team graph state\n",
    "class DocWritingState(TypedDict):\n",
    "    # This tracks the team's conversation internally\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    # This provides each worker with context on the others' skill sets\n",
    "    team_members: str\n",
    "    # This is how the supervisor tells langgraph who to work next\n",
    "    next: str\n",
    "    # This tracks the shared directory state\n",
    "    current_files: str\n",
    "\n",
    "\n",
    "# This will be run before each worker agent begins work\n",
    "# It makes it so they are more aware of the current state\n",
    "# of the working directory.\n",
    "def prelude(state):\n",
    "    written_files = []\n",
    "    if not WORKING_DIRECTORY.exists():\n",
    "        WORKING_DIRECTORY.mkdir()\n",
    "    try:\n",
    "        written_files = [\n",
    "            f.relative_to(WORKING_DIRECTORY) for f in WORKING_DIRECTORY.rglob(\"*\")\n",
    "        ]\n",
    "    except:\n",
    "        pass\n",
    "    if not written_files:\n",
    "        return {**state, \"current_files\": \"No files written.\"}\n",
    "    return {\n",
    "        **state,\n",
    "        \"current_files\": \"\\nBelow are files your team has written to the directory:\\n\"\n",
    "        + \"\\n\".join([f\" - {f}\" for f in written_files]),\n",
    "    }\n",
    "\n",
    "\n",
    "llm = OllamaFunctions(model='mistral:7b-instruct-v0.2-q6_K')\n",
    "\n",
    "doc_writer_agent = create_agent(\n",
    "    llm,\n",
    "    [write_document, edit_document, read_document],\n",
    "    \"You are an expert writing a research document.\\n\"\n",
    "    # The {current_files} value is populated automatically by the graph state\n",
    "    \"Below are files currently in your directory:\\n{current_files}\",\n",
    ")\n",
    "# Injects current directory working state before each call\n",
    "context_aware_doc_writer_agent = prelude | doc_writer_agent\n",
    "doc_writing_node = functools.partial(\n",
    "    agent_node, agent=context_aware_doc_writer_agent, name=\"Doc Writer\"\n",
    ")\n",
    "\n",
    "note_taking_agent = create_agent(\n",
    "    llm,\n",
    "    [create_outline, read_document],\n",
    "    \"You are an expert senior researcher tasked with writing a paper outline and\"\n",
    "    \" taking notes to craft a perfect paper.{current_files}\",\n",
    ")\n",
    "context_aware_note_taking_agent = prelude | note_taking_agent\n",
    "note_taking_node = functools.partial(\n",
    "    agent_node, agent=context_aware_note_taking_agent, name=\"Note Taker\"\n",
    ")\n",
    "\n",
    "chart_generating_agent = create_agent(\n",
    "    llm,\n",
    "    [read_document, python_repl],\n",
    "    \"You are a data viz expert tasked with generating charts for a research project.\"\n",
    "    \"{current_files}\",\n",
    ")\n",
    "context_aware_chart_generating_agent = prelude | chart_generating_agent\n",
    "chart_generating_node = functools.partial(\n",
    "    agent_node, agent=context_aware_note_taking_agent, name=\"Chart Generator\"\n",
    ")\n",
    "\n",
    "doc_writing_supervisor = create_team_supervisor(\n",
    "    llm,\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {team_members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\",\n",
    "    [\"Doc Writer\", \"Note Taker\", \"Chart Generator\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph here:\n",
    "# Note that we have unrolled the loop for the sake of this doc\n",
    "authoring_graph = StateGraph(DocWritingState)\n",
    "authoring_graph.add_node(\"Doc Writer\", doc_writing_node)\n",
    "authoring_graph.add_node(\"Note Taker\", note_taking_node)\n",
    "authoring_graph.add_node(\"Chart Generator\", chart_generating_node)\n",
    "authoring_graph.add_node(\"supervisor\", doc_writing_supervisor)\n",
    "\n",
    "# Add the edges that always occur\n",
    "authoring_graph.add_edge(\"Doc Writer\", \"supervisor\")\n",
    "authoring_graph.add_edge(\"Note Taker\", \"supervisor\")\n",
    "authoring_graph.add_edge(\"Chart Generator\", \"supervisor\")\n",
    "\n",
    "# Add the edges where routing applies\n",
    "authoring_graph.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda x: x[\"next\"],\n",
    "    {\n",
    "        \"Doc Writer\": \"Doc Writer\",\n",
    "        \"Note Taker\": \"Note Taker\",\n",
    "        \"Chart Generator\": \"Chart Generator\",\n",
    "        \"FINISH\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "authoring_graph.set_entry_point(\"supervisor\")\n",
    "chain = research_graph.compile()\n",
    "\n",
    "\n",
    "# The following functions interoperate between the top level graph state\n",
    "# and the state of the research sub-graph\n",
    "# this makes it so that the states of each graph don't get intermixed\n",
    "def enter_chain(message: str, members: List[str]):\n",
    "    results = {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "        \"team_members\": \", \".join(members),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "# We re-use the enter/exit functions to wrap the graph\n",
    "authoring_chain = (\n",
    "    functools.partial(enter_chain, members=authoring_graph.nodes)\n",
    "    | authoring_graph.compile()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "\n",
    "\n",
    "llm = OllamaFunctions(model='mistral:7b-instruct-v0.2-q6_K')\n",
    "\n",
    "supervisor_node = create_team_supervisor(\n",
    "    llm,\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following teams: {team_members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\",\n",
    "    [\"Research team\", \"Paper writing team\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-level graph state\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    next: str\n",
    "\n",
    "\n",
    "def get_last_message(state: State) -> str:\n",
    "    return state[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "def join_graph(response: dict):\n",
    "    return {\"messages\": [response[\"messages\"][-1]]}\n",
    "\n",
    "\n",
    "# Define the graph.\n",
    "super_graph = StateGraph(State)\n",
    "# First add the nodes, which will do the work\n",
    "super_graph.add_node(\"Research team\", get_last_message | research_chain | join_graph)\n",
    "super_graph.add_node(\n",
    "    \"Paper writing team\", get_last_message | authoring_chain | join_graph\n",
    ")\n",
    "super_graph.add_node(\"supervisor\", supervisor_node)\n",
    "\n",
    "# Define the graph connections, which controls how the logic\n",
    "# propagates through the program\n",
    "super_graph.add_edge(\"Research team\", \"supervisor\")\n",
    "super_graph.add_edge(\"Paper writing team\", \"supervisor\")\n",
    "super_graph.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda x: x[\"next\"],\n",
    "    {\n",
    "        \"Paper writing team\": \"Paper writing team\",\n",
    "        \"Research team\": \"Research team\",\n",
    "        \"FINISH\": END,\n",
    "    },\n",
    ")\n",
    "super_graph.set_entry_point(\"supervisor\")\n",
    "super_graph = super_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompiledGraph(nodes={'Research team': ChannelInvoke(bound=RunnableLambda(...)\n",
       "| RunnableLambda(get_last_message)\n",
       "| RunnableLambda(enter_chain)\n",
       "| CompiledGraph(nodes={'Search': ChannelInvoke(bound=RunnableLambda(...)\n",
       "  | RunnableLambda(...)\n",
       "  | ChannelWrite(channels=[('Search', None, False), ('messages', RunnableLambda(...), False), ('team_members', RunnableLambda(...), False), ('next', RunnableLambda(...), False)]), config={'tags': []}, channels={None: 'Search:inbox'}, triggers=['Search:inbox']), 'Web Scraper': ChannelInvoke(bound=RunnableLambda(...)\n",
       "  | RunnableLambda(...)\n",
       "  | ChannelWrite(channels=[('Web Scraper', None, False), ('messages', RunnableLambda(...), False), ('team_members', RunnableLambda(...), False), ('next', RunnableLambda(...), False)]), config={'tags': []}, channels={None: 'Web Scraper:inbox'}, triggers=['Web Scraper:inbox']), 'supervisor': ChannelInvoke(bound=RunnableLambda(...)\n",
       "  | ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'options': \"['FINISH', 'Search', 'Web Scraper']\", 'team_members': 'Search, Web Scraper'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a supervisor tasked with managing a conversation between the following workers:  Search, Web Scraper. Given the following user request, respond with the worker to act next. Each worker will perform a task and respond with their results and status. When finished, respond with FINISH.')), MessagesPlaceholder(variable_name='messages'), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['options'], template='Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}'))])\n",
       "  | RunnableBinding(bound=OllamaFunctions(llm=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', format='json'), tool_system_prompt_template='You have access to the following tools:\\n\\n{tools}\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{{\\n  \"tool\": <name of the selected tool>,\\n  \"tool_input\": <parameters for the selected tool, matching the tool\\'s JSON schema>\\n}}\\n'), kwargs={'functions': [{'name': 'route', 'description': 'Select the next role.', 'parameters': {'title': 'routeSchema', 'type': 'object', 'properties': {'next': {'title': 'Next', 'anyOf': [{'enum': ['FINISH', 'Search', 'Web Scraper']}]}}, 'required': ['next']}}], 'function_call': {'name': 'route'}})\n",
       "  | ChannelWrite(channels=[('supervisor', None, False), ('messages', RunnableLambda(...), False), ('team_members', RunnableLambda(...), False), ('next', RunnableLambda(...), False)]), config={'tags': []}, channels={None: 'supervisor:inbox'}, triggers=['supervisor:inbox']), 'Search:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
       "  | ChannelWrite(channels=[('supervisor:inbox', None, False)]), config={'tags': ['langsmith:hidden']}, channels={None: 'Search'}, triggers=['Search']), 'Web Scraper:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
       "  | ChannelWrite(channels=[('supervisor:inbox', None, False)]), config={'tags': ['langsmith:hidden']}, channels={None: 'Web Scraper'}, triggers=['Web Scraper']), 'supervisor:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
       "  | RunnableLambda(runnable), config={'tags': ['langsmith:hidden']}, channels={None: 'supervisor'}, triggers=['supervisor']), '__start__': ChannelInvoke(bound=ChannelWrite(channels=[('__start__', None, False), ('messages', RunnableLambda(...), False), ('team_members', RunnableLambda(...), False), ('next', RunnableLambda(...), False)]), config={'tags': ['langsmith:hidden']}, channels={None: '__start__:inbox'}, triggers=['__start__:inbox']), '__start__:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
       "  | ChannelWrite(channels=[('supervisor:inbox', None, False)]), config={'tags': ['langsmith:hidden']}, channels={None: '__start__'}, triggers=['__start__'])}, channels={'messages': <langgraph.channels.binop.BinaryOperatorAggregate object at 0x7f582ec640d0>, 'team_members': <langgraph.channels.last_value.LastValue object at 0x7f582ec651e0>, 'next': <langgraph.channels.last_value.LastValue object at 0x7f582ec66c20>, 'Search:inbox': <langgraph.channels.any_value.AnyValue object at 0x7f582ec65510>, 'Web Scraper:inbox': <langgraph.channels.any_value.AnyValue object at 0x7f582ec65540>, 'supervisor:inbox': <langgraph.channels.any_value.AnyValue object at 0x7f582ec66d10>, '__start__:inbox': <langgraph.channels.any_value.AnyValue object at 0x7f582ec66a70>, 'Search': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7f582ec66ad0>, 'Web Scraper': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7f582ec66ce0>, 'supervisor': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7f582ec66b90>, '__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7f582ec653f0>, '__end__': <langgraph.channels.last_value.LastValue object at 0x7f582ec678b0>, <ReservedChannels.is_last_step: 'is_last_step'>: <langgraph.channels.last_value.LastValue object at 0x7f582ec64820>}, output='__end__', hidden=['Search:inbox', 'Web Scraper:inbox', 'supervisor:inbox', '__start__', 'messages', 'team_members', 'next'], input='__start__:inbox', graph=<langgraph.graph.state.StateGraph object at 0x7f582ec646a0>)\n",
       "| RunnableLambda(join_graph)\n",
       "| ChannelWrite(channels=[('Research team', None, False), ('messages', RunnableLambda(...), False), ('next', RunnableLambda(...), False)]), config={'tags': []}, channels={None: 'Research team:inbox'}, triggers=['Research team:inbox']), 'Paper writing team': ChannelInvoke(bound=RunnableLambda(...)\n",
       "| RunnableLambda(get_last_message)\n",
       "| RunnableLambda(...)\n",
       "| CompiledGraph(nodes={'Doc Writer': ChannelInvoke(bound=RunnableLambda(...)\n",
       "  | RunnableLambda(...)\n",
       "  | ChannelWrite(channels=[('Doc Writer', None, False), ('messages', RunnableLambda(...), False), ('team_members', RunnableLambda(...), False), ('next', RunnableLambda(...), False), ('current_files', RunnableLambda(...), False)]), config={'tags': []}, channels={None: 'Doc Writer:inbox'}, triggers=['Doc Writer:inbox']), 'Note Taker': ChannelInvoke(bound=RunnableLambda(...)\n",
       "  | RunnableLambda(...)\n",
       "  | ChannelWrite(channels=[('Note Taker', None, False), ('messages', RunnableLambda(...), False), ('team_members', RunnableLambda(...), False), ('next', RunnableLambda(...), False), ('current_files', RunnableLambda(...), False)]), config={'tags': []}, channels={None: 'Note Taker:inbox'}, triggers=['Note Taker:inbox']), 'Chart Generator': ChannelInvoke(bound=RunnableLambda(...)\n",
       "  | RunnableLambda(...)\n",
       "  | ChannelWrite(channels=[('Chart Generator', None, False), ('messages', RunnableLambda(...), False), ('team_members', RunnableLambda(...), False), ('next', RunnableLambda(...), False), ('current_files', RunnableLambda(...), False)]), config={'tags': []}, channels={None: 'Chart Generator:inbox'}, triggers=['Chart Generator:inbox']), 'supervisor': ChannelInvoke(bound=RunnableLambda(...)\n",
       "  | ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'options': \"['FINISH', 'Doc Writer', 'Note Taker', 'Chart Generator']\", 'team_members': 'Doc Writer, Note Taker, Chart Generator'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['team_members'], template='You are a supervisor tasked with managing a conversation between the following workers:  {team_members}. Given the following user request, respond with the worker to act next. Each worker will perform a task and respond with their results and status. When finished, respond with FINISH.')), MessagesPlaceholder(variable_name='messages'), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['options'], template='Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}'))])\n",
       "  | RunnableBinding(bound=OllamaFunctions(llm=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', format='json'), tool_system_prompt_template='You have access to the following tools:\\n\\n{tools}\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{{\\n  \"tool\": <name of the selected tool>,\\n  \"tool_input\": <parameters for the selected tool, matching the tool\\'s JSON schema>\\n}}\\n'), kwargs={'functions': [{'name': 'route', 'description': 'Select the next role.', 'parameters': {'title': 'routeSchema', 'type': 'object', 'properties': {'next': {'title': 'Next', 'anyOf': [{'enum': ['FINISH', 'Doc Writer', 'Note Taker', 'Chart Generator']}]}}, 'required': ['next']}}], 'function_call': {'name': 'route'}})\n",
       "  | ChannelWrite(channels=[('supervisor', None, False), ('messages', RunnableLambda(...), False), ('team_members', RunnableLambda(...), False), ('next', RunnableLambda(...), False), ('current_files', RunnableLambda(...), False)]), config={'tags': []}, channels={None: 'supervisor:inbox'}, triggers=['supervisor:inbox']), 'Doc Writer:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
       "  | ChannelWrite(channels=[('supervisor:inbox', None, False)]), config={'tags': ['langsmith:hidden']}, channels={None: 'Doc Writer'}, triggers=['Doc Writer']), 'Note Taker:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
       "  | ChannelWrite(channels=[('supervisor:inbox', None, False)]), config={'tags': ['langsmith:hidden']}, channels={None: 'Note Taker'}, triggers=['Note Taker']), 'Chart Generator:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
       "  | ChannelWrite(channels=[('supervisor:inbox', None, False)]), config={'tags': ['langsmith:hidden']}, channels={None: 'Chart Generator'}, triggers=['Chart Generator']), 'supervisor:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
       "  | RunnableLambda(runnable), config={'tags': ['langsmith:hidden']}, channels={None: 'supervisor'}, triggers=['supervisor']), '__start__': ChannelInvoke(bound=ChannelWrite(channels=[('__start__', None, False), ('messages', RunnableLambda(...), False), ('team_members', RunnableLambda(...), False), ('next', RunnableLambda(...), False), ('current_files', RunnableLambda(...), False)]), config={'tags': ['langsmith:hidden']}, channels={None: '__start__:inbox'}, triggers=['__start__:inbox']), '__start__:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
       "  | ChannelWrite(channels=[('supervisor:inbox', None, False)]), config={'tags': ['langsmith:hidden']}, channels={None: '__start__'}, triggers=['__start__'])}, channels={'messages': <langgraph.channels.binop.BinaryOperatorAggregate object at 0x7f582ed9bd90>, 'team_members': <langgraph.channels.last_value.LastValue object at 0x7f582ee800a0>, 'next': <langgraph.channels.last_value.LastValue object at 0x7f582ee80ca0>, 'current_files': <langgraph.channels.last_value.LastValue object at 0x7f582ee812d0>, 'Doc Writer:inbox': <langgraph.channels.any_value.AnyValue object at 0x7f582ec66d40>, 'Note Taker:inbox': <langgraph.channels.any_value.AnyValue object at 0x7f582ec64130>, 'Chart Generator:inbox': <langgraph.channels.any_value.AnyValue object at 0x7f582ec66380>, 'supervisor:inbox': <langgraph.channels.any_value.AnyValue object at 0x7f582ec663b0>, '__start__:inbox': <langgraph.channels.any_value.AnyValue object at 0x7f582ec67b50>, 'Doc Writer': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7f582ec66770>, 'Note Taker': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7f582ec65e70>, 'Chart Generator': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7f582ec659f0>, 'supervisor': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7f582ec66d70>, '__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7f582ec66cb0>, '__end__': <langgraph.channels.last_value.LastValue object at 0x7f582ec67010>, <ReservedChannels.is_last_step: 'is_last_step'>: <langgraph.channels.last_value.LastValue object at 0x7f582ec66c80>}, output='__end__', hidden=['Doc Writer:inbox', 'Note Taker:inbox', 'Chart Generator:inbox', 'supervisor:inbox', '__start__', 'messages', 'team_members', 'next', 'current_files'], input='__start__:inbox', graph=<langgraph.graph.state.StateGraph object at 0x7f582ee80b20>)\n",
       "| RunnableLambda(join_graph)\n",
       "| ChannelWrite(channels=[('Paper writing team', None, False), ('messages', RunnableLambda(...), False), ('next', RunnableLambda(...), False)]), config={'tags': []}, channels={None: 'Paper writing team:inbox'}, triggers=['Paper writing team:inbox']), 'supervisor': ChannelInvoke(bound=RunnableLambda(...)\n",
       "| ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'options': \"['FINISH', 'Research team', 'Paper writing team']\", 'team_members': 'Research team, Paper writing team'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['team_members'], template='You are a supervisor tasked with managing a conversation between the following teams: {team_members}. Given the following user request, respond with the worker to act next. Each worker will perform a task and respond with their results and status. When finished, respond with FINISH.')), MessagesPlaceholder(variable_name='messages'), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['options'], template='Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}'))])\n",
       "| RunnableBinding(bound=OllamaFunctions(llm=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', format='json'), tool_system_prompt_template='You have access to the following tools:\\n\\n{tools}\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{{\\n  \"tool\": <name of the selected tool>,\\n  \"tool_input\": <parameters for the selected tool, matching the tool\\'s JSON schema>\\n}}\\n'), kwargs={'functions': [{'name': 'route', 'description': 'Select the next role.', 'parameters': {'title': 'routeSchema', 'type': 'object', 'properties': {'next': {'title': 'Next', 'anyOf': [{'enum': ['FINISH', 'Research team', 'Paper writing team']}]}}, 'required': ['next']}}], 'function_call': {'name': 'route'}})\n",
       "| ChannelWrite(channels=[('supervisor', None, False), ('messages', RunnableLambda(...), False), ('next', RunnableLambda(...), False)]), config={'tags': []}, channels={None: 'supervisor:inbox'}, triggers=['supervisor:inbox']), 'Research team:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
       "| ChannelWrite(channels=[('supervisor:inbox', None, False)]), config={'tags': ['langsmith:hidden']}, channels={None: 'Research team'}, triggers=['Research team']), 'Paper writing team:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
       "| ChannelWrite(channels=[('supervisor:inbox', None, False)]), config={'tags': ['langsmith:hidden']}, channels={None: 'Paper writing team'}, triggers=['Paper writing team']), 'supervisor:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
       "| RunnableLambda(runnable), config={'tags': ['langsmith:hidden']}, channels={None: 'supervisor'}, triggers=['supervisor']), '__start__': ChannelInvoke(bound=ChannelWrite(channels=[('__start__', None, False), ('messages', RunnableLambda(...), False), ('next', RunnableLambda(...), False)]), config={'tags': ['langsmith:hidden']}, channels={None: '__start__:inbox'}, triggers=['__start__:inbox']), '__start__:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
       "| ChannelWrite(channels=[('supervisor:inbox', None, False)]), config={'tags': ['langsmith:hidden']}, channels={None: '__start__'}, triggers=['__start__'])}, channels={'messages': <langgraph.channels.binop.BinaryOperatorAggregate object at 0x7f582ec67c40>, 'next': <langgraph.channels.last_value.LastValue object at 0x7f582ec64400>, 'Research team:inbox': <langgraph.channels.any_value.AnyValue object at 0x7f582ec65750>, 'Paper writing team:inbox': <langgraph.channels.any_value.AnyValue object at 0x7f582ec64d60>, 'supervisor:inbox': <langgraph.channels.any_value.AnyValue object at 0x7f582ec64d90>, '__start__:inbox': <langgraph.channels.any_value.AnyValue object at 0x7f582ec67d30>, 'Research team': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7f582ec64610>, 'Paper writing team': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7f582ec645e0>, 'supervisor': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7f582ec67e50>, '__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7f582ec67e80>, '__end__': <langgraph.channels.last_value.LastValue object at 0x7f582ec67070>, <ReservedChannels.is_last_step: 'is_last_step'>: <langgraph.channels.last_value.LastValue object at 0x7f582ec67fd0>}, output='__end__', hidden=['Research team:inbox', 'Paper writing team:inbox', 'supervisor:inbox', '__start__', 'messages', 'next'], input='__start__:inbox', graph=<langgraph.graph.state.StateGraph object at 0x7f582ec64a00>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "super_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug\n",
    "set_debug(True)\n",
    "from langchain.globals import set_verbose\n",
    "# set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/step]\u001b[0m \u001b[1mStarting step 0 with 1 task. Next tasks:\n",
      "\u001b[0m- __start__({'messages': [HumanMessage(content='Write a brief research report on the Recent trends in Generative AI. Include a chart.')]})\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 2:chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 2:chain:__start__] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 3:chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 3:chain:__start__] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 4:chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 4:chain:__start__] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[pregel/checkpoint]\u001b[0m \u001b[1mFinishing step 0. Channel values:\n",
      "\u001b[0m{'__start__': {...},\n",
      " '__start__:inbox': {...},\n",
      " <ReservedChannels.is_last_step: 'is_last_step'>: False,\n",
      " 'messages': [...]}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/step]\u001b[0m \u001b[1mStarting step 1 with 1 task. Next tasks:\n",
      "\u001b[0m- __start__:edges({'messages': [HumanMessage(content='Write a brief research report on the Recent trends in Generative AI. Include a chart.')]})\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 5:chain:__start__:edges] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 5:chain:__start__:edges > 6:chain:ChannelRead<['messages', 'next']>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 5:chain:__start__:edges > 6:chain:ChannelRead<['messages', 'next']>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 5:chain:__start__:edges > 7:chain:ChannelWrite<supervisor:inbox>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 5:chain:__start__:edges > 7:chain:ChannelWrite<supervisor:inbox>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 5:chain:__start__:edges] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[pregel/checkpoint]\u001b[0m \u001b[1mFinishing step 1. Channel values:\n",
      "\u001b[0m{'__start__:inbox': {...},\n",
      " <ReservedChannels.is_last_step: 'is_last_step'>: False,\n",
      " 'messages': [...],\n",
      " 'supervisor:inbox': {...}}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/step]\u001b[0m \u001b[1mStarting step 2 with 1 task. Next tasks:\n",
      "\u001b[0m- supervisor({'messages': [HumanMessage(content='Write a brief research report on the Recent trends in Generative AI. Include a chart.')],\n",
      " 'next': None})\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 8:chain:supervisor] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 8:chain:supervisor > 9:chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 8:chain:supervisor > 9:chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 8:chain:supervisor > 10:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 8:chain:supervisor > 10:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 8:chain:supervisor > 11:llm:OllamaFunctions] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a supervisor tasked with managing a conversation between the following teams: Research team, Paper writing team. Given the following user request, respond with the worker to act next. Each worker will perform a task and respond with their results and status. When finished, respond with FINISH.\\nHuman: Write a brief research report on the Recent trends in Generative AI. Include a chart.\\nSystem: Given the conversation above, who should act next? Or should we FINISH? Select one of: ['FINISH', 'Research team', 'Paper writing team']\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 8:chain:supervisor > 12:llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You have access to the following tools:\\n\\n[\\n  {\\n    \\\"name\\\": \\\"route\\\",\\n    \\\"description\\\": \\\"Select the next role.\\\",\\n    \\\"parameters\\\": {\\n      \\\"title\\\": \\\"routeSchema\\\",\\n      \\\"type\\\": \\\"object\\\",\\n      \\\"properties\\\": {\\n        \\\"next\\\": {\\n          \\\"title\\\": \\\"Next\\\",\\n          \\\"anyOf\\\": [\\n            {\\n              \\\"enum\\\": [\\n                \\\"FINISH\\\",\\n                \\\"Research team\\\",\\n                \\\"Paper writing team\\\"\\n              ]\\n            }\\n          ]\\n        }\\n      },\\n      \\\"required\\\": [\\n        \\\"next\\\"\\n      ]\\n    }\\n  }\\n]\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{\\n  \\\"tool\\\": <name of the selected tool>,\\n  \\\"tool_input\\\": <parameters for the selected tool, matching the tool's JSON schema>\\n}\\n\\nSystem: You are a supervisor tasked with managing a conversation between the following teams: Research team, Paper writing team. Given the following user request, respond with the worker to act next. Each worker will perform a task and respond with their results and status. When finished, respond with FINISH.\\nHuman: Write a brief research report on the Recent trends in Generative AI. Include a chart.\\nSystem: Given the conversation above, who should act next? Or should we FINISH? Select one of: ['FINISH', 'Research team', 'Paper writing team']\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 8:chain:supervisor > 12:llm:ChatOllama] [718ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\n  \\\"tool\\\": \\\"Research team\\\",\\n  \\\"tool_input\\\": {\\n    \\\"next\\\": \\\"Research\\\"\\n  }\\n}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"mistral:7b-instruct-v0.2-q6_K\",\n",
      "          \"created_at\": \"2024-03-19T05:40:03.240043366Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 715614585,\n",
      "          \"load_duration\": 232516,\n",
      "          \"prompt_eval_duration\": 46243000,\n",
      "          \"eval_count\": 34,\n",
      "          \"eval_duration\": 667073000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\n  \\\"tool\\\": \\\"Research team\\\",\\n  \\\"tool_input\\\": {\\n    \\\"next\\\": \\\"Research\\\"\\n  }\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[31;1m\u001b[1;3m[llm/error]\u001b[0m \u001b[1m[1:chain:LangGraph > 8:chain:supervisor > 11:llm:OllamaFunctions] [720ms] LLM run errored with error:\n",
      "\u001b[0m\"ValueError('Failed to parse a function call from mistral:7b-instruct-v0.2-q6_K output: {\\\\n  \\\"tool\\\": \\\"Research team\\\",\\\\n  \\\"tool_input\\\": {\\\\n    \\\"next\\\": \\\"Research\\\"\\\\n  }\\\\n}')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py\\\", line 110, in _generate\\n    raise ValueError(\\n\\n\\nValueError: Failed to parse a function call from mistral:7b-instruct-v0.2-q6_K output: {\\n  \\\"tool\\\": \\\"Research team\\\",\\n  \\\"tool_input\\\": {\\n    \\\"next\\\": \\\"Research\\\"\\n  }\\n}\"\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:LangGraph > 8:chain:supervisor] [723ms] Chain run errored with error:\n",
      "\u001b[0m\"ValueError('Failed to parse a function call from mistral:7b-instruct-v0.2-q6_K output: {\\\\n  \\\"tool\\\": \\\"Research team\\\",\\\\n  \\\"tool_input\\\": {\\\\n    \\\"next\\\": \\\"Research\\\"\\\\n  }\\\\n}')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 4069, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 166, in invoke\\n    self.generate_prompt(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 544, in generate_prompt\\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 408, in generate\\n    raise e\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py\\\", line 110, in _generate\\n    raise ValueError(\\n\\n\\nValueError: Failed to parse a function call from mistral:7b-instruct-v0.2-q6_K output: {\\n  \\\"tool\\\": \\\"Research team\\\",\\n  \\\"tool_input\\\": {\\n    \\\"next\\\": \\\"Research\\\"\\n  }\\n}\"\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:LangGraph] [734ms] Chain run errored with error:\n",
      "\u001b[0m\"ValueError('Failed to parse a function call from mistral:7b-instruct-v0.2-q6_K output: {\\\\n  \\\"tool\\\": \\\"Research team\\\",\\\\n  \\\"tool_input\\\": {\\\\n    \\\"next\\\": \\\"Research\\\"\\\\n  }\\\\n}')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 1513, in _transform_stream_with_config\\n    chunk: Output = context.run(next, iterator)  # type: ignore\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py\\\", line 355, in _transform\\n    _interrupt_or_proceed(done, inflight, step)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py\\\", line 698, in _interrupt_or_proceed\\n    raise exc\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/concurrent/futures/thread.py\\\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 4069, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 4069, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 166, in invoke\\n    self.generate_prompt(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 544, in generate_prompt\\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 408, in generate\\n    raise e\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py\\\", line 110, in _generate\\n    raise ValueError(\\n\\n\\nValueError: Failed to parse a function call from mistral:7b-instruct-v0.2-q6_K output: {\\n  \\\"tool\\\": \\\"Research team\\\",\\n  \\\"tool_input\\\": {\\n    \\\"next\\\": \\\"Research\\\"\\n  }\\n}\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to parse a function call from mistral:7b-instruct-v0.2-q6_K output: {\n  \"tool\": \"Research team\",\n  \"tool_input\": {\n    \"next\": \"Research\"\n  }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m super_graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m      2\u001b[0m     {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m      4\u001b[0m             HumanMessage(\n\u001b[1;32m      5\u001b[0m                 content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a brief research report on the Recent trends in Generative AI. Include a chart.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m             )\n\u001b[1;32m      7\u001b[0m         ],\n\u001b[1;32m      8\u001b[0m     },\n\u001b[1;32m      9\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m150\u001b[39m},\n\u001b[1;32m     10\u001b[0m ):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__end__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m s:\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(s)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py:615\u001b[0m, in \u001b[0;36mPregel.transform\u001b[0;34m(self, input, config, output_keys, input_keys, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    614\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]]:\n\u001b[0;32m--> 615\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[1;32m    616\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[1;32m    618\u001b[0m         config,\n\u001b[1;32m    619\u001b[0m         output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m    620\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    622\u001b[0m     ):\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:1513\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1513\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m   1515\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py:355\u001b[0m, in \u001b[0;36mPregel._transform\u001b[0;34m(self, input, run_manager, config, input_keys, output_keys, interrupt)\u001b[0m\n\u001b[1;32m    348\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    349\u001b[0m     futures,\n\u001b[1;32m    350\u001b[0m     return_when\u001b[38;5;241m=\u001b[39mconcurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m    351\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m    352\u001b[0m )\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# interrupt on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m \u001b[43m_interrupt_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# apply writes to channels\u001b[39;00m\n\u001b[1;32m    358\u001b[0m _apply_writes(\n\u001b[1;32m    359\u001b[0m     checkpoint, channels, pending_writes, config, step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    360\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py:698\u001b[0m, in \u001b[0;36m_interrupt_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m    696\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m--> 698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;66;03m# TODO this is where retry of an entire step would happen\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:4069\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4065\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4066\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4067\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:4069\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4065\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4066\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4067\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:166\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    163\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    165\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    175\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py:110\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m         called_tool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m    107\u001b[0m             (fn \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m functions \u001b[38;5;28;01mif\u001b[39;00m fn[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m called_tool_name), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    108\u001b[0m         )\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m called_tool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    111\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to parse a function call from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124moutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchat_generation_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m             )\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m called_tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m DEFAULT_RESPONSE_FUNCTION[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    115\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(\n\u001b[1;32m    116\u001b[0m                 generations\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    117\u001b[0m                     ChatGeneration(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 ]\n\u001b[1;32m    123\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to parse a function call from mistral:7b-instruct-v0.2-q6_K output: {\n  \"tool\": \"Research team\",\n  \"tool_input\": {\n    \"next\": \"Research\"\n  }\n}"
     ]
    }
   ],
   "source": [
    "for s in super_graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Write a brief research report on the Recent trends in Generative AI. Include a chart.\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    {\"recursion_limit\": 150},\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Ollama call failed with status code 400. Details: {\"error\":\"exception server shutting down\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m super_graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m      2\u001b[0m     {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m      4\u001b[0m             HumanMessage(\n\u001b[1;32m      5\u001b[0m                 content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a brief research report on the Recent trends in Generative AI. Include a chart.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m             )\n\u001b[1;32m      7\u001b[0m         ],\n\u001b[1;32m      8\u001b[0m     },\n\u001b[1;32m      9\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m150\u001b[39m},\n\u001b[1;32m     10\u001b[0m ):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__end__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m s:\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(s)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py:615\u001b[0m, in \u001b[0;36mPregel.transform\u001b[0;34m(self, input, config, output_keys, input_keys, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    614\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]]:\n\u001b[0;32m--> 615\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[1;32m    616\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[1;32m    618\u001b[0m         config,\n\u001b[1;32m    619\u001b[0m         output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m    620\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    622\u001b[0m     ):\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:1513\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1513\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m   1515\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py:355\u001b[0m, in \u001b[0;36mPregel._transform\u001b[0;34m(self, input, run_manager, config, input_keys, output_keys, interrupt)\u001b[0m\n\u001b[1;32m    348\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    349\u001b[0m     futures,\n\u001b[1;32m    350\u001b[0m     return_when\u001b[38;5;241m=\u001b[39mconcurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m    351\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m    352\u001b[0m )\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# interrupt on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m \u001b[43m_interrupt_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# apply writes to channels\u001b[39;00m\n\u001b[1;32m    358\u001b[0m _apply_writes(\n\u001b[1;32m    359\u001b[0m     checkpoint, channels, pending_writes, config, step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    360\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py:698\u001b[0m, in \u001b[0;36m_interrupt_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m    696\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m--> 698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;66;03m# TODO this is where retry of an entire step would happen\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:4069\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4065\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4066\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4067\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:4069\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4065\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4066\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4067\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:166\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    163\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    165\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    175\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py:92\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 92\u001b[0m response_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m chat_generation_content \u001b[38;5;241m=\u001b[39m response_message\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_generation_content, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:747\u001b[0m, in \u001b[0;36mBaseChatModel.predict_messages\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    746\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[0;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:691\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1.7\u001b[39m\u001b[38;5;124m\"\u001b[39m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvoke\u001b[39m\u001b[38;5;124m\"\u001b[39m, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    690\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 691\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py:257\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    235\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    239\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    240\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m            ])\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    265\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext),\n\u001b[1;32m    266\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mgeneration_info,\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[chat_generation])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py:188\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    186\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    187\u001b[0m     final_chunk: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[1;32m    190\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _chat_stream_response_to_chat_generation_chunk(stream_resp)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py:161\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_chat_stream\u001b[39m(\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    154\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m    155\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    157\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    158\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_messages_to_ollama_messages(messages),\n\u001b[1;32m    160\u001b[0m     }\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/llms/ollama.py:240\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[0;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m         optional_detail \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    241\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama call failed with status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Details: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptional_detail\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m         )\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines(decode_unicode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: Ollama call failed with status code 400. Details: {\"error\":\"exception server shutting down\"}"
     ]
    }
   ],
   "source": [
    "for s in super_graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Write a brief research report on the Recent trends in Generative AI. Include a chart.\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    {\"recursion_limit\": 150},\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"---\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueError: Failed to parse a function call from mistral:7b-instruct-v0.2-q8_0 output: {\n",
    "  \"tool\": \"Research team\",\n",
    "  \"tool_input\": {\n",
    "    \"next\": \"Research\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = {\n",
    "  \"generations\": [\n",
    "    [\n",
    "      {\n",
    "        \"text\": \"{\\n  \\\"tool\\\": \\\"Research team\\\",\\n  \\\"tool_input\\\": {\\n    \\\"next\\\": \\\"Research\\\"\\n  }\\n}\",\n",
    "        \"generation_info\": {\n",
    "          \"model\": \"mistral:7b-instruct-v0.2-q8_0\",\n",
    "          \"created_at\": \"2024-03-15T13:17:14.712580891Z\",\n",
    "          \"message\": {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"\"\n",
    "          },\n",
    "          \"done\": 'true',\n",
    "          \"total_duration\": 6579789112,\n",
    "          \"load_duration\": 3031967062,\n",
    "          \"prompt_eval_count\": 353,\n",
    "          \"prompt_eval_duration\": 825254000,\n",
    "          \"eval_count\": 34,\n",
    "          \"eval_duration\": 2720940000\n",
    "        },\n",
    "        \"type\": \"ChatGeneration\",\n",
    "        \"message\": {\n",
    "          \"lc\": 1,\n",
    "          \"type\": \"constructor\",\n",
    "          \"id\": [\n",
    "            \"langchain\",\n",
    "            \"schema\",\n",
    "            \"messages\",\n",
    "            \"AIMessage\"\n",
    "          ],\n",
    "          \"kwargs\": {\n",
    "            \"content\": \"{\\n  \\\"tool\\\": \\\"Research team\\\",\\n  \\\"tool_input\\\": {\\n    \\\"next\\\": \\\"Research\\\"\\n  }\\n}\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  ],\n",
    "  \"llm_output\": 'null',\n",
    "  \"run\": 'null'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(out)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parser' is not defined"
     ]
    }
   ],
   "source": [
    "parser.invoke(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RunnableSequence' object has no attribute 'format'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msupervisor_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m(member\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResearch team\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPaper writing team\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RunnableSequence' object has no attribute 'format'"
     ]
    }
   ],
   "source": [
    "# supervisor_node.format(member=['Research team', 'Paper writing team'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# llm = ChatOpenAI(base_url='http://localhost:11434/v1', api_key='ollama', model='phi:2.7b-chat-v2-q8_0', temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.invoke('mine was bit bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super_graph.get_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "\n",
    "# Image(super_graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph(nodes={'__start__': Node(id='__start__', data=<class 'pydantic.v1.main.LangGraphInput'>), '__end__': Node(id='__end__', data=<class 'pydantic.v1.main.LangGraphOutput'>), 'Research team': Node(id='Research team', data=RunnableLambda(get_last_message)\n",
    "| RunnableLambda(enter_chain)\n",
    "| CompiledGraph(nodes={'Search': ChannelInvoke(bound=RunnableLambda(...)\n",
    "  | RunnableLambda(...)\n",
    "  | ChannelWrite(channels=[('Search', None, False), ('messages', RunnableLambda(...), False), ('team_members', RunnableLambda(...), False), ('next', RunnableLambda(...), False)]), config={'tags': []}, channels={None: 'Search:inbox'}, triggers=['Search:inbox']), 'Web Scraper': ChannelInvoke(bound=RunnableLambda(...)\n",
    "  | RunnableLambda(...)\n",
    "  | ChannelWrite(channels=[('Web Scraper', None, False), ('messages', RunnableLambda(...), False), ('team_members', RunnableLambda(...), False), ('next', RunnableLambda(...), False)]), config={'tags': []}, channels={None: 'Web Scraper:inbox'}, triggers=['Web Scraper:inbox']), 'supervisor': ChannelInvoke(bound=RunnableLambda(...)\n",
    "  | ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'options': \"['FINISH', 'Search', 'Web Scraper']\", 'team_members': 'Search, Web Scraper'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a supervisor tasked with managing a conversation between the following workers:  Search, Web Scraper. Given the following user request, respond with the worker to act next. Each worker will perform a task and respond with their results and status. When finished, respond with FINISH.')), MessagesPlaceholder(variable_name='messages'), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['options'], template='Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}'))])\n",
    "  | RunnableBinding(bound=OllamaFunctions(llm=ChatOllama(model='mistral:7b-instruct-v0.2-q8_0', format='json'), tool_system_prompt_template='You have access to the following tools:\\n\\n{tools}\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{{\\n  \"tool\": <name of the selected tool>,\\n  \"tool_input\": <parameters for the selected tool, matching the tool\\'s JSON schema>\\n}}\\n'), kwargs={'functions': [{'name': 'route', 'description': 'Select the next role.', 'parameters': {'title': 'routeSchema', 'type': 'object', 'properties': {'next': {'title': 'Next', 'anyOf': [{'enum': ['FINISH', 'Search', 'Web Scraper']}]}}, 'required': ['next']}}], 'function_call': 'route'})\n",
    "  | JsonOutputFunctionsParser()\n",
    "  | ChannelWrite(channels=[('supervisor', None, False), ('messages', RunnableLambda(...), False), ('team_members', RunnableLambda(...), False), ('next', RunnableLambda(...), False)]), config={'tags': []}, channels={None: 'supervisor:inbox'}, triggers=['supervisor:inbox']), 'Search:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
    "  | ChannelWrite(channels=[('supervisor:inbox', None, False)]), config={'tags': ['langsmith:hidden']}, channels={None: 'Search'}, triggers=['Search']), 'Web Scraper:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
    "  | ChannelWrite(channels=[('supervisor:inbox', None, False)]), config={'tags': ['langsmith:hidden']}, channels={None: 'Web Scraper'}, triggers=['Web Scraper']), 'supervisor:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
    "  | RunnableLambda(runnable), config={'tags': ['langsmith:hidden']}, channels={None: 'supervisor'}, triggers=['supervisor']), '__start__': ChannelInvoke(bound=ChannelWrite(channels=[('__start__', None, False), ('messages', RunnableLambda(...), False), ('team_members', RunnableLambda(...), False), ('next', RunnableLambda(...), False)]), config={'tags': ['langsmith:hidden']}, channels={None: '__start__:inbox'}, triggers=['__start__:inbox']), '__start__:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
    "  | ChannelWrite(channels=[('supervisor:inbox', None, False)]), config={'tags': ['langsmith:hidden']}, channels={None: '__start__'}, triggers=['__start__'])}, channels={'messages': <langgraph.channels.binop.BinaryOperatorAggregate object at 0x7fb1a41801f0>, 'team_members': <langgraph.channels.last_value.LastValue object at 0x7fb1a4180550>, 'next': <langgraph.channels.last_value.LastValue object at 0x7fb1a41817e0>, 'Search:inbox': <langgraph.channels.any_value.AnyValue object at 0x7fb1a4370670>, 'Web Scraper:inbox': <langgraph.channels.any_value.AnyValue object at 0x7fb1a4183f70>, 'supervisor:inbox': <langgraph.channels.any_value.AnyValue object at 0x7fb1a41823b0>, '__start__:inbox': <langgraph.channels.any_value.AnyValue object at 0x7fb1a41821a0>, 'Search': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fb1a4180610>, 'Web Scraper': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fb1a41805b0>, 'supervisor': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fb1a4182110>, '__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fb1a41834c0>, '__end__': <langgraph.channels.last_value.LastValue object at 0x7fb1a4181c90>, <ReservedChannels.is_last_step: 'is_last_step'>: <langgraph.channels.last_value.LastValue object at 0x7fb1a41800d0>}, output='__end__', hidden=['Search:inbox', 'Web Scraper:inbox', 'supervisor:inbox', '__start__', 'messages', 'team_members', 'next'], input='__start__:inbox', graph=<langgraph.graph.state.StateGraph object at 0x7fb1a4183850>)\n",
    "| RunnableLambda(join_graph)), 'Paper writing team': Node(id='Paper writing team', data=RunnableLambda(get_last_message)\n",
    "| RunnableLambda(...)\n",
    "| CompiledGraph(nodes={'Doc Writer': ChannelInvoke(bound=RunnableLambda(...)\n",
    "  | RunnableLambda(...)\n",
    "  | ChannelWrite(channels=[('Doc Writer', None, False), ('messages', RunnableLambda(...), False), ('team_members', RunnableLambda(...), False), ('next', RunnableLambda(...), False), ('current_files', RunnableLambda(...), False)]), config={'tags': []}, channels={None: 'Doc Writer:inbox'}, triggers=['Doc Writer:inbox']), 'Note Taker': ChannelInvoke(bound=RunnableLambda(...)\n",
    "  | RunnableLambda(...)\n",
    "  | ChannelWrite(channels=[('Note Taker', None, False), ('messages', RunnableLambda(...), False), ('team_members', RunnableLambda(...), False), ('next', RunnableLambda(...), False), ('current_files', RunnableLambda(...), False)]), config={'tags': []}, channels={None: 'Note Taker:inbox'}, triggers=['Note Taker:inbox']), 'Chart Generator': ChannelInvoke(bound=RunnableLambda(...)\n",
    "  | RunnableLambda(...)\n",
    "  | ChannelWrite(channels=[('Chart Generator', None, False), ('messages', RunnableLambda(...), False), ('team_members', RunnableLambda(...), False), ('next', RunnableLambda(...), False), ('current_files', RunnableLambda(...), False)]), config={'tags': []}, channels={None: 'Chart Generator:inbox'}, triggers=['Chart Generator:inbox']), 'supervisor': ChannelInvoke(bound=RunnableLambda(...)\n",
    "  | ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'options': \"['FINISH', 'Doc Writer', 'Note Taker', 'Chart Generator']\", 'team_members': 'Doc Writer, Note Taker, Chart Generator'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['team_members'], template='You are a supervisor tasked with managing a conversation between the following workers:  {team_members}. Given the following user request, respond with the worker to act next. Each worker will perform a task and respond with their results and status. When finished, respond with FINISH.')), MessagesPlaceholder(variable_name='messages'), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['options'], template='Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}'))])\n",
    "  | RunnableBinding(bound=ChatOllama(model='mistral:7b-instruct-v0.2-q8_0'), kwargs={'functions': [{'name': 'route', 'description': 'Select the next role.', 'parameters': {'title': 'routeSchema', 'type': 'object', 'properties': {'next': {'title': 'Next', 'anyOf': [{'enum': ['FINISH', 'Doc Writer', 'Note Taker', 'Chart Generator']}]}}, 'required': ['next']}}], 'function_call': 'route'})\n",
    "  | JsonOutputFunctionsParser()\n",
    "  | ChannelWrite(channels=[('supervisor', None, False), ('messages', RunnableLambda(...), False), ('team_members', RunnableLambda(...), False), ('next', RunnableLambda(...), False), ('current_files', RunnableLambda(...), False)]), config={'tags': []}, channels={None: 'supervisor:inbox'}, triggers=['supervisor:inbox']), 'Doc Writer:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
    "  | ChannelWrite(channels=[('supervisor:inbox', None, False)]), config={'tags': ['langsmith:hidden']}, channels={None: 'Doc Writer'}, triggers=['Doc Writer']), 'Note Taker:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
    "  | ChannelWrite(channels=[('supervisor:inbox', None, False)]), config={'tags': ['langsmith:hidden']}, channels={None: 'Note Taker'}, triggers=['Note Taker']), 'Chart Generator:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
    "  | ChannelWrite(channels=[('supervisor:inbox', None, False)]), config={'tags': ['langsmith:hidden']}, channels={None: 'Chart Generator'}, triggers=['Chart Generator']), 'supervisor:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
    "  | RunnableLambda(runnable), config={'tags': ['langsmith:hidden']}, channels={None: 'supervisor'}, triggers=['supervisor']), '__start__': ChannelInvoke(bound=ChannelWrite(channels=[('__start__', None, False), ('messages', RunnableLambda(...), False), ('team_members', RunnableLambda(...), False), ('next', RunnableLambda(...), False), ('current_files', RunnableLambda(...), False)]), config={'tags': ['langsmith:hidden']}, channels={None: '__start__:inbox'}, triggers=['__start__:inbox']), '__start__:edges': ChannelInvoke(bound=RunnableLambda(_read)\n",
    "  | ChannelWrite(channels=[('supervisor:inbox', None, False)]), config={'tags': ['langsmith:hidden']}, channels={None: '__start__'}, triggers=['__start__'])}, channels={'messages': <langgraph.channels.binop.BinaryOperatorAggregate object at 0x7fb1a43b8e20>, 'team_members': <langgraph.channels.last_value.LastValue object at 0x7fb1a43b89d0>, 'next': <langgraph.channels.last_value.LastValue object at 0x7fb1a43b8e80>, 'current_files': <langgraph.channels.last_value.LastValue object at 0x7fb1a43b86d0>, 'Doc Writer:inbox': <langgraph.channels.any_value.AnyValue object at 0x7fb1a43b9360>, 'Note Taker:inbox': <langgraph.channels.any_value.AnyValue object at 0x7fb1a43b8cd0>, 'Chart Generator:inbox': <langgraph.channels.any_value.AnyValue object at 0x7fb1a43b8fd0>, 'supervisor:inbox': <langgraph.channels.any_value.AnyValue object at 0x7fb1a425bd90>, '__start__:inbox': <langgraph.channels.any_value.AnyValue object at 0x7fb1a425aef0>, 'Doc Writer': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fb1a425bb20>, 'Note Taker': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fb1a425b400>, 'Chart Generator': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fb1a425bc70>, 'supervisor': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fb1a425ae90>, '__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fb1a425b160>, '__end__': <langgraph.channels.last_value.LastValue object at 0x7fb1a425bc40>, <ReservedChannels.is_last_step: 'is_last_step'>: <langgraph.channels.last_value.LastValue object at 0x7fb1a425b6a0>}, output='__end__', hidden=['Doc Writer:inbox', 'Note Taker:inbox', 'Chart Generator:inbox', 'supervisor:inbox', '__start__', 'messages', 'team_members', 'next', 'current_files'], input='__start__:inbox', graph=<langgraph.graph.state.StateGraph object at 0x7fb1a43b8bb0>)\n",
    "| RunnableLambda(join_graph)), 'supervisor': Node(id='supervisor', data=ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'options': \"['FINISH', 'Research team', 'Paper writing team']\", 'team_members': 'Research team, Paper writing team'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['team_members'], template='You are a supervisor tasked with managing a conversation between the following teams: {team_members}. Given the following user request, respond with the worker to act next. Each worker will perform a task and respond with their results and status. When finished, respond with FINISH.')), MessagesPlaceholder(variable_name='messages'), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['options'], template='Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}'))])\n",
    "| RunnableBinding(bound=ChatOllama(model='mistral:7b-instruct-v0.2-q8_0'), kwargs={'functions': [{'name': 'route', 'description': 'Select the next role.', 'parameters': {'title': 'routeSchema', 'type': 'object', 'properties': {'next': {'title': 'Next', 'anyOf': [{'enum': ['FINISH', 'Research team', 'Paper writing team']}]}}, 'required': ['next']}}], 'function_call': 'route'})\n",
    "| JsonOutputFunctionsParser()), 'supervisor_<lambda>': Node(id='supervisor_<lambda>', data=RunnableLambda(runnable))}, edges=[Edge(source='Research team', target='supervisor', data=None), Edge(source='Paper writing team', target='supervisor', data=None), Edge(source='supervisor', target='supervisor_<lambda>', data=None), Edge(source='supervisor_<lambda>', target='Paper writing team', data='Paper writing team'), Edge(source='supervisor_<lambda>', target='Research team', data='Research team'), Edge(source='supervisor_<lambda>', target='__end__', data='FINISH'), Edge(source='__start__', target='supervisor', data=None)])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
