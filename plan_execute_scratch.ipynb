{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tools = [TavilySearchResults(max_results=3)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent Execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from json import JSONDecodeError\n",
    "from typing import List, Union\n",
    "\n",
    "from langchain_core.agents import AgentAction, AgentActionMessageLog, AgentFinish\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    ")\n",
    "from langchain_core.outputs import ChatGeneration, Generation\n",
    "\n",
    "from langchain.agents.agent import AgentOutputParser\n",
    "\n",
    "\n",
    "class OpenAIFunctionsAgentOutputParser(AgentOutputParser):\n",
    "    \"\"\"Parses a message into agent action/finish.\n",
    "\n",
    "    Is meant to be used with OpenAI models, as it relies on the specific\n",
    "    function_call parameter from OpenAI to convey what tools to use.\n",
    "\n",
    "    If a function_call parameter is passed, then that is used to get\n",
    "    the tool and tool input.\n",
    "\n",
    "    If one is not passed, then the AIMessage is assumed to be the final output.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"openai-functions-agent\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_ai_message(message: BaseMessage) -> Union[AgentAction, AgentFinish]:\n",
    "        \"\"\"Parse an AI message.\"\"\"\n",
    "        if not isinstance(message, AIMessage):\n",
    "            raise TypeError(f\"Expected an AI message got {type(message)}\")\n",
    "\n",
    "        function_call = message.additional_kwargs.get(\"function_call\", {})\n",
    "\n",
    "        if function_call:\n",
    "            function_name = function_call[\"name\"]\n",
    "            try:\n",
    "                if len(function_call[\"arguments\"].strip()) == 0:\n",
    "                    # OpenAI returns an empty string for functions containing no args\n",
    "                    _tool_input = {}\n",
    "                else:\n",
    "                    # otherwise it returns a json object\n",
    "                    _tool_input = json.loads(function_call[\"arguments\"], strict=False)\n",
    "            except JSONDecodeError:\n",
    "                raise OutputParserException(\n",
    "                    f\"Could not parse tool input: {function_call} because \"\n",
    "                    f\"the `arguments` is not valid JSON.\"\n",
    "                )\n",
    "\n",
    "            # HACK HACK HACK:\n",
    "            # The code that encodes tool input into Open AI uses a special variable\n",
    "            # name called `__arg1` to handle old style tools that do not expose a\n",
    "            # schema and expect a single string argument as an input.\n",
    "            # We unpack the argument here if it exists.\n",
    "            # Open AI does not support passing in a JSON array as an argument.\n",
    "            if \"__arg1\" in _tool_input:\n",
    "                tool_input = _tool_input[\"__arg1\"]\n",
    "            else:\n",
    "                tool_input = _tool_input\n",
    "\n",
    "            content_msg = f\"responded: {message.content}\\n\" if message.content else \"\\n\"\n",
    "            log = f\"\\nInvoking: `{function_name}` with `{tool_input}`\\n{content_msg}\\n\"\n",
    "            return AgentActionMessageLog(\n",
    "                tool=function_name,\n",
    "                tool_input=tool_input,\n",
    "                log=log,\n",
    "                message_log=[message],\n",
    "            )\n",
    "\n",
    "        return AgentFinish(\n",
    "            return_values={\"output\": message.content}, log=str(message.content)\n",
    "        )\n",
    "\n",
    "    def parse_result(\n",
    "        self, result: List[Generation], *, partial: bool = False\n",
    "    ) -> Union[AgentAction, AgentFinish]:\n",
    "        if not isinstance(result[0], ChatGeneration):\n",
    "            raise ValueError(\"This output parser only works on ChatGeneration output\")\n",
    "        message = result[0].message\n",
    "        return self._parse_ai_message(message)\n",
    "\n",
    "    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n",
    "        raise ValueError(\"Can only parse messages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_core.runnables import Runnable, RunnablePassthrough\n",
    "from langchain.agents.format_scratchpad.openai_functions import (\n",
    "    format_to_openai_function_messages,\n",
    ")\n",
    "\n",
    "# from langchain.agents.output_parsers.openai_functions import (\n",
    "#     OpenAIFunctionsAgentOutputParser,\n",
    "# )\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "# Choose the LLM that will drive the agent\n",
    "# llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "# llm = OllamaFunctions(model='mistral:7b-instruct-v0.2-q6_K', tool_system_prompt_template='')\n",
    "\n",
    "llm = OllamaFunctions(model='mistral:7b-instruct-v0.2-q6_K')\n",
    "\n",
    "# Construct the OpenAI Functions agent\n",
    "agent_runnable = create_openai_functions_agent(llm, tools, prompt)\n",
    "\n",
    "# llm_with_tools = llm.bind(functions=[convert_to_openai_function(t) for t in tools])\n",
    "\n",
    "llm_with_tools = llm.bind(\n",
    "    functions=[\n",
    "    {'name': 'tavily_search_results_json', \n",
    "     'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer     questions about current events. Input should be a search query.', \n",
    "     'parameters': {'type': 'object', \n",
    "                    'properties': \n",
    "                        {'query': {'description': 'search query to look up', 'type': 'string'}}, \n",
    "                        'required': ['query']}}\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "agent = (\n",
    "    RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        )\n",
    "    )\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_function_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=OllamaFunctions(llm=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', format='json'), tool_system_prompt_template='You have access to the following tools:\\n\\n{tools}\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{{\\n  \"tool\": <name of the selected tool>,\\n  \"tool_input\": <parameters for the selected tool, matching the tool\\'s JSON schema>\\n}}\\n'), kwargs={'functions': [{'name': 'tavily_search_results_json', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer     questions about current events. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}}]})\n",
       "| OpenAIFunctionsAgentOutputParser()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_agent_executor\n",
    "\n",
    "agent_executor = create_agent_executor(agent, tools)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)\n",
    "from langchain.globals import set_verbose\n",
    "\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/step]\u001b[0m \u001b[1mStarting step 0 with 1 task. Next tasks:\n",
      "\u001b[0m- __start__({'chat_history': [], 'input': 'who is the winnner of the us open'})\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 2:chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 2:chain:__start__] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"who is the winnner of the us open\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 3:chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 3:chain:__start__] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 4:chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 4:chain:__start__] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 5:chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 5:chain:__start__] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:__start__] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/checkpoint]\u001b[0m \u001b[1mFinishing step 0. Channel values:\n",
      "\u001b[0m{'__start__': {...},\n",
      " '__start__:inbox': {...},\n",
      " 'chat_history': [],\n",
      " 'input': 'who is the winnner of the us open',\n",
      " 'intermediate_steps': [],\n",
      " <ReservedChannels.is_last_step: 'is_last_step'>: False}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/step]\u001b[0m \u001b[1mStarting step 1 with 1 task. Next tasks:\n",
      "\u001b[0m- __start__:edges({'chat_history': [], 'input': 'who is the winnner of the us open'})\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 7:chain:__start__:edges] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 7:chain:__start__:edges > 8:chain:ChannelRead<['input', 'chat_history', 'agent_outcome', 'intermediate_steps']>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 7:chain:__start__:edges > 8:chain:ChannelRead<['input', 'chat_history', 'agent_outcome', 'intermediate_steps']>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 7:chain:__start__:edges > 9:chain:ChannelWrite<agent:inbox>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 7:chain:__start__:edges > 9:chain:ChannelWrite<agent:inbox>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 7:chain:__start__:edges] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/checkpoint]\u001b[0m \u001b[1mFinishing step 1. Channel values:\n",
      "\u001b[0m{'__start__:inbox': {...},\n",
      " 'agent:inbox': {...},\n",
      " 'chat_history': [],\n",
      " 'input': 'who is the winnner of the us open',\n",
      " 'intermediate_steps': [],\n",
      " <ReservedChannels.is_last_step: 'is_last_step'>: False}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/step]\u001b[0m \u001b[1mStarting step 2 with 1 task. Next tasks:\n",
      "\u001b[0m- agent({'agent_outcome': None,\n",
      " 'chat_history': [],\n",
      " 'input': 'who is the winnner of the us open',\n",
      " 'intermediate_steps': []})\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 11:chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 11:chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 14:chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 14:chain:RunnableAssign<agent_scratchpad> > 15:chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 14:chain:RunnableAssign<agent_scratchpad> > 15:chain:RunnableParallel<agent_scratchpad> > 16:chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 14:chain:RunnableAssign<agent_scratchpad> > 15:chain:RunnableParallel<agent_scratchpad> > 16:chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 14:chain:RunnableAssign<agent_scratchpad> > 15:chain:RunnableParallel<agent_scratchpad>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"agent_scratchpad\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 14:chain:RunnableAssign<agent_scratchpad>] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": [],\n",
      "  \"agent_scratchpad\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 17:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": [],\n",
      "  \"agent_scratchpad\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 17:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 18:llm:OllamaFunctions] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a helpful assistant\\nHuman: who is the winnner of the us open\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 19:llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You have access to the following tools:\\n\\n[\\n  {\\n    \\\"name\\\": \\\"tavily_search_results_json\\\",\\n    \\\"description\\\": \\\"A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer     questions about current events. Input should be a search query.\\\",\\n    \\\"parameters\\\": {\\n      \\\"type\\\": \\\"object\\\",\\n      \\\"properties\\\": {\\n        \\\"query\\\": {\\n          \\\"description\\\": \\\"search query to look up\\\",\\n          \\\"type\\\": \\\"string\\\"\\n        }\\n      },\\n      \\\"required\\\": [\\n        \\\"query\\\"\\n      ]\\n    }\\n  }\\n]\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{\\n  \\\"tool\\\": <name of the selected tool>,\\n  \\\"tool_input\\\": <parameters for the selected tool, matching the tool's JSON schema>\\n}\\n\\nSystem: You are a helpful assistant\\nHuman: who is the winnner of the us open\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 19:llm:ChatOllama] [1.99s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Invalid tool selection. The provided text does not represent a valid tool name from the given tools.\\\"\\n  }\\n}\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Invalid tool selection. The provided text does not represent a valid tool name from the given tools.\\\"\\n  }\\n}\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[31;1m\u001b[1;3m[llm/error]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 18:llm:OllamaFunctions] [1.99s] LLM run errored with error:\n",
      "\u001b[0m\"KeyError('tool')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py\\\", line 104, in _generate\\n    called_tool_name = parsed_chat_result[\\\"tool\\\"]\\n\\n\\nKeyError: 'tool'\"\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence] [2.00s] Chain run errored with error:\n",
      "\u001b[0m\"KeyError('tool')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 4069, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 166, in invoke\\n    self.generate_prompt(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 544, in generate_prompt\\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 408, in generate\\n    raise e\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py\\\", line 104, in _generate\\n    called_tool_name = parsed_chat_result[\\\"tool\\\"]\\n\\n\\nKeyError: 'tool'\"\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent] [2.01s] Chain run errored with error:\n",
      "\u001b[0m\"KeyError('tool')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 1262, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/config.py\\\", line 326, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 3397, in _invoke\\n    output = call_func_with_variable_args(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/config.py\\\", line 326, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/prebuilt/agent_executor.py\\\", line 64, in run_agent\\n    agent_outcome = agent_runnable.invoke(data)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 4069, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 166, in invoke\\n    self.generate_prompt(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 544, in generate_prompt\\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 408, in generate\\n    raise e\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py\\\", line 104, in _generate\\n    called_tool_name = parsed_chat_result[\\\"tool\\\"]\\n\\n\\nKeyError: 'tool'\"\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent] [2.01s] Chain run errored with error:\n",
      "\u001b[0m\"KeyError('tool')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 3523, in invoke\\n    return self._call_with_config(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 1262, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/config.py\\\", line 326, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 3397, in _invoke\\n    output = call_func_with_variable_args(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/config.py\\\", line 326, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/prebuilt/agent_executor.py\\\", line 64, in run_agent\\n    agent_outcome = agent_runnable.invoke(data)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 4069, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 166, in invoke\\n    self.generate_prompt(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 544, in generate_prompt\\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 408, in generate\\n    raise e\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py\\\", line 104, in _generate\\n    called_tool_name = parsed_chat_result[\\\"tool\\\"]\\n\\n\\nKeyError: 'tool'\"\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:LangGraph] [2.03s] Chain run errored with error:\n",
      "\u001b[0m\"KeyError('tool')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 1513, in _transform_stream_with_config\\n    chunk: Output = context.run(next, iterator)  # type: ignore\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py\\\", line 355, in _transform\\n    _interrupt_or_proceed(done, inflight, step)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py\\\", line 698, in _interrupt_or_proceed\\n    raise exc\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/concurrent/futures/thread.py\\\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 4069, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 3523, in invoke\\n    return self._call_with_config(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 1262, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/config.py\\\", line 326, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 3397, in _invoke\\n    output = call_func_with_variable_args(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/config.py\\\", line 326, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/prebuilt/agent_executor.py\\\", line 64, in run_agent\\n    agent_outcome = agent_runnable.invoke(data)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 4069, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 166, in invoke\\n    self.generate_prompt(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 544, in generate_prompt\\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 408, in generate\\n    raise e\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py\\\", line 104, in _generate\\n    called_tool_name = parsed_chat_result[\\\"tool\\\"]\\n\\n\\nKeyError: 'tool'\"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'tool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwho is the winnner of the us open\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py:579\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, output_keys, input_keys, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    577\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]:\n\u001b[1;32m    578\u001b[0m     latest: Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    581\u001b[0m         config,\n\u001b[1;32m    582\u001b[0m         output_keys\u001b[38;5;241m=\u001b[39moutput_keys \u001b[38;5;28;01mif\u001b[39;00m output_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput,\n\u001b[1;32m    583\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    585\u001b[0m     ):\n\u001b[1;32m    586\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m latest\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py:615\u001b[0m, in \u001b[0;36mPregel.transform\u001b[0;34m(self, input, config, output_keys, input_keys, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    614\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]]:\n\u001b[0;32m--> 615\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[1;32m    616\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[1;32m    618\u001b[0m         config,\n\u001b[1;32m    619\u001b[0m         output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m    620\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    622\u001b[0m     ):\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:1513\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1513\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m   1515\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py:355\u001b[0m, in \u001b[0;36mPregel._transform\u001b[0;34m(self, input, run_manager, config, input_keys, output_keys, interrupt)\u001b[0m\n\u001b[1;32m    348\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    349\u001b[0m     futures,\n\u001b[1;32m    350\u001b[0m     return_when\u001b[38;5;241m=\u001b[39mconcurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m    351\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m    352\u001b[0m )\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# interrupt on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m \u001b[43m_interrupt_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# apply writes to channels\u001b[39;00m\n\u001b[1;32m    358\u001b[0m _apply_writes(\n\u001b[1;32m    359\u001b[0m     checkpoint, channels, pending_writes, config, step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    360\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py:698\u001b[0m, in \u001b[0;36m_interrupt_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m    696\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m--> 698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;66;03m# TODO this is where retry of an entire step would happen\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:4069\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4065\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4066\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4067\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:3523\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3521\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m   3522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 3523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3524\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3525\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3526\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3527\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3528\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3529\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3531\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3533\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:1262\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1259\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m   1260\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1261\u001b[0m         Output,\n\u001b[0;32m-> 1262\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1265\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1266\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1270\u001b[0m     )\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1272\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:3397\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3395\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   3396\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3397\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3398\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   3399\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3400\u001b[0m \u001b[38;5;66;03m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[1;32m   3401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/prebuilt/agent_executor.py:64\u001b[0m, in \u001b[0;36mcreate_agent_executor.<locals>.run_agent\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_agent\u001b[39m(data):\n\u001b[0;32m---> 64\u001b[0m     agent_outcome \u001b[38;5;241m=\u001b[39m \u001b[43magent_runnable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_outcome\u001b[39m\u001b[38;5;124m\"\u001b[39m: agent_outcome}\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:4069\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4065\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4066\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4067\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:166\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    163\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    165\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    175\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py:104\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m did not respond with valid JSON. Please try again.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    103\u001b[0m     )\n\u001b[0;32m--> 104\u001b[0m called_tool_name \u001b[38;5;241m=\u001b[39m \u001b[43mparsed_chat_result\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    105\u001b[0m called_tool_arguments \u001b[38;5;241m=\u001b[39m parsed_chat_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_input\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    106\u001b[0m called_tool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m    107\u001b[0m     (fn \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m functions \u001b[38;5;28;01mif\u001b[39;00m fn[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m called_tool_name), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    108\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tool'"
     ]
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\"input\": \"who is the winnner of the us open\", \"chat_history\": []}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan and Execute - mar 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os \n",
    "from langchain_community.llms import Ollama \n",
    "# from langchain_experimental.llms.ollama_functions import OllamaFunctions \n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tools = [TavilySearchResults(max_results=3)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from typing import Any, List, Optional, Sequence, Tuple, Type, Union\n",
    "from langchain_core.prompts.chat import (\n",
    "    BaseMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain.agents.format_scratchpad.openai_functions import (format_to_openai_function_messages)\n",
    "from langchain.agents.output_parsers.openai_functions import (\n",
    "    OpenAIFunctionsAgentOutputParser)\n",
    "from langchain_core.runnables import Runnable, RunnablePassthrough\n",
    "\n",
    "\n",
    "def create_openai_functions_agent(\n",
    "    llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: ChatPromptTemplate\n",
    ") -> Runnable:\n",
    "   \n",
    "    if \"agent_scratchpad\" not in prompt.input_variables:\n",
    "        raise ValueError(\n",
    "            \"Prompt must have input variable `agent_scratchpad`, but wasn't found. \"\n",
    "            f\"Found {prompt.input_variables} instead.\"\n",
    "        )\n",
    "        \n",
    "    llm_with_tools = llm.bind(functions=[convert_to_openai_function(t) for t in tools])\n",
    "    agent = (\n",
    "        RunnablePassthrough.assign(\n",
    "            agent_scratchpad=lambda x: format_to_openai_function_messages(\n",
    "                x[\"intermediate_steps\"]\n",
    "            )\n",
    "        )\n",
    "        # | prompt\n",
    "        | llm_with_tools\n",
    "        # | OpenAIFunctionsAgentOutputParser()\n",
    "    )\n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprompt\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_with_tools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm_with_tools\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm_with_tools' is not defined"
     ]
    }
   ],
   "source": [
    "llm_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import AIMessage, BaseMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate\n",
    "\n",
    "from langchain_experimental.pydantic_v1 import root_validator\n",
    "\n",
    "DEFAULT_SYSTEM_TEMPLATE = \"\"\"You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "You must always select one of the above tools and respond with only a JSON object matching the following schema:\n",
    "\n",
    "{{\n",
    "  \"tool\": <name of the selected tool>,\n",
    "  \"tool_input\": <parameters for the selected tool, matching the tool's JSON schema>\n",
    "}}\n",
    "\"\"\"  # noqa: E501\n",
    "\n",
    "\n",
    "DEFAULT_RESPONSE_FUNCTION = {\n",
    "    \"name\": \"__conversational_response\",\n",
    "    \"description\": (\n",
    "        \"Respond conversationally if no other tools should be called for a given query.\"\n",
    "    ),\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"response\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Conversational response to the user.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"response\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "class OllamaFunctions(BaseChatModel):\n",
    "    \"\"\"Function chat model that uses Ollama API.\"\"\"\n",
    "\n",
    "    llm: ChatOllama\n",
    "\n",
    "    tool_system_prompt_template: str\n",
    "\n",
    "    @root_validator(pre=True)\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        values[\"llm\"] = values.get(\"llm\") or ChatOllama(**values, format=\"json\")\n",
    "        values[\"tool_system_prompt_template\"] = (\n",
    "            values.get(\"tool_system_prompt_template\") or DEFAULT_SYSTEM_TEMPLATE\n",
    "        )\n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    def model(self) -> BaseChatModel:\n",
    "        \"\"\"For backwards compatibility.\"\"\"\n",
    "        return self.llm\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        functions = kwargs.get(\"functions\", [])\n",
    "        if \"function_call\" in kwargs:\n",
    "            functions = [\n",
    "                fn for fn in functions if fn[\"name\"] == kwargs[\"function_call\"][\"name\"]\n",
    "            ]\n",
    "            if not functions:\n",
    "                raise ValueError(\n",
    "                    'If \"function_call\" is specified, you must also pass a matching \\\n",
    "function in \"functions\".'\n",
    "                )\n",
    "            del kwargs[\"function_call\"]\n",
    "        elif not functions:\n",
    "            functions.append(DEFAULT_RESPONSE_FUNCTION)\n",
    "        system_message_prompt_template = SystemMessagePromptTemplate.from_template(\n",
    "            self.tool_system_prompt_template\n",
    "        )\n",
    "        system_message = system_message_prompt_template.format(\n",
    "            tools=json.dumps(functions, indent=2)\n",
    "        )\n",
    "        if \"functions\" in kwargs:\n",
    "            del kwargs[\"functions\"]\n",
    "        response_message = self.llm.predict_messages(\n",
    "            [system_message] + messages, stop=stop, callbacks=run_manager, **kwargs\n",
    "        )\n",
    "        chat_generation_content = response_message.content\n",
    "        if not isinstance(chat_generation_content, str):\n",
    "            raise ValueError(\"OllamaFunctions does not support non-string output.\")\n",
    "        try:\n",
    "            parsed_chat_result = json.loads(chat_generation_content)\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\n",
    "                f'\"{self.llm.model}\" did not respond with valid JSON. Please try again.'\n",
    "            )\n",
    "        called_tool_name = parsed_chat_result[\"tool\"]\n",
    "        called_tool_arguments = parsed_chat_result[\"tool_input\"]\n",
    "        called_tool = next(\n",
    "            (fn for fn in functions if fn[\"name\"] == called_tool_name), None\n",
    "        )\n",
    "        if called_tool is None:\n",
    "            raise ValueError(\n",
    "                f\"Failed to parse a function call from {self.llm.model} \\\n",
    "output: {chat_generation_content}\"\n",
    "            )\n",
    "        if called_tool[\"name\"] == DEFAULT_RESPONSE_FUNCTION[\"name\"]:\n",
    "            return ChatResult(\n",
    "                generations=[\n",
    "                    ChatGeneration(\n",
    "                        message=AIMessage(\n",
    "                            content=called_tool_arguments[\"response\"],\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        response_message_with_functions = AIMessage(\n",
    "            content=\"\",\n",
    "            additional_kwargs={\n",
    "                \"function_call\": {\n",
    "                    \"name\": called_tool_name,\n",
    "                    \"arguments\": json.dumps(called_tool_arguments)\n",
    "                    if called_tool_arguments\n",
    "                    else \"\",\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "\n",
    "        return ChatResult(\n",
    "            generations=[ChatGeneration(message=response_message_with_functions)]\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ollama_functions\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful assistant\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{chat_history}\u001b[0m\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{input}\u001b[0m\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{agent_scratchpad}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "# from langchain.agents import create_openai_functions_agent\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "# Choose the LLM that will drive the agent\n",
    "# llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "llm = OllamaFunctions(model='mistral:7b-instruct-v0.2-q6_K', tool_system_prompt_template='')\n",
    "\n",
    "# Construct the OpenAI Functions agent\n",
    "\n",
    "# agent_runnable = create_openai_functions_agent(llm, tools, prompt)\n",
    "\n",
    "if \"agent_scratchpad\" not in prompt.input_variables:\n",
    "    raise ValueError(\n",
    "        \"Prompt must have input variable `agent_scratchpad`, but wasn't found. \"\n",
    "        f\"Found {prompt.input_variables} instead.\"\n",
    "    )\n",
    "    \n",
    "llm_with_tools = llm.bind(functions=[convert_to_openai_function(t) for t in tools],\n",
    "                          function_call={\"name\": \"tavily_search_results_json\"},)\n",
    "\n",
    "agent = (\n",
    "    RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        )\n",
    "    )\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=OllamaFunctions(llm=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', format='json'), tool_system_prompt_template='You have access to the following tools:\\n\\n{tools}\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{{\\n  \"tool\": <name of the selected tool>,\\n  \"tool_input\": <parameters for the selected tool, matching the tool\\'s JSON schema>\\n}}\\n'), kwargs={'functions': [{'name': 'tavily_search_results_json', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}}], 'function_call': {'name': 'tavily_search_results_json'}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_function_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=OllamaFunctions(llm=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', format='json'), tool_system_prompt_template='You have access to the following tools:\\n\\n{tools}\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{{\\n  \"tool\": <name of the selected tool>,\\n  \"tool_input\": <parameters for the selected tool, matching the tool\\'s JSON schema>\\n}}\\n'), kwargs={'functions': [{'name': 'tavily_search_results_json', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}}], 'function_call': {'name': 'tavily_search_results_json'}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import hub\n",
    "# # from langchain.agents import create_openai_functions_agent\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# # Get the prompt to use - you can modify this!\n",
    "# prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "# # Choose the LLM that will drive the agent\n",
    "# # llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "# llm = OllamaFunctions(model='mistral:7b-instruct-v0.2-q6_K')\n",
    "\n",
    "# # Construct the OpenAI Functions agent\n",
    "# agent_runnable = create_openai_functions_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug\n",
    "set_debug(True)\n",
    "from langchain.globals import set_verbose\n",
    "# set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_function_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=OllamaFunctions(llm=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', format='json'), tool_system_prompt_template='You have access to the following tools:\\n\\n{tools}\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{{\\n  \"tool\": <name of the selected tool>,\\n  \"tool_input\": <parameters for the selected tool, matching the tool\\'s JSON schema>\\n}}\\n'), kwargs={'functions': [{'name': 'tavily_search_results_json', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}}], 'function_call': {'name': 'tavily_search_results_json'}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/step]\u001b[0m \u001b[1mStarting step 0 with 1 task. Next tasks:\n",
      "\u001b[0m- __start__({'chat_history': [], 'input': 'who is the winnner of the us open'})\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 2:chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 2:chain:__start__] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"who is the winnner of the us open\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 3:chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 3:chain:__start__] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 4:chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 4:chain:__start__] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 5:chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 5:chain:__start__] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:__start__] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/checkpoint]\u001b[0m \u001b[1mFinishing step 0. Channel values:\n",
      "\u001b[0m{'__start__': {...},\n",
      " '__start__:inbox': {...},\n",
      " 'chat_history': [],\n",
      " 'input': 'who is the winnner of the us open',\n",
      " 'intermediate_steps': [],\n",
      " <ReservedChannels.is_last_step: 'is_last_step'>: False}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/step]\u001b[0m \u001b[1mStarting step 1 with 1 task. Next tasks:\n",
      "\u001b[0m- __start__:edges({'chat_history': [], 'input': 'who is the winnner of the us open'})\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 7:chain:__start__:edges] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 7:chain:__start__:edges > 8:chain:ChannelRead<['input', 'chat_history', 'agent_outcome', 'intermediate_steps']>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 7:chain:__start__:edges > 8:chain:ChannelRead<['input', 'chat_history', 'agent_outcome', 'intermediate_steps']>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 7:chain:__start__:edges > 9:chain:ChannelWrite<agent:inbox>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 7:chain:__start__:edges > 9:chain:ChannelWrite<agent:inbox>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 7:chain:__start__:edges] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/checkpoint]\u001b[0m \u001b[1mFinishing step 1. Channel values:\n",
      "\u001b[0m{'__start__:inbox': {...},\n",
      " 'agent:inbox': {...},\n",
      " 'chat_history': [],\n",
      " 'input': 'who is the winnner of the us open',\n",
      " 'intermediate_steps': [],\n",
      " <ReservedChannels.is_last_step: 'is_last_step'>: False}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/step]\u001b[0m \u001b[1mStarting step 2 with 1 task. Next tasks:\n",
      "\u001b[0m- agent({'agent_outcome': None,\n",
      " 'chat_history': [],\n",
      " 'input': 'who is the winnner of the us open',\n",
      " 'intermediate_steps': []})\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 11:chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 11:chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 14:chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 14:chain:RunnableAssign<agent_scratchpad> > 15:chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 14:chain:RunnableAssign<agent_scratchpad> > 15:chain:RunnableParallel<agent_scratchpad> > 16:chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 14:chain:RunnableAssign<agent_scratchpad> > 15:chain:RunnableParallel<agent_scratchpad> > 16:chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 14:chain:RunnableAssign<agent_scratchpad> > 15:chain:RunnableParallel<agent_scratchpad>] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"agent_scratchpad\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 14:chain:RunnableAssign<agent_scratchpad>] [4ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": [],\n",
      "  \"agent_scratchpad\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 17:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"who is the winnner of the us open\",\n",
      "  \"chat_history\": [],\n",
      "  \"agent_outcome\": null,\n",
      "  \"intermediate_steps\": [],\n",
      "  \"agent_scratchpad\": []\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 17:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 18:llm:OllamaFunctions] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a helpful assistant\\nHuman: who is the winnner of the us open\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 19:llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You have access to the following tools:\\n\\n[\\n  {\\n    \\\"name\\\": \\\"tavily_search_results_json\\\",\\n    \\\"description\\\": \\\"A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\\\",\\n    \\\"parameters\\\": {\\n      \\\"type\\\": \\\"object\\\",\\n      \\\"properties\\\": {\\n        \\\"query\\\": {\\n          \\\"description\\\": \\\"search query to look up\\\",\\n          \\\"type\\\": \\\"string\\\"\\n        }\\n      },\\n      \\\"required\\\": [\\n        \\\"query\\\"\\n      ]\\n    }\\n  }\\n]\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{\\n  \\\"tool\\\": <name of the selected tool>,\\n  \\\"tool_input\\\": <parameters for the selected tool, matching the tool's JSON schema>\\n}\\n\\nSystem: You are a helpful assistant\\nHuman: who is the winnner of the us open\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 19:llm:ChatOllama] [2.21s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Your query does not match the expected format. Please use a valid search query as input for the provided tool.\\\",\\n    \\\"code\\\": 400\\n  }\\n}\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"Your query does not match the expected format. Please use a valid search query as input for the provided tool.\\\",\\n    \\\"code\\\": 400\\n  }\\n}\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[31;1m\u001b[1;3m[llm/error]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence > 18:llm:OllamaFunctions] [2.21s] LLM run errored with error:\n",
      "\u001b[0m\"KeyError('tool')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/tmp/ipykernel_475235/1364418659.py\\\", line 104, in _generate\\n    called_tool_name = parsed_chat_result[\\\"tool\\\"]\\n\\n\\nKeyError: 'tool'\"\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent > 13:chain:RunnableSequence] [2.22s] Chain run errored with error:\n",
      "\u001b[0m\"KeyError('tool')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 4069, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 166, in invoke\\n    self.generate_prompt(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 544, in generate_prompt\\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 408, in generate\\n    raise e\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/tmp/ipykernel_475235/1364418659.py\\\", line 104, in _generate\\n    called_tool_name = parsed_chat_result[\\\"tool\\\"]\\n\\n\\nKeyError: 'tool'\"\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent > 12:chain:run_agent] [2.23s] Chain run errored with error:\n",
      "\u001b[0m\"KeyError('tool')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 1262, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/config.py\\\", line 326, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 3397, in _invoke\\n    output = call_func_with_variable_args(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/config.py\\\", line 326, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/prebuilt/agent_executor.py\\\", line 64, in run_agent\\n    agent_outcome = agent_runnable.invoke(data)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 4069, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 166, in invoke\\n    self.generate_prompt(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 544, in generate_prompt\\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 408, in generate\\n    raise e\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/tmp/ipykernel_475235/1364418659.py\\\", line 104, in _generate\\n    called_tool_name = parsed_chat_result[\\\"tool\\\"]\\n\\n\\nKeyError: 'tool'\"\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:LangGraph > 10:chain:agent] [2.23s] Chain run errored with error:\n",
      "\u001b[0m\"KeyError('tool')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 3523, in invoke\\n    return self._call_with_config(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 1262, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/config.py\\\", line 326, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 3397, in _invoke\\n    output = call_func_with_variable_args(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/config.py\\\", line 326, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/prebuilt/agent_executor.py\\\", line 64, in run_agent\\n    agent_outcome = agent_runnable.invoke(data)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 4069, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 166, in invoke\\n    self.generate_prompt(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 544, in generate_prompt\\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 408, in generate\\n    raise e\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/tmp/ipykernel_475235/1364418659.py\\\", line 104, in _generate\\n    called_tool_name = parsed_chat_result[\\\"tool\\\"]\\n\\n\\nKeyError: 'tool'\"\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:LangGraph] [2.25s] Chain run errored with error:\n",
      "\u001b[0m\"KeyError('tool')Traceback (most recent call last):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 1513, in _transform_stream_with_config\\n    chunk: Output = context.run(next, iterator)  # type: ignore\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py\\\", line 355, in _transform\\n    _interrupt_or_proceed(done, inflight, step)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py\\\", line 698, in _interrupt_or_proceed\\n    raise exc\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/concurrent/futures/thread.py\\\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 4069, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 3523, in invoke\\n    return self._call_with_config(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 1262, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/config.py\\\", line 326, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 3397, in _invoke\\n    output = call_func_with_variable_args(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/config.py\\\", line 326, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/prebuilt/agent_executor.py\\\", line 64, in run_agent\\n    agent_outcome = agent_runnable.invoke(data)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 4069, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 166, in invoke\\n    self.generate_prompt(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 544, in generate_prompt\\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 408, in generate\\n    raise e\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/tmp/ipykernel_475235/1364418659.py\\\", line 104, in _generate\\n    called_tool_name = parsed_chat_result[\\\"tool\\\"]\\n\\n\\nKeyError: 'tool'\"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'tool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprebuilt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_agent_executor\n\u001b[1;32m      3\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m create_agent_executor(agent, tools)\n\u001b[0;32m----> 5\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwho is the winnner of the us open\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py:579\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, output_keys, input_keys, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    577\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]:\n\u001b[1;32m    578\u001b[0m     latest: Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    581\u001b[0m         config,\n\u001b[1;32m    582\u001b[0m         output_keys\u001b[38;5;241m=\u001b[39moutput_keys \u001b[38;5;28;01mif\u001b[39;00m output_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput,\n\u001b[1;32m    583\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    585\u001b[0m     ):\n\u001b[1;32m    586\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m latest\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py:615\u001b[0m, in \u001b[0;36mPregel.transform\u001b[0;34m(self, input, config, output_keys, input_keys, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    614\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]]:\n\u001b[0;32m--> 615\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[1;32m    616\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[1;32m    618\u001b[0m         config,\n\u001b[1;32m    619\u001b[0m         output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m    620\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    622\u001b[0m     ):\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:1513\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1513\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m   1515\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py:355\u001b[0m, in \u001b[0;36mPregel._transform\u001b[0;34m(self, input, run_manager, config, input_keys, output_keys, interrupt)\u001b[0m\n\u001b[1;32m    348\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    349\u001b[0m     futures,\n\u001b[1;32m    350\u001b[0m     return_when\u001b[38;5;241m=\u001b[39mconcurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m    351\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m    352\u001b[0m )\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# interrupt on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m \u001b[43m_interrupt_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# apply writes to channels\u001b[39;00m\n\u001b[1;32m    358\u001b[0m _apply_writes(\n\u001b[1;32m    359\u001b[0m     checkpoint, channels, pending_writes, config, step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    360\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py:698\u001b[0m, in \u001b[0;36m_interrupt_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m    696\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m--> 698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;66;03m# TODO this is where retry of an entire step would happen\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:4069\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4065\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4066\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4067\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:3523\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3521\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[39;00m\n\u001b[1;32m   3522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 3523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3524\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3525\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3526\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3527\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3528\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3529\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3531\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3533\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:1262\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1259\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m   1260\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1261\u001b[0m         Output,\n\u001b[0;32m-> 1262\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1265\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1266\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1270\u001b[0m     )\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1272\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:3397\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3395\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   3396\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3397\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3398\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   3399\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3400\u001b[0m \u001b[38;5;66;03m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[1;32m   3401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/prebuilt/agent_executor.py:64\u001b[0m, in \u001b[0;36mcreate_agent_executor.<locals>.run_agent\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_agent\u001b[39m(data):\n\u001b[0;32m---> 64\u001b[0m     agent_outcome \u001b[38;5;241m=\u001b[39m \u001b[43magent_runnable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_outcome\u001b[39m\u001b[38;5;124m\"\u001b[39m: agent_outcome}\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:4069\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4065\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4066\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4067\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:166\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    163\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    165\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    175\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[5], line 104\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m did not respond with valid JSON. Please try again.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    103\u001b[0m     )\n\u001b[0;32m--> 104\u001b[0m called_tool_name \u001b[38;5;241m=\u001b[39m \u001b[43mparsed_chat_result\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    105\u001b[0m called_tool_arguments \u001b[38;5;241m=\u001b[39m parsed_chat_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_input\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    106\u001b[0m called_tool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m    107\u001b[0m     (fn \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m functions \u001b[38;5;28;01mif\u001b[39;00m fn[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m called_tool_name), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    108\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tool'"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_agent_executor\n",
    "\n",
    "agent_executor = create_agent_executor(agent, tools)\n",
    "\n",
    "agent_executor.invoke(\n",
    "    {\"input\": \"who is the winnner of the us open\", \"chat_history\": []}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
