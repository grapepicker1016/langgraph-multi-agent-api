{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "# from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# llm = ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', temperature=0.1)\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "# ollama_func = OllamaFunctions(llm = llm)\n",
    "\n",
    "llm = OllamaFunctions(model='mistral:7b-instruct-v0.2-q6_K')\n",
    "\n",
    "# out = create_structured_output_runnable(ExecuteCode, ollama_func, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=RunnableBinding(bound=OllamaFunctions(llm=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', format='json'), tool_system_prompt_template='You have access to the following tools:\\n\\n{tools}\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{{\\n  \"tool\": <name of the selected tool>,\\n  \"tool_input\": <parameters for the selected tool, matching the tool\\'s JSON schema>\\n}}\\n'), kwargs={'functions': [{'name': '_OutputFormatter', 'description': 'Output formatter. Should always be used to format your response to the user.', 'parameters': {'type': 'object', 'properties': {'output': {'description': 'Record some identifying information about a dog.', 'type': 'object', 'properties': {'name': {'description': \"The dog's name\", 'type': 'string'}, 'color': {'description': \"The dog's color\", 'type': 'string'}, 'fav_food': {'description': \"The dog's favorite food\", 'type': 'string'}}, 'required': ['name', 'color']}}, 'required': ['output']}}], 'function_call': {'name': '_OutputFormatter'}}) last=PydanticAttrOutputFunctionsParser(pydantic_schema=<class 'langchain.chains.structured_output.base._create_openai_functions_structured_output_runnable.<locals>._OutputFormatter'>, attr_name='output')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 35\u001b[0m\n\u001b[1;32m     25\u001b[0m structured_llm \u001b[38;5;241m=\u001b[39m create_structured_output_runnable(\n\u001b[1;32m     26\u001b[0m     RecordDog,\n\u001b[1;32m     27\u001b[0m     llm,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     return_single\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(structured_llm)\n\u001b[0;32m---> 35\u001b[0m \u001b[43mstructured_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHarry was a chubby brown beagle who loved chicken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# -> RecordDog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:4069\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4065\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4066\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4067\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:167\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    163\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    165\u001b[0m         ChatGeneration,\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m--> 167\u001b[0m             [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m],\n\u001b[1;32m    168\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    169\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    170\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    171\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    172\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    173\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    174\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    175\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:150\u001b[0m, in \u001b[0;36mBaseChatModel._convert_input\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39mconvert_to_messages(\u001b[38;5;28minput\u001b[39m))\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain.chains import create_structured_output_runnable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class RecordDog(BaseModel):\n",
    "    '''Record some identifying information about a dog.'''\n",
    "\n",
    "    name: str = Field(..., description=\"The dog's name\")\n",
    "    color: str = Field(..., description=\"The dog's color\")\n",
    "    fav_food: Optional[str] = Field(None, description=\"The dog's favorite food\")\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an extraction algorithm. Please extract every possible instance\"),\n",
    "        ('human', '{input}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "structured_llm = create_structured_output_runnable(\n",
    "    RecordDog,\n",
    "    llm,\n",
    "    mode=\"openai-functions\",\n",
    "    enforce_function_usage=True,\n",
    "    return_single=True\n",
    ")\n",
    "\n",
    "print(structured_llm)\n",
    "\n",
    "structured_llm.invoke({\"input\": \"Harry was a chubby brown beagle who loved chicken\"})\n",
    "# -> RecordDog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first=RunnableBinding(bound=OllamaFunctions(llm=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', temperature=0.1), tool_system_prompt_template='You have access to the following tools:\\n\\n{tools}\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{{\\n  \"tool\": <name of the selected tool>,\\n  \"tool_input\": <parameters for the selected tool, matching the tool\\'s JSON schema>\\n}}\\n'), kwargs={'functions': [{'name': '_OutputFormatter', 'description': 'Output formatter. Should always be used to format your response to the user.', 'parameters': {'type': 'object', 'properties': {'output': {'description': 'Record some identifying information about a dog.', 'type': 'object', 'properties': {'name': {'description': \"The dog's name\", 'type': 'string'}, 'color': {'description': \"The dog's color\", 'type': 'string'}, 'fav_food': {'description': \"The dog's favorite food\", 'type': 'string'}}, 'required': ['name', 'color']}}, 'required': ['output']}}], 'function_call': {'name': '_OutputFormatter'}}) last=PydanticAttrOutputFunctionsParser(pydantic_schema=<class 'langchain.chains.structured_output.base._create_openai_functions_structured_output_runnable.<locals>._OutputFormatter'>, attr_name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'functions': [\n",
    "    {'name': '_OutputFormatter', \n",
    "                'description': 'Output formatter. Should always be used to format your response to the user.', \n",
    "                'parameters': {'type': 'object', \n",
    "                               'properties': {'output': \n",
    "                                   {'description': 'Record some identifying information about a dog.', \n",
    "                                    'type': 'object', \n",
    "                                    'properties': {'name': \n",
    "                                        {'description': \"The dog's name\", \n",
    "                                         'type': 'string'}, \n",
    "                                        'color': {'description': \"The dog's color\", \n",
    "                                                  'type': 'string'}, \n",
    "                                        'fav_food': {'description': \"The dog's favorite food\", 'type': 'string'}}, 'required': ['name', 'color']}}, 'required': ['output']}}], 'function_call': {'name': '_OutputFormatter'}}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "_SYSTEM_PROMPT = \"\"\"Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
    "\n",
    "Question: ${{Question with math problem.}}\n",
    "```text\n",
    "${{single line mathematical expression that solves the problem}}\n",
    "```\n",
    "...numexpr.evaluate(text)...\n",
    "```output\n",
    "${{Output of running the code}}\n",
    "```\n",
    "Answer: ${{Answer}}\n",
    "\n",
    "Begin.\n",
    "\n",
    "Question: What is 37593 * 67?\n",
    "ExecuteCode({{code: \"37593 * 67\"}})\n",
    "...numexpr.evaluate(\"37593 * 67\")...\n",
    "```output\n",
    "2518731\n",
    "```\n",
    "Answer: 2518731\n",
    "\n",
    "Question: 37593^(1/5)\n",
    "ExecuteCode({{code: \"37593**(1/5)\"}})\n",
    "...numexpr.evaluate(\"37593**(1/5)\")...\n",
    "```output\n",
    "8.222831614237718\n",
    "```\n",
    "Answer: 8.222831614237718\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", _SYSTEM_PROMPT),\n",
    "            (\"user\", \"{problem}\"),\n",
    "            MessagesPlaceholder(variable_name=\"context\", optional=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "class ExecuteCode(BaseModel):\n",
    "    \"\"\"The input to the numexpr.evaluate() function.\"\"\"\n",
    "\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"The reasoning behind the code expression, including how context is included, if applicable.\",\n",
    "    )\n",
    "\n",
    "    code: str = Field(\n",
    "        ...,\n",
    "        description=\"The simple code expresssion to execute by numexpr.evaluate().\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatPromptTemplate(input_variables=['problem'], input_types={'context': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'context': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\nExecuteCode({{code: \"37593 * 67\"}})\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\nExecuteCode({{code: \"37593**(1/5)\"}})\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['problem'], template='{problem}')), MessagesPlaceholder(variable_name='context', optional=True)])\n",
      "| RunnableBinding(bound=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', temperature=0.1), kwargs={'functions': [{'name': '_OutputFormatter', 'description': 'Output formatter. Should always be used to format your response to the user.', 'parameters': {'type': 'object', 'properties': {'output': {'description': 'The input to the numexpr.evaluate() function.', 'type': 'object', 'properties': {'reasoning': {'description': 'The reasoning behind the code expression, including how context is included, if applicable.', 'type': 'string'}, 'code': {'description': 'The simple code expresssion to execute by numexpr.evaluate().', 'type': 'string'}}, 'required': ['reasoning', 'code']}}, 'required': ['output']}}], 'function_call': {'name': '_OutputFormatter'}})\n",
      "| PydanticAttrOutputFunctionsParser(pydantic_schema=<class 'langchain.chains.structured_output.base._create_openai_functions_structured_output_runnable.<locals>._OutputFormatter'>, attr_name='output')\n"
     ]
    }
   ],
   "source": [
    "extractor = create_structured_output_runnable(ExecuteCode, llm, prompt, \n",
    "                                                mode='openai-functions',\n",
    "                                                enforce_function_usage=True)\n",
    "from pprint import pprint\n",
    "pprint(extractor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatPromptTemplate(\n",
    "    input_variables=['problem'], \n",
    "    input_types={'context': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, \n",
    "    partial_variables={'context': []}, \n",
    "    messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], \n",
    "                                                                template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\nExecuteCode({{code: \"37593 * 67\"}})\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\nExecuteCode({{code: \"37593**(1/5)\"}})\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n')), \n",
    "              HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['problem'], template='{problem}')), \n",
    "              MessagesPlaceholder(variable_name='context', optional=True)])\n",
    "| \n",
    "RunnableBinding(bound=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', temperature=0.1), \n",
    "                kwargs={'functions': \n",
    "                    [\n",
    "                        {\n",
    "                      'name': '_OutputFormatter', \n",
    "                      'description': 'Output formatter. Should always be used to format your response to the user.', \n",
    "                      'parameters': {\n",
    "                          'type': 'object', \n",
    "                                     'properties': {\n",
    "                                         'output': {\n",
    "                                             'description': 'The input to the numexpr.evaluate() function.',\n",
    "                                             'type': 'object', \n",
    "                                             'properties': {\n",
    "                                                'reasoning': \n",
    "                                                     {\n",
    "                                                         'description': 'The reasoning behind the code expression, including how context is included, if applicable.', \n",
    "                                                         'type': 'string'\n",
    "                                                         },                                          \n",
    "                                                'code': \n",
    "                                                    {\n",
    "                                                        'description': 'The simple code expresssion to execute by numexpr.evaluate().', \n",
    "                                                        'type': 'string'\n",
    "                                                        }\n",
    "                                                    },\n",
    "                                            'required': ['reasoning', 'code']\n",
    "                                            }\n",
    "                                         },\n",
    "                                    'required': ['output']\n",
    "                                    }\n",
    "                      }\n",
    "                     ], \n",
    "                    'function_call': {'name': '_OutputFormatter'}})\n",
    "\n",
    "| PydanticAttrOutputFunctionsParser(pydantic_schema=<class 'langchain.chains.structured_output.base._create_openai_functions_structured_output_runnable.<locals>._OutputFormatter'>, attr_name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_extraction_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "import numexpr\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "_MATH_DESCRIPTION = (\n",
    "    \"math(problem: str, context: Optional[list[str]]) -> float:\\n\"\n",
    "    \" - Solves the provided math problem.\\n\"\n",
    "    ' - `problem` can be either a simple math problem (e.g. \"1 + 3\") or a word problem (e.g. \"how many apples are there if there are 3 apples and 2 apples\").\\n'\n",
    "    \" - You cannot calculate multiple expressions in one call. For instance, `math('1 + 3, 2 + 4')` does not work. \"\n",
    "    \"If you need to calculate multiple expressions, you need to call them separately like `math('1 + 3')` and then `math('2 + 4')`\\n\"\n",
    "    \" - Minimize the number of `math` actions as much as possible. For instance, instead of calling \"\n",
    "    '2. math(\"what is the 10% of $1\") and then call 3. math(\"$1 + $2\"), '\n",
    "    'you MUST call 2. math(\"what is the 110% of $1\") instead, which will reduce the number of math actions.\\n'\n",
    "    # Context specific rules below\n",
    "    \" - You can optionally provide a list of strings as `context` to help the agent solve the problem. \"\n",
    "    \"If there are multiple contexts you need to answer the question, you can provide them as a list of strings.\\n\"\n",
    "    \" - `math` action will not see the output of the previous actions unless you provide it as `context`. \"\n",
    "    \"You MUST provide the output of the previous actions as `context` if you need to do math on it.\\n\"\n",
    "    \" - You MUST NEVER provide `search` type action's outputs as a variable in the `problem` argument. \"\n",
    "    \"This is because `search` returns a text blob that contains the information about the entity, not a number or value. \"\n",
    "    \"Therefore, when you need to provide an output of `search` action, you MUST provide it as a `context` argument to `math` action. \"\n",
    "    'For example, 1. search(\"Barack Obama\") and then 2. math(\"age of $1\") is NEVER allowed. '\n",
    "    'Use 2. math(\"age of Barack Obama\", context=[\"$1\"]) instead.\\n'\n",
    "    \" - When you ask a question about `context`, specify the units. \"\n",
    "    'For instance, \"what is xx in height?\" or \"what is xx in millions?\" instead of \"what is xx?\"\\n'\n",
    ")\n",
    "\n",
    "\n",
    "_SYSTEM_PROMPT = \"\"\"Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
    "\n",
    "Question: ${{Question with math problem.}}\n",
    "```text\n",
    "${{single line mathematical expression that solves the problem}}\n",
    "```\n",
    "...numexpr.evaluate(text)...\n",
    "```output\n",
    "${{Output of running the code}}\n",
    "```\n",
    "Answer: ${{Answer}}\n",
    "\n",
    "Begin.\n",
    "\n",
    "Question: What is 37593 * 67?\n",
    "ExecuteCode({{code: \"37593 * 67\"}})\n",
    "...numexpr.evaluate(\"37593 * 67\")...\n",
    "```output\n",
    "2518731\n",
    "```\n",
    "Answer: 2518731\n",
    "\n",
    "Question: 37593^(1/5)\n",
    "ExecuteCode({{code: \"37593**(1/5)\"}})\n",
    "...numexpr.evaluate(\"37593**(1/5)\")...\n",
    "```output\n",
    "8.222831614237718\n",
    "```\n",
    "Answer: 8.222831614237718\n",
    "\"\"\"\n",
    "\n",
    "_ADDITIONAL_CONTEXT_PROMPT = \"\"\"The following additional context is provided from other functions.\\\n",
    "    Use it to substitute into any ${{#}} variables or other words in the problem.\\\n",
    "    \\n\\n${context}\\n\\nNote that context varibles are not defined in code yet.\\\n",
    "You must extract the relevant numbers and directly put them in code.\"\"\"\n",
    "\n",
    "\n",
    "class ExecuteCode(BaseModel):\n",
    "    \"\"\"The input to the numexpr.evaluate() function.\"\"\"\n",
    "\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"The reasoning behind the code expression, including how context is included, if applicable.\",\n",
    "    )\n",
    "\n",
    "    code: str = Field(\n",
    "        ...,\n",
    "        description=\"The simple code expresssion to execute by numexpr.evaluate().\",\n",
    "    )\n",
    "\n",
    "\n",
    "def _evaluate_expression(expression: str) -> str:\n",
    "    try:\n",
    "        local_dict = {\"pi\": math.pi, \"e\": math.e}\n",
    "        output = str(\n",
    "            numexpr.evaluate(\n",
    "                expression.strip(),\n",
    "                global_dict={},  # restrict access to globals\n",
    "                local_dict=local_dict,  # add common mathematical functions\n",
    "            )\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise ValueError(\n",
    "            f'Failed to evaluate \"{expression}\". Raised error: {repr(e)}.'\n",
    "            \" Please try again with a valid numerical expression\"\n",
    "        )\n",
    "\n",
    "    # Remove any leading and trailing brackets from the output\n",
    "    return re.sub(r\"^\\[|\\]$\", \"\", output)\n",
    "\n",
    "\n",
    "def get_math_tool(llm: ChatOllama):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", _SYSTEM_PROMPT),\n",
    "            (\"user\", \"{problem}\"),\n",
    "            MessagesPlaceholder(variable_name=\"context\", optional=True),\n",
    "        ]\n",
    "    )\n",
    "    extractor = create_structured_output_runnable(ExecuteCode, llm, prompt, mode='openai-functions')\n",
    "\n",
    "    def calculate_expression(\n",
    "        problem: str,\n",
    "        context: Optional[List[str]] = None,\n",
    "        config: Optional[RunnableConfig] = None,\n",
    "    ):\n",
    "        chain_input = {\"problem\": problem}\n",
    "        if context:\n",
    "            context_str = \"\\n\".join(context)\n",
    "            if context_str.strip():\n",
    "                context_str = _ADDITIONAL_CONTEXT_PROMPT.format(\n",
    "                    context=context_str.strip()\n",
    "                )\n",
    "                chain_input[\"context\"] = [SystemMessage(content=context_str)]\n",
    "        code_model = extractor.invoke(chain_input, config)\n",
    "        try:\n",
    "            return _evaluate_expression(code_model.code)\n",
    "        except Exception as e:\n",
    "            return repr(e)\n",
    "\n",
    "    return StructuredTool.from_function(\n",
    "        name=\"math\",\n",
    "        func=calculate_expression,\n",
    "        description=_MATH_DESCRIPTION,\n",
    "    )\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "# from LLMCompiler.math_tools import get_math_tool\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', temperature=0.1)\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "ollama_func = OllamaFunctions(llm = llm, tool_system_prompt_template='')\n",
    "\n",
    "\n",
    "search = TavilySearchResults(\n",
    "    max_results=1,\n",
    "    description='tavily_search_results_json(query=\"the search query\") - a search engine.',\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_f = ollama_func.bind(functions = \n",
    "                    [\n",
    "                        {\n",
    "                      'name': '_OutputFormatter', \n",
    "                      'description': 'Output formatter. Should always be used to format your response to the user.', \n",
    "                      'parameters': {\n",
    "                          'type': 'object', \n",
    "                                     'properties': {\n",
    "                                         'output': {\n",
    "                                             'description': 'The input to the numexpr.evaluate() function.',\n",
    "                                             'type': 'object', \n",
    "                                             'properties': {\n",
    "                                                'reasoning': \n",
    "                                                     {\n",
    "                                                         'description': 'The reasoning behind the code expression, including how context is included, if applicable.', \n",
    "                                                         'type': 'string'\n",
    "                                                         },                                          \n",
    "                                                'code': \n",
    "                                                    {\n",
    "                                                        'description': 'The simple code expresssion to execute by numexpr.evaluate().', \n",
    "                                                        'type': 'string'\n",
    "                                                        }\n",
    "                                                    },\n",
    "                                            'required': ['reasoning', 'code']\n",
    "                                            }\n",
    "                                         },\n",
    "                                    'required': ['output']\n",
    "                                    }\n",
    "                      }\n",
    "                     ], \n",
    "                    function_call= {'name': '_OutputFormatter'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate = get_math_tool(ollama_f)  \n",
    "\n",
    "tools = [search, calculate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=OllamaFunctions(llm=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', temperature=0.1), tool_system_prompt_template='You have access to the following tools:\\n\\n{tools}\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{{\\n  \"tool\": <name of the selected tool>,\\n  \"tool_input\": <parameters for the selected tool, matching the tool\\'s JSON schema>\\n}}\\n'), kwargs={'functions': [{'name': '_OutputFormatter', 'description': 'Output formatter. Should always be used to format your response to the user.', 'parameters': {'type': 'object', 'properties': {'output': {'description': 'The input to the numexpr.evaluate() function.', 'type': 'object', 'properties': {'reasoning': {'description': 'The reasoning behind the code expression, including how context is included, if applicable.', 'type': 'string'}, 'code': {'description': 'The simple code expresssion to execute by numexpr.evaluate().', 'type': 'string'}}, 'required': ['reasoning', 'code']}}, 'required': ['output']}}], 'function_call': {'name': '_OutputFormatter'}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\"mistral:7b-instruct-v0.2-q6_K\" did not respond with valid JSON. Please try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py:99\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     parsed_chat_result \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_generation_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 2 (char 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcalculate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproblem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms the temp of sf + 5?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThet empreature of sf is 33 degrees\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/tools.py:240\u001b[0m, in \u001b[0;36mBaseTool.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, Dict],\n\u001b[1;32m    236\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    239\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/tools.py:419\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;28mstr\u001b[39m(observation), color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    423\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/tools.py:376\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    373\u001b[0m     parsed_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_input(tool_input)\n\u001b[1;32m    374\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[1;32m    375\u001b[0m     observation \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 376\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_validation_error:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/tools.py:701\u001b[0m, in \u001b[0;36mStructuredTool._run\u001b[0;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc:\n\u001b[1;32m    693\u001b[0m     new_argument_supported \u001b[38;5;241m=\u001b[39m signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\n\u001b[1;32m    696\u001b[0m             \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    697\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    698\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    699\u001b[0m         )\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_argument_supported\n\u001b[0;32m--> 701\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m     )\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool does not support sync\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 134\u001b[0m, in \u001b[0;36mget_math_tool.<locals>.calculate_expression\u001b[0;34m(problem, context, config)\u001b[0m\n\u001b[1;32m    130\u001b[0m         context_str \u001b[38;5;241m=\u001b[39m _ADDITIONAL_CONTEXT_PROMPT\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    131\u001b[0m             context\u001b[38;5;241m=\u001b[39mcontext_str\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    132\u001b[0m         )\n\u001b[1;32m    133\u001b[0m         chain_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [SystemMessage(content\u001b[38;5;241m=\u001b[39mcontext_str)]\n\u001b[0;32m--> 134\u001b[0m code_model \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchain_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_expression(code_model\u001b[38;5;241m.\u001b[39mcode)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:4069\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4065\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4066\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4067\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:166\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    163\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    165\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    175\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py:101\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m     parsed_chat_result \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(chat_generation_content)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m did not respond with valid JSON. Please try again.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    103\u001b[0m     )\n\u001b[1;32m    104\u001b[0m called_tool_name \u001b[38;5;241m=\u001b[39m parsed_chat_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    105\u001b[0m called_tool_arguments \u001b[38;5;241m=\u001b[39m parsed_chat_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_input\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: \"mistral:7b-instruct-v0.2-q6_K\" did not respond with valid JSON. Please try again."
     ]
    }
   ],
   "source": [
    "calculate.invoke(\n",
    "    {\n",
    "        \"problem\": \"What's the temp of sf + 5?\",\n",
    "        \"context\": [\"Thet empreature of sf is 33 degrees\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function langchain.chains.structured_output.base.create_structured_output_runnable(output_schema: Union[Dict[str, Any], Type[pydantic.main.BaseModel]], llm: langchain_core.runnables.base.Runnable, prompt: Optional[langchain_core.prompts.base.BasePromptTemplate] = None, *, output_parser: Union[langchain_core.output_parsers.base.BaseOutputParser, langchain_core.output_parsers.base.BaseGenerationOutputParser, NoneType] = None, enforce_function_usage: bool = True, return_single: bool = True, mode: Literal['openai-functions', 'openai-tools', 'openai-json'] = 'openai-functions', **kwargs: Any) -> langchain_core.runnables.base.Runnable>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_structured_output_runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# structured runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Callable, Dict, Literal, Optional, Sequence, Type, Union\n",
    "\n",
    "from langchain_core.output_parsers import (\n",
    "    BaseGenerationOutputParser,\n",
    "    BaseOutputParser,\n",
    "    JsonOutputParser,\n",
    ")\n",
    "from langchain_core.output_parsers.openai_functions import (\n",
    "    JsonOutputFunctionsParser,\n",
    "    PydanticAttrOutputFunctionsParser,\n",
    "    PydanticOutputFunctionsParser,\n",
    ")\n",
    "from langchain_core.prompts import BasePromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.utils.function_calling import (\n",
    "    convert_to_openai_function,\n",
    "    convert_to_openai_tool,\n",
    ")\n",
    "\n",
    "from langchain.output_parsers import (\n",
    "    JsonOutputKeyToolsParser,\n",
    "    PydanticOutputParser,\n",
    "    PydanticToolsParser,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_output_parser(\n",
    "    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\n",
    ") -> Union[BaseOutputParser, BaseGenerationOutputParser]:\n",
    "    \"\"\"Get the appropriate function output parser given the user functions.\n",
    "\n",
    "    Args:\n",
    "        functions: Sequence where element is a dictionary, a pydantic.BaseModel class,\n",
    "            or a Python function. If a dictionary is passed in, it is assumed to\n",
    "            already be a valid OpenAI function.\n",
    "\n",
    "    Returns:\n",
    "        A PydanticOutputFunctionsParser if functions are Pydantic classes, otherwise\n",
    "            a JsonOutputFunctionsParser. If there's only one function and it is\n",
    "            not a Pydantic class, then the output parser will automatically extract\n",
    "            only the function arguments and not the function name.\n",
    "    \"\"\"\n",
    "    if isinstance(functions[0], type) and issubclass(functions[0], BaseModel):\n",
    "        if len(functions) > 1:\n",
    "            pydantic_schema: Union[Dict, Type[BaseModel]] = {\n",
    "                convert_to_openai_function(fn)[\"name\"]: fn for fn in functions\n",
    "            }\n",
    "        else:\n",
    "            pydantic_schema = functions[0]\n",
    "        output_parser: Union[\n",
    "            BaseOutputParser, BaseGenerationOutputParser\n",
    "        ] = PydanticOutputFunctionsParser(pydantic_schema=pydantic_schema)\n",
    "    else:\n",
    "        output_parser = JsonOutputFunctionsParser(args_only=len(functions) <= 1)\n",
    "    return output_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_openai_fn_runnable(\n",
    "    functions: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable]],\n",
    "    llm: Runnable,\n",
    "    prompt: Optional[BasePromptTemplate] = None,\n",
    "    *,\n",
    "    enforce_single_function_usage: bool = True,\n",
    "    output_parser: Optional[Union[BaseOutputParser, BaseGenerationOutputParser]] = None,\n",
    "    **llm_kwargs: Any,\n",
    ") -> Runnable:\n",
    "    \n",
    "    \"\"\"Create a runnable sequence that uses OpenAI functions.\"\"\"\n",
    "\n",
    "    if not functions:\n",
    "        raise ValueError(\"Need to pass in at least one function. Received zero.\")\n",
    "    \n",
    "    openai_functions = [convert_to_openai_function(f) for f in functions]\n",
    "    llm_kwargs_: Dict[str, Any] = {\"functions\": openai_functions, **llm_kwargs}\n",
    "    \n",
    "    if len(openai_functions) == 1 and enforce_single_function_usage:\n",
    "        llm_kwargs_[\"function_call\"] = {\"name\": openai_functions[0][\"name\"]}\n",
    "        \n",
    "    output_parser = output_parser or get_openai_output_parser(functions)\n",
    "    \n",
    "    if prompt:\n",
    "        return prompt | llm.bind(**llm_kwargs_) | output_parser\n",
    "    else:\n",
    "        return llm.bind(**llm_kwargs_) | output_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_openai_functions_structured_output_runnable(\n",
    "    output_schema: Union[Dict[str, Any], Type[BaseModel]],\n",
    "    llm: Runnable,\n",
    "    prompt: Optional[BasePromptTemplate] = None,\n",
    "    *,\n",
    "    output_parser: Optional[Union[BaseOutputParser, BaseGenerationOutputParser]] = None,\n",
    "    **llm_kwargs: Any,\n",
    ") -> Runnable:\n",
    "    if isinstance(output_schema, dict):\n",
    "        function: Any = {\n",
    "            \"name\": \"output_formatter\",\n",
    "            \"description\": (\n",
    "                \"Output formatter. Should always be used to format your response to the\"\n",
    "                \" user.\"\n",
    "            ),\n",
    "            \"parameters\": output_schema,\n",
    "        }\n",
    "    else:\n",
    "\n",
    "        class _OutputFormatter(BaseModel):\n",
    "            \"\"\"Output formatter. Should always be used to format your response to the user.\"\"\"  # noqa: E501\n",
    "\n",
    "            output: output_schema  # type: ignore\n",
    "\n",
    "        function = _OutputFormatter\n",
    "        output_parser = output_parser or PydanticAttrOutputFunctionsParser(\n",
    "            pydantic_schema=_OutputFormatter, attr_name=\"output\"\n",
    "        )\n",
    "    return create_openai_fn_runnable(\n",
    "        [function],\n",
    "        llm,\n",
    "        prompt=prompt,\n",
    "        output_parser=output_parser,\n",
    "        **llm_kwargs,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_structured_output_runnable(\n",
    "    output_schema: Union[Dict[str, Any], Type[BaseModel]],\n",
    "    llm: Runnable,\n",
    "    prompt: Optional[BasePromptTemplate] = None,\n",
    "    *,\n",
    "    output_parser: Optional[Union[BaseOutputParser, BaseGenerationOutputParser]] = None,\n",
    "    enforce_function_usage: bool = True,\n",
    "    return_single: bool = True,\n",
    "    mode: Literal[\n",
    "        \"openai-functions\", \"openai-tools\", \"openai-json\"\n",
    "    ] = \"openai-functions\",\n",
    "    **kwargs: Any,\n",
    ") -> Runnable:\n",
    "    \"\"\"Create a runnable for extracting structured outputs.\"\"\"\n",
    "\n",
    "    # for backwards compatibility\n",
    "    force_function_usage = kwargs.get(\n",
    "        \"enforce_single_function_usage\", enforce_function_usage\n",
    "    )\n",
    "\n",
    "    if mode == \"openai-functions\":\n",
    "        return _create_openai_functions_structured_output_runnable(\n",
    "            output_schema,\n",
    "            llm,\n",
    "            prompt=prompt,\n",
    "            output_parser=output_parser,\n",
    "            enforce_single_function_usage=force_function_usage,\n",
    "            **kwargs,  # llm-specific kwargs\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid mode {mode}. Expected one of 'openai-tools', 'openai-functions', \"\n",
    "            f\"'openai-json'.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=ChatPromptTemplate(input_variables=['input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an extraction algorithm. Please extract every possible instance')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))]) middle=[RunnableBinding(bound=OllamaFunctions(llm=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', temperature=0.1), tool_system_prompt_template='You have access to the following tools:\\n\\n{tools}\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{{\\n  \"tool\": <name of the selected tool>,\\n  \"tool_input\": <parameters for the selected tool, matching the tool\\'s JSON schema>\\n}}\\n'), kwargs={'functions': [{'name': '_OutputFormatter', 'description': 'Output formatter. Should always be used to format your response to the user.', 'parameters': {'type': 'object', 'properties': {'output': {'description': 'Record some identifying information about a dog.', 'type': 'object', 'properties': {'name': {'description': \"The dog's name\", 'type': 'string'}, 'color': {'description': \"The dog's color\", 'type': 'string'}, 'fav_food': {'description': \"The dog's favorite food\", 'type': 'string'}}, 'required': ['name', 'color']}}, 'required': ['output']}}], 'function_call': {'name': '_OutputFormatter'}})] last=PydanticAttrOutputFunctionsParser(pydantic_schema=<class 'langchain.chains.structured_output.base._create_openai_functions_structured_output_runnable.<locals>._OutputFormatter'>, attr_name='output')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\"mistral:7b-instruct-v0.2-q6_K\" did not respond with valid JSON. Please try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py:99\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     parsed_chat_result \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_generation_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 2 (char 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m\n\u001b[1;32m     32\u001b[0m structured_llm \u001b[38;5;241m=\u001b[39m create_structured_output_runnable(\n\u001b[1;32m     33\u001b[0m     RecordDog,\n\u001b[1;32m     34\u001b[0m     ollama_func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     return_single\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(structured_llm)\n\u001b[0;32m---> 43\u001b[0m \u001b[43mstructured_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHarry was a chubby brown beagle who loved chicken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# -> RecordDog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:4069\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4065\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4066\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4067\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:166\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    163\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    165\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    175\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py:101\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m     parsed_chat_result \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(chat_generation_content)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m did not respond with valid JSON. Please try again.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    103\u001b[0m     )\n\u001b[1;32m    104\u001b[0m called_tool_name \u001b[38;5;241m=\u001b[39m parsed_chat_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    105\u001b[0m called_tool_arguments \u001b[38;5;241m=\u001b[39m parsed_chat_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_input\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: \"mistral:7b-instruct-v0.2-q6_K\" did not respond with valid JSON. Please try again."
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain.chains import create_structured_output_runnable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class RecordDog(BaseModel):\n",
    "    '''Record some identifying information about a dog.'''\n",
    "\n",
    "    name: str = Field(..., description=\"The dog's name\")\n",
    "    color: str = Field(..., description=\"The dog's color\")\n",
    "    fav_food: Optional[str] = Field(None, description=\"The dog's favorite food\")\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an extraction algorithm. Please extract every possible instance\"),\n",
    "        ('human', '{input}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', temperature=0.1)\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "ollama_func = OllamaFunctions(llm = llm, tool_system_prompt_template='')\n",
    "\n",
    "structured_llm = create_structured_output_runnable(\n",
    "    RecordDog,\n",
    "    ollama_func,\n",
    "    prompt,\n",
    "    mode=\"openai-functions\",\n",
    "    enforce_function_usage=True,\n",
    "    return_single=True\n",
    ")\n",
    "\n",
    "print(structured_llm)\n",
    "\n",
    "structured_llm.invoke({\"input\": \"Harry was a chubby brown beagle who loved chicken\"})\n",
    "# -> RecordDog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an extraction algorithm. Please extract every possible instance')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| RunnableBinding(bound=OllamaFunctions(llm=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', temperature=0.1), tool_system_prompt_template='You have access to the following tools:\\n\\n{tools}\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{{\\n  \"tool\": <name of the selected tool>,\\n  \"tool_input\": <parameters for the selected tool, matching the tool\\'s JSON schema>\\n}}\\n'), kwargs={'functions': [{'name': '_OutputFormatter', 'description': 'Output formatter. Should always be used to format your response to the user.', 'parameters': {'type': 'object', 'properties': {'output': {'description': 'Record some identifying information about a dog.', 'type': 'object', 'properties': {'name': {'description': \"The dog's name\", 'type': 'string'}, 'color': {'description': \"The dog's color\", 'type': 'string'}, 'fav_food': {'description': \"The dog's favorite food\", 'type': 'string'}}, 'required': ['name', 'color']}}, 'required': ['output']}}], 'function_call': {'name': '_OutputFormatter'}})\n",
       "| PydanticAttrOutputFunctionsParser(pydantic_schema=<class '__main__._create_openai_functions_structured_output_runnable.<locals>._OutputFormatter'>, attr_name='output')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg =_create_openai_functions_structured_output_runnable(\n",
    "            RecordDog,\n",
    "            ollama_func,\n",
    "            prompt,\n",
    "            # output_parser=output_parser,\n",
    "            # enforce_single_function_usage=force_function_usage,\n",
    "            # **kwargs,  # llm-specific kwargs\n",
    "        )\n",
    "# gg.invoke({\"input\": \"Harry was a chubby brown beagle who loved chicken\"})\n",
    "gg\n",
    "# RecordDog,\n",
    "#     ollama_func,\n",
    "#     prompt,\n",
    "#     mode=\"openai-functions\",\n",
    "#     enforce_function_usage=True,\n",
    "#     return_single=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an extraction algorithm. Please extract every possible instance')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| RunnableBinding(bound=OllamaFunctions(llm=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', temperature=0.1), tool_system_prompt_template='You have access to the following tools:\\n\\n{tools}\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{{\\n  \"tool\": <name of the selected tool>,\\n  \"tool_input\": <parameters for the selected tool, matching the tool\\'s JSON schema>\\n}}\\n'), kwargs={'functions': [{'name': '_OutputFormatter', 'description': 'Output formatter. Should always be used to format your response to the user.', 'parameters': {'type': 'object', 'properties': {'output': {'description': 'Record some identifying information about a dog.', 'type': 'object', 'properties': {'name': {'description': \"The dog's name\", 'type': 'string'}, 'color': {'description': \"The dog's color\", 'type': 'string'}, 'fav_food': {'description': \"The dog's favorite food\", 'type': 'string'}}, 'required': ['name', 'color']}}, 'required': ['output']}}], 'function_call': {'name': '_OutputFormatter'}})\n",
       "| PydanticAttrOutputFunctionsParser(pydantic_schema=<class '__main__._OutputFormatter'>, attr_name='output')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class _OutputFormatter(BaseModel):\n",
    "    \"\"\"Output formatter. Should always be used to format your response to the user.\"\"\"  # noqa: E501\n",
    "\n",
    "    output: RecordDog  # type: ignore\n",
    "\n",
    "function = _OutputFormatter\n",
    "output_parser = PydanticAttrOutputFunctionsParser(\n",
    "    pydantic_schema=_OutputFormatter, attr_name=\"output\"\n",
    ")\n",
    "\n",
    "\n",
    "fn_run = create_openai_fn_runnable(\n",
    "        [function],\n",
    "        ollama_func,\n",
    "        prompt,\n",
    "        output_parser=output_parser,\n",
    "        # **llm_kwargs,\n",
    ")\n",
    "# fn_run.invoke({\"input\": \"Harry was a chubby brown beagle who loved chicken\"})\n",
    "fn_run\n",
    "# RecordDog,\n",
    "#             ollama_func,\n",
    "#             prompt,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_openai_fn_runnable\n",
    "enforce_single_function_usage: bool = True\n",
    "llm_kwargs: Any\n",
    "\n",
    "openai_functions = [convert_to_openai_function(f) for f in [function]]\n",
    "llm_kwargs_: Dict[str, Any] = {\"functions\": openai_functions}\n",
    "\n",
    "\n",
    "if len(openai_functions) == 1 and enforce_single_function_usage:\n",
    "    llm_kwargs_[\"function_call\"] = {\"name\": openai_functions[0][\"name\"]}\n",
    "        \n",
    "output_parser = output_parser\n",
    "\n",
    "fn = prompt | llm.bind(**llm_kwargs_) | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'functions': [{'name': '_OutputFormatter',\n",
       "   'description': 'Output formatter. Should always be used to format your response to the user.',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'output': {'description': 'Record some identifying information about a dog.',\n",
       "      'type': 'object',\n",
       "      'properties': {'name': {'description': \"The dog's name\",\n",
       "        'type': 'string'},\n",
       "       'color': {'description': \"The dog's color\", 'type': 'string'},\n",
       "       'fav_food': {'description': \"The dog's favorite food\",\n",
       "        'type': 'string'}},\n",
       "      'required': ['name', 'color']}},\n",
       "    'required': ['output']}}],\n",
       " 'function_call': {'name': '_OutputFormatter'}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_functions = [convert_to_openai_function(f) for f in [function]]\n",
    "\n",
    "llm_kwargs_[\"function_call\"] = {\"name\": openai_functions[0][\"name\"]}\n",
    "\n",
    "llm_kwargs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an extraction algorithm. Please extract every possible instance')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| RunnableBinding(bound=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', temperature=0.1), kwargs={'functions': [{'name': '_OutputFormatter', 'description': 'Output formatter. Should always be used to format your response to the user.', 'parameters': {'type': 'object', 'properties': {'output': {'description': 'Record some identifying information about a dog.', 'type': 'object', 'properties': {'name': {'description': \"The dog's name\", 'type': 'string'}, 'color': {'description': \"The dog's color\", 'type': 'string'}, 'fav_food': {'description': \"The dog's favorite food\", 'type': 'string'}}, 'required': ['name', 'color']}}, 'required': ['output']}}], 'function_call': {'name': '_OutputFormatter'}})\n",
       "| PydanticAttrOutputFunctionsParser(pydantic_schema=<class '__main__._OutputFormatter'>, attr_name='output')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': '_OutputFormatter',\n",
       " 'description': 'Output formatter. Should always be used to format your response to the user.',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'output': {'description': 'Record some identifying information about a dog.',\n",
       "    'type': 'object',\n",
       "    'properties': {'name': {'description': \"The dog's name\", 'type': 'string'},\n",
       "     'color': {'description': \"The dog's color\", 'type': 'string'},\n",
       "     'fav_food': {'description': \"The dog's favorite food\", 'type': 'string'}},\n",
       "    'required': ['name', 'color']}},\n",
       "  'required': ['output']}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_openai_function(function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "model = OllamaFunctions(model=\"mistral:7b-instruct-v0.2-q6_K\")\n",
    "\n",
    "model = model.bind(\n",
    "    functions=[\n",
    "        {'name': '_OutputFormatter',\n",
    " 'description': 'Output formatter. Should always be used to format your response to the user.',\n",
    " 'parameters': {'type': 'object',\n",
    "  'properties': {'output': {'description': 'Record some identifying information about a dog.',\n",
    "    'type': 'object',\n",
    "    'properties': {'name': {'description': \"The dog's name\", 'type': 'string'},\n",
    "     'color': {'description': \"The dog's color\", 'type': 'string'},\n",
    "     'fav_food': {'description': \"The dog's favorite food\", 'type': 'string'}},\n",
    "    'required': ['name', 'color']}},\n",
    "  'required': ['output']}}\n",
    "    ],\n",
    "    function_call={\"name\": \"_OutputFormatter\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn1 = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an extraction algorithm. Please extract every possible instance')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| RunnableBinding(bound=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', temperature=0.1), kwargs={'functions': [{'name': '_OutputFormatter', 'description': 'Output formatter. Should always be used to format your response to the user.', 'parameters': {'type': 'object', 'properties': {'output': {'description': 'Record some identifying information about a dog.', 'type': 'object', 'properties': {'name': {'description': \"The dog's name\", 'type': 'string'}, 'color': {'description': \"The dog's color\", 'type': 'string'}, 'fav_food': {'description': \"The dog's favorite food\", 'type': 'string'}}, 'required': ['name', 'color']}}, 'required': ['output']}}], 'function_call': {'name': '_OutputFormatter'}})\n",
       "| PydanticAttrOutputFunctionsParser(pydantic_schema=<class '__main__._OutputFormatter'>, attr_name='output')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an extraction algorithm. Please extract every possible instance')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| RunnableBinding(bound=OllamaFunctions(llm=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', format='json'), tool_system_prompt_template='You have access to the following tools:\\n\\n{tools}\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{{\\n  \"tool\": <name of the selected tool>,\\n  \"tool_input\": <parameters for the selected tool, matching the tool\\'s JSON schema>\\n}}\\n'), kwargs={'functions': [{'name': '_OutputFormatter', 'description': 'Output formatter. Should always be used to format your response to the user.', 'parameters': {'type': 'object', 'properties': {'output': {'description': 'Record some identifying information about a dog.', 'type': 'object', 'properties': {'name': {'description': \"The dog's name\", 'type': 'string'}, 'color': {'description': \"The dog's color\", 'type': 'string'}, 'fav_food': {'description': \"The dog's favorite food\", 'type': 'string'}}, 'required': ['name', 'color']}}, 'required': ['output']}}], 'function_call': {'name': '_OutputFormatter'}})\n",
       "| PydanticAttrOutputFunctionsParser(pydantic_schema=<class '__main__._OutputFormatter'>, attr_name='output')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecordDog(name='Harry', color='brown', fav_food='chicken')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn1.invoke({\"input\": \"Harry was a chubby brown beagle who loved chicken\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ExecuteCode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m extractor \u001b[38;5;241m=\u001b[39m create_structured_output_runnable(\u001b[43mExecuteCode\u001b[49m, llm, prompt, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai-functions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ExecuteCode' is not defined"
     ]
    }
   ],
   "source": [
    "extractor = create_structured_output_runnable(ExecuteCode, llm, prompt, mode='openai-functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['problem'], input_types={'context': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'context': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\nExecuteCode({{code: \"37593 * 67\"}})\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\nExecuteCode({{code: \"37593**(1/5)\"}})\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['problem'], template='{problem}')), MessagesPlaceholder(variable_name='context', optional=True)])\n",
       "| RunnableBinding(bound=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', temperature=0.1), kwargs={'functions': [{'name': '_OutputFormatter', 'description': 'Output formatter. Should always be used to format your response to the user.', 'parameters': {'type': 'object', 'properties': {'output': {'description': 'The input to the numexpr.evaluate() function.', 'type': 'object', 'properties': {'reasoning': {'description': 'The reasoning behind the code expression, including how context is included, if applicable.', 'type': 'string'}, 'code': {'description': 'The simple code expresssion to execute by numexpr.evaluate().', 'type': 'string'}}, 'required': ['reasoning', 'code']}}, 'required': ['output']}}], 'function_call': {'name': '_OutputFormatter'}})\n",
       "| PydanticAttrOutputFunctionsParser(pydantic_schema=<class '__main__._create_openai_functions_structured_output_runnable.<locals>._OutputFormatter'>, attr_name='output')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying custom code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecordDog(name='Harry', color='brown', fav_food='chicken')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain.chains import create_structured_output_runnable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class RecordDog(BaseModel):\n",
    "    '''Record some identifying information about a dog.'''\n",
    "\n",
    "    name: str = Field(..., description=\"The dog's name\")\n",
    "    color: str = Field(..., description=\"The dog's color\")\n",
    "    fav_food: Optional[str] = Field(None, description=\"The dog's favorite food\")\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an extraction algorithm. Please extract every possible instance\"),\n",
    "        ('human', '{input}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "\n",
    "llm = OllamaFunctions(model='mistral:7b-instruct-v0.2-q6_K')\n",
    "\n",
    "\n",
    "from helpers.structured_output_runnable_ollama_functions import create_structured_output_runnable_custom\n",
    "\n",
    "\n",
    "structured_llm = create_structured_output_runnable_custom(\n",
    "    RecordDog,\n",
    "    llm,\n",
    "    prompt,\n",
    ")\n",
    "\n",
    "# print(structured_llm)\n",
    "\n",
    "structured_llm.invoke({\"input\": \"Harry was a chubby brown beagle who loved chicken\"})\n",
    "# -> RecordDog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
