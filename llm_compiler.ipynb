{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numexpr in /home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages (from numexpr) (1.26.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "import numexpr\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from helpers.structured_output_runnable_ollama_functions import create_structured_output_runnable_custom\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "_MATH_DESCRIPTION = (\n",
    "    \"math(problem: str, context: Optional[list[str]]) -> float:\\n\"\n",
    "    \" - Solves the provided math problem.\\n\"\n",
    "    ' - `problem` can be either a simple math problem (e.g. \"1 + 3\") or a word problem (e.g. \"how many apples are there if there are 3 apples and 2 apples\").\\n'\n",
    "    \" - You cannot calculate multiple expressions in one call. For instance, `math('1 + 3, 2 + 4')` does not work. \"\n",
    "    \"If you need to calculate multiple expressions, you need to call them separately like `math('1 + 3')` and then `math('2 + 4')`\\n\"\n",
    "    \" - Minimize the number of `math` actions as much as possible. For instance, instead of calling \"\n",
    "    '2. math(\"what is the 10% of $1\") and then call 3. math(\"$1 + $2\"), '\n",
    "    'you MUST call 2. math(\"what is the 110% of $1\") instead, which will reduce the number of math actions.\\n'\n",
    "    # Context specific rules below\n",
    "    \" - You can optionally provide a list of strings as `context` to help the agent solve the problem. \"\n",
    "    \"If there are multiple contexts you need to answer the question, you can provide them as a list of strings.\\n\"\n",
    "    \" - `math` action will not see the output of the previous actions unless you provide it as `context`. \"\n",
    "    \"You MUST provide the output of the previous actions as `context` if you need to do math on it.\\n\"\n",
    "    \" - You MUST NEVER provide `search` type action's outputs as a variable in the `problem` argument. \"\n",
    "    \"This is because `search` returns a text blob that contains the information about the entity, not a number or value. \"\n",
    "    \"Therefore, when you need to provide an output of `search` action, you MUST provide it as a `context` argument to `math` action. \"\n",
    "    'For example, 1. search(\"Barack Obama\") and then 2. math(\"age of $1\") is NEVER allowed. '\n",
    "    'Use 2. math(\"age of Barack Obama\", context=[\"$1\"]) instead.\\n'\n",
    "    \" - When you ask a question about `context`, specify the units. \"\n",
    "    'For instance, \"what is xx in height?\" or \"what is xx in millions?\" instead of \"what is xx?\"\\n'\n",
    ")\n",
    "\n",
    "\n",
    "_SYSTEM_PROMPT = \"\"\"Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
    "\n",
    "Question: ${{Question with math problem.}}\n",
    "```text\n",
    "${{single line mathematical expression that solves the problem}}\n",
    "```\n",
    "...numexpr.evaluate(text)...\n",
    "```output\n",
    "${{Output of running the code}}\n",
    "```\n",
    "Answer: ${{Answer}}\n",
    "\n",
    "Begin.\n",
    "\n",
    "Question: What is 37593 * 67?\n",
    "ExecuteCode({{code: \"37593 * 67\"}})\n",
    "...numexpr.evaluate(\"37593 * 67\")...\n",
    "```output\n",
    "2518731\n",
    "```\n",
    "Answer: 2518731\n",
    "\n",
    "Question: 37593^(1/5)\n",
    "ExecuteCode({{code: \"37593**(1/5)\"}})\n",
    "...numexpr.evaluate(\"37593**(1/5)\")...\n",
    "```output\n",
    "8.222831614237718\n",
    "```\n",
    "Answer: 8.222831614237718\n",
    "\"\"\"\n",
    "\n",
    "_ADDITIONAL_CONTEXT_PROMPT = \"\"\"The following additional context is provided from other functions.\\\n",
    "    Use it to substitute into any ${{#}} variables or other words in the problem.\\\n",
    "    \\n\\n${context}\\n\\nNote that context varibles are not defined in code yet.\\\n",
    "You must extract the relevant numbers and directly put them in code.\"\"\"\n",
    "\n",
    "\n",
    "class ExecuteCode(BaseModel):\n",
    "    \"\"\"The input to the numexpr.evaluate() function.\"\"\"\n",
    "\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"The reasoning behind the code expression, including how context is included, if applicable.\",\n",
    "    )\n",
    "\n",
    "    code: str = Field(\n",
    "        ...,\n",
    "        description=\"The simple code expresssion to execute by numexpr.evaluate().\",\n",
    "    )\n",
    "\n",
    "\n",
    "def _evaluate_expression(expression: str) -> str:\n",
    "    try:\n",
    "        local_dict = {\"pi\": math.pi, \"e\": math.e}\n",
    "        output = str(\n",
    "            numexpr.evaluate(\n",
    "                expression.strip(),\n",
    "                global_dict={},  # restrict access to globals\n",
    "                local_dict=local_dict,  # add common mathematical functions\n",
    "            )\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise ValueError(\n",
    "            f'Failed to evaluate \"{expression}\". Raised error: {repr(e)}.'\n",
    "            \" Please try again with a valid numerical expression\"\n",
    "        )\n",
    "\n",
    "    # Remove any leading and trailing brackets from the output\n",
    "    return re.sub(r\"^\\[|\\]$\", \"\", output)\n",
    "\n",
    "\n",
    "def get_math_tool(llm: ChatOllama):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", _SYSTEM_PROMPT),\n",
    "            (\"user\", \"{problem}\"),\n",
    "            MessagesPlaceholder(variable_name=\"context\", optional=True),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    extractor = create_structured_output_runnable_custom(ExecuteCode, llm, prompt)\n",
    "    # extractor = create_structured_output_runnable(ExecuteCode, llm, prompt, mode='openai-functions')\n",
    "\n",
    "    def calculate_expression(\n",
    "        problem: str,\n",
    "        context: Optional[List[str]] = None,\n",
    "        config: Optional[RunnableConfig] = None,\n",
    "    ):\n",
    "        chain_input = {\"problem\": problem}\n",
    "        if context:\n",
    "            context_str = \"\\n\".join(context)\n",
    "            if context_str.strip():\n",
    "                context_str = _ADDITIONAL_CONTEXT_PROMPT.format(\n",
    "                    context=context_str.strip()\n",
    "                )\n",
    "                chain_input[\"context\"] = [SystemMessage(content=context_str)]\n",
    "        code_model = extractor.invoke(chain_input, config)\n",
    "        print(code_model)\n",
    "        try:\n",
    "            return _evaluate_expression(code_model.code)\n",
    "        except Exception as e:\n",
    "            return repr(e)\n",
    "\n",
    "    return StructuredTool.from_function(\n",
    "        name=\"math\",\n",
    "        func=calculate_expression,\n",
    "        description=_MATH_DESCRIPTION,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "# from LLMCompiler.math_tools import get_math_tool\n",
    "\n",
    "\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "llm = OllamaFunctions(model='mistral:7b-instruct-v0.2-q8_0')\n",
    "\n",
    "\n",
    "search = TavilySearchResults(\n",
    "    max_results=1,\n",
    "    description='tavily_search_results_json(query=\"the search query\") - a search engine.',\n",
    ")\n",
    "\n",
    "calculate = get_math_tool(llm)  \n",
    "\n",
    "tools = [search, calculate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)\n",
    "from langchain.globals import set_verbose\n",
    "\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:tool:math] Entering Tool run with input:\n",
      "\u001b[0m\"{'problem': 'What is 5 + 5?'}\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"problem\": \"What is 5 + 5?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"problem\": \"What is 5 + 5?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:OllamaFunctions] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${Question with math problem.}\\n```text\\n${single line mathematical expression that solves the problem}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${Output of running the code}\\n```\\nAnswer: ${Answer}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\nExecuteCode({code: \\\"37593 * 67\\\"})\\n...numexpr.evaluate(\\\"37593 * 67\\\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\nExecuteCode({code: \\\"37593**(1/5)\\\"})\\n...numexpr.evaluate(\\\"37593**(1/5)\\\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nHuman: What is 5 + 5?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You have access to the following tools:\\n\\n[\\n  {\\n    \\\"name\\\": \\\"_OutputFormatter\\\",\\n    \\\"description\\\": \\\"Output formatter. Should always be used to format your response to the user.\\\",\\n    \\\"parameters\\\": {\\n      \\\"type\\\": \\\"object\\\",\\n      \\\"properties\\\": {\\n        \\\"output\\\": {\\n          \\\"description\\\": \\\"The input to the numexpr.evaluate() function.\\\",\\n          \\\"type\\\": \\\"object\\\",\\n          \\\"properties\\\": {\\n            \\\"reasoning\\\": {\\n              \\\"description\\\": \\\"The reasoning behind the code expression, including how context is included, if applicable.\\\",\\n              \\\"type\\\": \\\"string\\\"\\n            },\\n            \\\"code\\\": {\\n              \\\"description\\\": \\\"The simple code expresssion to execute by numexpr.evaluate().\\\",\\n              \\\"type\\\": \\\"string\\\"\\n            }\\n          },\\n          \\\"required\\\": [\\n            \\\"reasoning\\\",\\n            \\\"code\\\"\\n          ]\\n        }\\n      },\\n      \\\"required\\\": [\\n        \\\"output\\\"\\n      ]\\n    }\\n  }\\n]\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{\\n  \\\"tool\\\": <name of the selected tool>,\\n  \\\"tool_input\\\": <parameters for the selected tool, matching the tool's JSON schema>\\n}\\n\\nSystem: Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${Question with math problem.}\\n```text\\n${single line mathematical expression that solves the problem}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${Output of running the code}\\n```\\nAnswer: ${Answer}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\nExecuteCode({code: \\\"37593 * 67\\\"})\\n...numexpr.evaluate(\\\"37593 * 67\\\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\nExecuteCode({code: \\\"37593**(1/5)\\\"})\\n...numexpr.evaluate(\\\"37593**(1/5)\\\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nHuman: What is 5 + 5?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[31;1m\u001b[1;3m[llm/error]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:llm:ChatOllama] [28.99s] LLM run errored with error:\n",
      "\u001b[0m\"KeyboardInterrupt()Traceback (most recent call last):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\\\", line 257, in _generate\\n    final_chunk = self._chat_stream_with_aggregation(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\\\", line 188, in _chat_stream_with_aggregation\\n    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\\\", line 161, in _create_chat_stream\\n    yield from self._create_stream(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py\\\", line 865, in iter_lines\\n    for chunk in self.iter_content(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/utils.py\\\", line 571, in stream_decode_response_unicode\\n    for chunk in iterator:\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py\\\", line 816, in generate\\n    yield from self.raw.stream(chunk_size, decode_content=True)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\\\", line 1030, in stream\\n    yield from self.read_chunked(amt, decode_content=decode_content)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\\\", line 1170, in read_chunked\\n    self._update_chunk_length()\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\\\", line 1098, in _update_chunk_length\\n    line = self._fp.fp.readline()  # type: ignore[union-attr]\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/socket.py\\\", line 705, in readinto\\n    return self._sock.recv_into(b)\\n\\n\\nKeyboardInterrupt\"\n",
      "\u001b[31;1m\u001b[1;3m[llm/error]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:OllamaFunctions] [28.99s] LLM run errored with error:\n",
      "\u001b[0m\"KeyboardInterrupt()Traceback (most recent call last):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py\\\", line 92, in _generate\\n    response_message = self.llm.predict_messages(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 145, in warning_emitting_wrapper\\n    return wrapped(*args, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 747, in predict_messages\\n    return self(messages, stop=_stop, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 145, in warning_emitting_wrapper\\n    return wrapped(*args, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 691, in __call__\\n    generation = self.generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 408, in generate\\n    raise e\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\\\", line 257, in _generate\\n    final_chunk = self._chat_stream_with_aggregation(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\\\", line 188, in _chat_stream_with_aggregation\\n    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\\\", line 161, in _create_chat_stream\\n    yield from self._create_stream(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py\\\", line 865, in iter_lines\\n    for chunk in self.iter_content(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/utils.py\\\", line 571, in stream_decode_response_unicode\\n    for chunk in iterator:\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py\\\", line 816, in generate\\n    yield from self.raw.stream(chunk_size, decode_content=True)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\\\", line 1030, in stream\\n    yield from self.read_chunked(amt, decode_content=decode_content)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\\\", line 1170, in read_chunked\\n    self._update_chunk_length()\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\\\", line 1098, in _update_chunk_length\\n    line = self._fp.fp.readline()  # type: ignore[union-attr]\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/socket.py\\\", line 705, in readinto\\n    return self._sock.recv_into(b)\\n\\n\\nKeyboardInterrupt\"\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [28.99s] Chain run errored with error:\n",
      "\u001b[0m\"KeyboardInterrupt()Traceback (most recent call last):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 4069, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 166, in invoke\\n    self.generate_prompt(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 544, in generate_prompt\\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 408, in generate\\n    raise e\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py\\\", line 92, in _generate\\n    response_message = self.llm.predict_messages(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 145, in warning_emitting_wrapper\\n    return wrapped(*args, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 747, in predict_messages\\n    return self(messages, stop=_stop, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 145, in warning_emitting_wrapper\\n    return wrapped(*args, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 691, in __call__\\n    generation = self.generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 408, in generate\\n    raise e\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\\\", line 257, in _generate\\n    final_chunk = self._chat_stream_with_aggregation(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\\\", line 188, in _chat_stream_with_aggregation\\n    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\\\", line 161, in _create_chat_stream\\n    yield from self._create_stream(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py\\\", line 865, in iter_lines\\n    for chunk in self.iter_content(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/utils.py\\\", line 571, in stream_decode_response_unicode\\n    for chunk in iterator:\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py\\\", line 816, in generate\\n    yield from self.raw.stream(chunk_size, decode_content=True)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\\\", line 1030, in stream\\n    yield from self.read_chunked(amt, decode_content=decode_content)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\\\", line 1170, in read_chunked\\n    self._update_chunk_length()\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\\\", line 1098, in _update_chunk_length\\n    line = self._fp.fp.readline()  # type: ignore[union-attr]\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/socket.py\\\", line 705, in readinto\\n    return self._sock.recv_into(b)\\n\\n\\nKeyboardInterrupt\"\n",
      "\u001b[31;1m\u001b[1;3m[tool/error]\u001b[0m \u001b[1m[1:tool:math] [29.00s] \u001b[0mTool run errored with error:\n",
      "KeyboardInterrupt()Traceback (most recent call last):\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/tools.py\", line 376, in run\n",
      "    self._run(*tool_args, run_manager=run_manager, **tool_kwargs)\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/tools.py\", line 701, in _run\n",
      "    else self.func(*args, **kwargs)\n",
      "\n",
      "\n",
      "  File \"/tmp/ipykernel_7705/3126306044.py\", line 137, in calculate_expression\n",
      "    code_model = extractor.invoke(chain_input, config)\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2075, in invoke\n",
      "    input = step.invoke(\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4069, in invoke\n",
      "    return self.bound.invoke(\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 166, in invoke\n",
      "    self.generate_prompt(\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 544, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 408, in generate\n",
      "    raise e\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 398, in generate\n",
      "    self._generate_with_cache(\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 577, in _generate_with_cache\n",
      "    return self._generate(\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py\", line 92, in _generate\n",
      "    response_message = self.llm.predict_messages(\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 747, in predict_messages\n",
      "    return self(messages, stop=_stop, **kwargs)\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 691, in __call__\n",
      "    generation = self.generate(\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 408, in generate\n",
      "    raise e\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 398, in generate\n",
      "    self._generate_with_cache(\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 577, in _generate_with_cache\n",
      "    return self._generate(\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\", line 257, in _generate\n",
      "    final_chunk = self._chat_stream_with_aggregation(\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\", line 188, in _chat_stream_with_aggregation\n",
      "    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\", line 161, in _create_chat_stream\n",
      "    yield from self._create_stream(\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py\", line 865, in iter_lines\n",
      "    for chunk in self.iter_content(\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/utils.py\", line 571, in stream_decode_response_unicode\n",
      "    for chunk in iterator:\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py\", line 816, in generate\n",
      "    yield from self.raw.stream(chunk_size, decode_content=True)\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\", line 1030, in stream\n",
      "    yield from self.read_chunked(amt, decode_content=decode_content)\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\", line 1170, in read_chunked\n",
      "    self._update_chunk_length()\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\", line 1098, in _update_chunk_length\n",
      "    line = self._fp.fp.readline()  # type: ignore[union-attr]\n",
      "\n",
      "\n",
      "  File \"/home/amaithi/anaconda3/envs/lang/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcalculate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproblem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is 5 + 5?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# \"context\": [\"The tempreature of sf is 33 degrees\"],\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/tools.py:240\u001b[0m, in \u001b[0;36mBaseTool.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, Dict],\n\u001b[1;32m    236\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    239\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/tools.py:419\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;28mstr\u001b[39m(observation), color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    423\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/tools.py:376\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    373\u001b[0m     parsed_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_input(tool_input)\n\u001b[1;32m    374\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[1;32m    375\u001b[0m     observation \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 376\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_validation_error:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/tools.py:701\u001b[0m, in \u001b[0;36mStructuredTool._run\u001b[0;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc:\n\u001b[1;32m    693\u001b[0m     new_argument_supported \u001b[38;5;241m=\u001b[39m signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\n\u001b[1;32m    696\u001b[0m             \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    697\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    698\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    699\u001b[0m         )\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_argument_supported\n\u001b[0;32m--> 701\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m     )\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool does not support sync\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 137\u001b[0m, in \u001b[0;36mget_math_tool.<locals>.calculate_expression\u001b[0;34m(problem, context, config)\u001b[0m\n\u001b[1;32m    133\u001b[0m         context_str \u001b[38;5;241m=\u001b[39m _ADDITIONAL_CONTEXT_PROMPT\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    134\u001b[0m             context\u001b[38;5;241m=\u001b[39mcontext_str\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    135\u001b[0m         )\n\u001b[1;32m    136\u001b[0m         chain_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [SystemMessage(content\u001b[38;5;241m=\u001b[39mcontext_str)]\n\u001b[0;32m--> 137\u001b[0m code_model \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchain_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(code_model)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:4069\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4065\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4066\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4067\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:166\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    163\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    165\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    175\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py:92\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 92\u001b[0m response_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m chat_generation_content \u001b[38;5;241m=\u001b[39m response_message\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_generation_content, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:747\u001b[0m, in \u001b[0;36mBaseChatModel.predict_messages\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    746\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[0;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:691\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1.7\u001b[39m\u001b[38;5;124m\"\u001b[39m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvoke\u001b[39m\u001b[38;5;124m\"\u001b[39m, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    690\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 691\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py:257\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    235\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    239\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    240\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m            ])\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    265\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext),\n\u001b[1;32m    266\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mgeneration_info,\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[chat_generation])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py:188\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    186\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    187\u001b[0m     final_chunk: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[1;32m    190\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _chat_stream_response_to_chat_generation_chunk(stream_resp)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py:161\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_chat_stream\u001b[39m(\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    154\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m    155\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    157\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    158\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_messages_to_ollama_messages(messages),\n\u001b[1;32m    160\u001b[0m     }\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[1;32m    162\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload, stop\u001b[38;5;241m=\u001b[39mstop, api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    163\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py:865\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(\n\u001b[1;32m    866\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, decode_unicode\u001b[38;5;241m=\u001b[39mdecode_unicode\n\u001b[1;32m    867\u001b[0m ):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    870\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m pending \u001b[38;5;241m+\u001b[39m chunk\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/requests/utils.py:571\u001b[0m, in \u001b[0;36mstream_decode_response_unicode\u001b[0;34m(iterator, r)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    570\u001b[0m decoder \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetincrementaldecoder(r\u001b[38;5;241m.\u001b[39mencoding)(errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m    572\u001b[0m     rv \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(chunk)\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rv:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py:1030\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1030\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py:1170\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1170\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1172\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py:1098\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "calculate.invoke(\n",
    "    {\n",
    "        \"problem\": \"What is 5 + 5?\",\n",
    "        # \"context\": [\"The tempreature of sf is 33 degrees\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='math', description='math(problem: str, context: Optional[List[str]] = None, config: Optional[langchain_core.runnables.config.RunnableConfig] = None) - math(problem: str, context: Optional[list[str]]) -> float:\\n - Solves the provided math problem.\\n - `problem` can be either a simple math problem (e.g. \"1 + 3\") or a word problem (e.g. \"how many apples are there if there are 3 apples and 2 apples\").\\n - You cannot calculate multiple expressions in one call. For instance, `math(\\'1 + 3, 2 + 4\\')` does not work. If you need to calculate multiple expressions, you need to call them separately like `math(\\'1 + 3\\')` and then `math(\\'2 + 4\\')`\\n - Minimize the number of `math` actions as much as possible. For instance, instead of calling 2. math(\"what is the 10% of $1\") and then call 3. math(\"$1 + $2\"), you MUST call 2. math(\"what is the 110% of $1\") instead, which will reduce the number of math actions.\\n - You can optionally provide a list of strings as `context` to help the agent solve the problem. If there are multiple contexts you need to answer the question, you can provide them as a list of strings.\\n - `math` action will not see the output of the previous actions unless you provide it as `context`. You MUST provide the output of the previous actions as `context` if you need to do math on it.\\n - You MUST NEVER provide `search` type action\\'s outputs as a variable in the `problem` argument. This is because `search` returns a text blob that contains the information about the entity, not a number or value. Therefore, when you need to provide an output of `search` action, you MUST provide it as a `context` argument to `math` action. For example, 1. search(\"Barack Obama\") and then 2. math(\"age of $1\") is NEVER allowed. Use 2. math(\"age of Barack Obama\", context=[\"$1\"]) instead.\\n - When you ask a question about `context`, specify the units. For instance, \"what is xx in height?\" or \"what is xx in millions?\" instead of \"what is xx?\"', args_schema=<class 'pydantic.main.mathSchema'>, func=<function get_math_tool.<locals>.calculate_expression at 0x7fec4d20f370>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### math Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "import numexpr\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "_MATH_DESCRIPTION = (\n",
    "    \"math(problem: str, context: Optional[list[str]]) -> float:\\n\"\n",
    "    \" - Solves the provided math problem.\\n\"\n",
    "    ' - `problem` can be either a simple math problem (e.g. \"1 + 3\") or a word problem (e.g. \"how many apples are there if there are 3 apples and 2 apples\").\\n'\n",
    "    \" - You cannot calculate multiple expressions in one call. For instance, `math('1 + 3, 2 + 4')` does not work. \"\n",
    "    \"If you need to calculate multiple expressions, you need to call them separately like `math('1 + 3')` and then `math('2 + 4')`\\n\"\n",
    "    \" - Minimize the number of `math` actions as much as possible. For instance, instead of calling \"\n",
    "    '2. math(\"what is the 10% of $1\") and then call 3. math(\"$1 + $2\"), '\n",
    "    'you MUST call 2. math(\"what is the 110% of $1\") instead, which will reduce the number of math actions.\\n'\n",
    "    # Context specific rules below\n",
    "    \" - You can optionally provide a list of strings as `context` to help the agent solve the problem. \"\n",
    "    \"If there are multiple contexts you need to answer the question, you can provide them as a list of strings.\\n\"\n",
    "    \" - `math` action will not see the output of the previous actions unless you provide it as `context`. \"\n",
    "    \"You MUST provide the output of the previous actions as `context` if you need to do math on it.\\n\"\n",
    "    \" - You MUST NEVER provide `search` type action's outputs as a variable in the `problem` argument. \"\n",
    "    \"This is because `search` returns a text blob that contains the information about the entity, not a number or value. \"\n",
    "    \"Therefore, when you need to provide an output of `search` action, you MUST provide it as a `context` argument to `math` action. \"\n",
    "    'For example, 1. search(\"Barack Obama\") and then 2. math(\"age of $1\") is NEVER allowed. '\n",
    "    'Use 2. math(\"age of Barack Obama\", context=[\"$1\"]) instead.\\n'\n",
    "    \" - When you ask a question about `context`, specify the units. \"\n",
    "    'For instance, \"what is xx in height?\" or \"what is xx in millions?\" instead of \"what is xx?\"\\n'\n",
    ")\n",
    "\n",
    "\n",
    "_SYSTEM_PROMPT = \"\"\"Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
    "\n",
    "Question: ${{Question with math problem.}}\n",
    "```text\n",
    "${{single line mathematical expression that solves the problem}}\n",
    "```\n",
    "...numexpr.evaluate(text)...\n",
    "```output\n",
    "${{Output of running the code}}\n",
    "```\n",
    "Answer: ${{Answer}}\n",
    "\n",
    "Begin.\n",
    "\n",
    "Question: What is 37593 * 67?\n",
    "ExecuteCode({{code: \"37593 * 67\"}})\n",
    "...numexpr.evaluate(\"37593 * 67\")...\n",
    "```output\n",
    "2518731\n",
    "```\n",
    "Answer: 2518731\n",
    "\n",
    "Question: 37593^(1/5)\n",
    "ExecuteCode({{code: \"37593**(1/5)\"}})\n",
    "...numexpr.evaluate(\"37593**(1/5)\")...\n",
    "```output\n",
    "8.222831614237718\n",
    "```\n",
    "Answer: 8.222831614237718\n",
    "\"\"\"\n",
    "\n",
    "_ADDITIONAL_CONTEXT_PROMPT = \"\"\"The following additional context is provided from other functions.\\\n",
    "    Use it to substitute into any ${{#}} variables or other words in the problem.\\\n",
    "    \\n\\n${context}\\n\\nNote that context varibles are not defined in code yet.\\\n",
    "You must extract the relevant numbers and directly put them in code.\"\"\"\n",
    "\n",
    "\n",
    "class ExecuteCode(BaseModel):\n",
    "    \"\"\"The input to the numexpr.evaluate() function.\"\"\"\n",
    "\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"The reasoning behind the code expression, including how context is included, if applicable.\",\n",
    "    )\n",
    "\n",
    "    code: str = Field(\n",
    "        ...,\n",
    "        description=\"The simple code expresssion to execute by numexpr.evaluate().\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _evaluate_expression(expression: str) -> str:\n",
    "    try:\n",
    "        local_dict = {\"pi\": math.pi, \"e\": math.e}\n",
    "        output = str(\n",
    "            numexpr.evaluate(\n",
    "                expression.strip(),\n",
    "                global_dict={},  # restrict access to globals\n",
    "                local_dict=local_dict,  # add common mathematical functions\n",
    "            )\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise ValueError(\n",
    "            f'Failed to evaluate \"{expression}\". Raised error: {repr(e)}.'\n",
    "            \" Please try again with a valid numerical expression\"\n",
    "        )\n",
    "\n",
    "    # Remove any leading and trailing brackets from the output\n",
    "    return re.sub(r\"^\\[|\\]$\", \"\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_math_tool(llm):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", _SYSTEM_PROMPT),\n",
    "            (\"user\", \"{problem}\"),\n",
    "            MessagesPlaceholder(variable_name=\"context\", optional=True),\n",
    "        ]\n",
    "    )\n",
    "    extractor = create_structured_output_runnable(ExecuteCode, llm, prompt, \n",
    "                                                  mode='openai-functions',\n",
    "                                                  enforce_function_usage=True)\n",
    "    \n",
    "    print(extractor)\n",
    "\n",
    "    def calculate_expression(\n",
    "        problem: str,\n",
    "        context: Optional[List[str]] = None,\n",
    "        config: Optional[RunnableConfig] = None,\n",
    "    ):\n",
    "        chain_input = {\"problem\": problem}\n",
    "        if context:\n",
    "            context_str = \"\\n\".join(context)\n",
    "            if context_str.strip():\n",
    "                context_str = _ADDITIONAL_CONTEXT_PROMPT.format(\n",
    "                    context=context_str.strip()\n",
    "                )\n",
    "                chain_input[\"context\"] = [SystemMessage(content=context_str)]\n",
    "        print(chain_input)\n",
    "        code_model = extractor.invoke(chain_input, config)\n",
    "        print(code_model)\n",
    "        \n",
    "        try:\n",
    "            return _evaluate_expression(code_model.code)\n",
    "        except Exception as e:\n",
    "            return repr(e)\n",
    "\n",
    "    return StructuredTool.from_function(\n",
    "        name=\"math\",\n",
    "        func=calculate_expression,\n",
    "        description=_MATH_DESCRIPTION,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatPromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m\n\u001b[1;32m     14\u001b[0m ollama_func \u001b[38;5;241m=\u001b[39m OllamaFunctions(llm \u001b[38;5;241m=\u001b[39m llm)\n\u001b[1;32m     17\u001b[0m search \u001b[38;5;241m=\u001b[39m TavilySearchResults(\n\u001b[1;32m     18\u001b[0m     max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     19\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtavily_search_results_json(query=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe search query\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) - a search engine.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m calculate \u001b[38;5;241m=\u001b[39m \u001b[43mget_math_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mollama_func\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     24\u001b[0m tools \u001b[38;5;241m=\u001b[39m [search, calculate]\n",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m, in \u001b[0;36mget_math_tool\u001b[0;34m(llm)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_math_tool\u001b[39m(llm):\n\u001b[0;32m----> 2\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[43mChatPromptTemplate\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[1;32m      3\u001b[0m         [\n\u001b[1;32m      4\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, _SYSTEM_PROMPT),\n\u001b[1;32m      5\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{problem}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      6\u001b[0m             MessagesPlaceholder(variable_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      7\u001b[0m         ]\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m      9\u001b[0m     extractor \u001b[38;5;241m=\u001b[39m create_structured_output_runnable(ExecuteCode, llm, prompt, \n\u001b[1;32m     10\u001b[0m                                                   mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai-functions\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m                                                   enforce_function_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(extractor)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ChatPromptTemplate' is not defined"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "# from LLMCompiler.math_tools import get_math_tool\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', temperature=0.1)\n",
    "\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "ollama_func = OllamaFunctions(llm = llm)\n",
    "\n",
    "\n",
    "search = TavilySearchResults(\n",
    "    max_results=1,\n",
    "    description='tavily_search_results_json(query=\"the search query\") - a search engine.',\n",
    ")\n",
    "\n",
    "calculate = get_math_tool(ollama_func)  \n",
    "\n",
    "tools = [search, calculate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcalculate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproblem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms the temp of sf + 5?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThet empreature of sf is 33 degrees\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/tools.py:240\u001b[0m, in \u001b[0;36mBaseTool.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, Dict],\n\u001b[1;32m    236\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    239\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/tools.py:419\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;28mstr\u001b[39m(observation), color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    423\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/tools.py:376\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    373\u001b[0m     parsed_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_input(tool_input)\n\u001b[1;32m    374\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[1;32m    375\u001b[0m     observation \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 376\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_validation_error:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/tools.py:701\u001b[0m, in \u001b[0;36mStructuredTool._run\u001b[0;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc:\n\u001b[1;32m    693\u001b[0m     new_argument_supported \u001b[38;5;241m=\u001b[39m signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\n\u001b[1;32m    696\u001b[0m             \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    697\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    698\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    699\u001b[0m         )\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_argument_supported\n\u001b[0;32m--> 701\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m     )\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool does not support sync\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/LLMCompiler/math_tools.py:145\u001b[0m, in \u001b[0;36mget_math_tool.<locals>.calculate_expression\u001b[0;34m(problem, context, config)\u001b[0m\n\u001b[1;32m    139\u001b[0m         context_str \u001b[38;5;241m=\u001b[39m _ADDITIONAL_CONTEXT_PROMPT\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    140\u001b[0m             context\u001b[38;5;241m=\u001b[39mcontext_str\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    141\u001b[0m         )\n\u001b[1;32m    142\u001b[0m         chain_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [SystemMessage(content\u001b[38;5;241m=\u001b[39mcontext_str)]\n\u001b[0;32m--> 145\u001b[0m code_model \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchain_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(code_model)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:4069\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4065\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4066\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4067\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:166\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    163\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    165\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    175\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py:92\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 92\u001b[0m response_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m chat_generation_content \u001b[38;5;241m=\u001b[39m response_message\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_generation_content, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:747\u001b[0m, in \u001b[0;36mBaseChatModel.predict_messages\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    746\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[0;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:691\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1.7\u001b[39m\u001b[38;5;124m\"\u001b[39m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvoke\u001b[39m\u001b[38;5;124m\"\u001b[39m, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    690\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 691\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py:257\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    235\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    239\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    240\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m            ])\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    265\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext),\n\u001b[1;32m    266\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mgeneration_info,\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[chat_generation])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py:188\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    186\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    187\u001b[0m     final_chunk: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[1;32m    190\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _chat_stream_response_to_chat_generation_chunk(stream_resp)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py:161\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_chat_stream\u001b[39m(\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    154\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m    155\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    157\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    158\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_messages_to_ollama_messages(messages),\n\u001b[1;32m    160\u001b[0m     }\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[1;32m    162\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload, stop\u001b[38;5;241m=\u001b[39mstop, api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    163\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py:865\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(\n\u001b[1;32m    866\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, decode_unicode\u001b[38;5;241m=\u001b[39mdecode_unicode\n\u001b[1;32m    867\u001b[0m ):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    870\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m pending \u001b[38;5;241m+\u001b[39m chunk\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/requests/utils.py:571\u001b[0m, in \u001b[0;36mstream_decode_response_unicode\u001b[0;34m(iterator, r)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    570\u001b[0m decoder \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetincrementaldecoder(r\u001b[38;5;241m.\u001b[39mencoding)(errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m    572\u001b[0m     rv \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(chunk)\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rv:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py:1030\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1030\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py:1170\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1170\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1172\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py:1098\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "calculate.invoke(\n",
    "    {\n",
    "        \"problem\": \"What's the temp of sf + 5?\",\n",
    "        \"context\": [\"Thet empreature of sf is 33 degrees\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', temperature=0.1)\n",
    "\n",
    "ollama_func = OllamaFunctions(llm = llm)\n",
    "\n",
    "\n",
    "extractor = create_structured_output_runnable(ExecuteCode, ollama_func, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Extract information about any dogs mentioned in the user input.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| RunnableBinding(bound=OllamaFunctions(llm=ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', temperature=0.1), tool_system_prompt_template='You have access to the following tools:\\n\\n{tools}\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{{\\n  \"tool\": <name of the selected tool>,\\n  \"tool_input\": <parameters for the selected tool, matching the tool\\'s JSON schema>\\n}}\\n'), kwargs={'functions': [{'name': '_OutputFormatter', 'description': 'Output formatter. Should always be used to format your response to the user.', 'parameters': {'type': 'object', 'properties': {'output': {'description': 'The input to the numexpr.evaluate() function.', 'type': 'object', 'properties': {'reasoning': {'description': 'The reasoning behind the code expression, including how context is included, if applicable.', 'type': 'string'}, 'code': {'description': 'The simple code expresssion to execute by numexpr.evaluate().', 'type': 'string'}}, 'required': ['reasoning', 'code']}}, 'required': ['output']}}], 'function_call': {'name': '_OutputFormatter'}})\n",
       "| PydanticAttrOutputFunctionsParser(pydantic_schema=<class 'langchain.chains.structured_output.base._create_openai_functions_structured_output_runnable.<locals>._OutputFormatter'>, attr_name='output')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {'problem': \"What's the temp of sf + 5?\", 'context': ['Thet empreature of sf is 33 degrees']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extractor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mextractor\u001b[49m\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the temp of sf + 5?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThet empreature of sf is 33 degrees\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extractor' is not defined"
     ]
    }
   ],
   "source": [
    "extractor.invoke({'input': \"What's the temp of sf + 5?\", 'context': ['Thet empreature of sf is 33 degrees']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "funcutions example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Harry was a chubby brown beagle who loved chicken\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Harry was a chubby brown beagle who loved chicken\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:OllamaFunctions] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Extract information about any dogs mentioned in the user input.\\nHuman: Harry was a chubby brown beagle who loved chicken\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You have access to the following tools:\\n\\n[\\n  {\\n    \\\"name\\\": \\\"_OutputFormatter\\\",\\n    \\\"description\\\": \\\"Output formatter. Should always be used to format your response to the user.\\\",\\n    \\\"parameters\\\": {\\n      \\\"type\\\": \\\"object\\\",\\n      \\\"properties\\\": {\\n        \\\"output\\\": {\\n          \\\"description\\\": \\\"Identifying information about a dog.\\\",\\n          \\\"type\\\": \\\"object\\\",\\n          \\\"properties\\\": {\\n            \\\"name\\\": {\\n              \\\"description\\\": \\\"The dog's name\\\",\\n              \\\"type\\\": \\\"string\\\"\\n            },\\n            \\\"color\\\": {\\n              \\\"description\\\": \\\"The dog's color\\\",\\n              \\\"type\\\": \\\"string\\\"\\n            },\\n            \\\"fav_food\\\": {\\n              \\\"description\\\": \\\"The dog's favorite food\\\",\\n              \\\"type\\\": \\\"string\\\"\\n            }\\n          },\\n          \\\"required\\\": [\\n            \\\"name\\\",\\n            \\\"color\\\"\\n          ]\\n        }\\n      },\\n      \\\"required\\\": [\\n        \\\"output\\\"\\n      ]\\n    }\\n  }\\n]\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{\\n  \\\"tool\\\": <name of the selected tool>,\\n  \\\"tool_input\\\": <parameters for the selected tool, matching the tool's JSON schema>\\n}\\n\\nSystem: Extract information about any dogs mentioned in the user input.\\nHuman: Harry was a chubby brown beagle who loved chicken\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:llm:ChatOllama] [3.23s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" {\\n  \\\"tool\\\": \\\"_OutputFormatter\\\",\\n  \\\"tool_input\\\": {\\n    \\\"output\\\": {\\n      \\\"name\\\": \\\"Harry\\\",\\n      \\\"color\\\": \\\"brown\\\",\\n      \\\"fav_food\\\": \\\"chicken\\\"\\n    }\\n  }\\n}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"mistral:7b-instruct-v0.2-q6_K\",\n",
      "          \"created_at\": \"2024-03-11T05:01:51.538210211Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 3223720719,\n",
      "          \"load_duration\": 1309873927,\n",
      "          \"prompt_eval_count\": 341,\n",
      "          \"prompt_eval_duration\": 451450000,\n",
      "          \"eval_count\": 63,\n",
      "          \"eval_duration\": 1461021000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \" {\\n  \\\"tool\\\": \\\"_OutputFormatter\\\",\\n  \\\"tool_input\\\": {\\n    \\\"output\\\": {\\n      \\\"name\\\": \\\"Harry\\\",\\n      \\\"color\\\": \\\"brown\\\",\\n      \\\"fav_food\\\": \\\"chicken\\\"\\n    }\\n  }\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:OllamaFunctions] [3.23s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"function_call\": {\n",
      "                \"name\": \"_OutputFormatter\",\n",
      "                \"arguments\": \"{\\\"output\\\": {\\\"name\\\": \\\"Harry\\\", \\\"color\\\": \\\"brown\\\", \\\"fav_food\\\": \\\"chicken\\\"}}\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 5:parser:PydanticAttrOutputFunctionsParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 5:parser:PydanticAttrOutputFunctionsParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [3.23s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dog(name='Harry', color='brown', fav_food='chicken')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from typing import Optional\n",
    "\n",
    "from langchain.chains import create_structured_output_runnable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Dog(BaseModel):\n",
    "    '''Identifying information about a dog.'''\n",
    "\n",
    "    name: str = Field(..., description=\"The dog's name\")\n",
    "    color: str = Field(..., description=\"The dog's color\")\n",
    "    fav_food: Optional[str] = Field(None, description=\"The dog's favorite food\")\n",
    "\n",
    "llm = ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', temperature=0.1)\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "ollama_func = OllamaFunctions(llm = llm)\n",
    "\n",
    "\n",
    "structured_llm = create_structured_output_runnable(Dog, ollama_func, mode=\"openai-functions\")\n",
    "system = '''Extract information about any dogs mentioned in the user input.'''\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system), (\"human\", \"{input}\"),]\n",
    ")\n",
    "chain = prompt | structured_llm\n",
    "chain.invoke({\"input\": \"Harry was a chubby brown beagle who loved chicken\"})\n",
    "# -> Dog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Alex is 5 feet tall. Claudia is 1 feet taller than Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:OllamaFunctions] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Extract and save the relevant entities mentioned in the following passage together with their properties.\\n\\nOnly extract the properties mentioned in the 'information_extraction' function.\\n\\nIf a property is not present and is not required in the function parameters, do not include it in the output.\\n\\nPassage:\\nAlex is 5 feet tall. Claudia is 1 feet taller than Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LLMChain > 3:llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You have access to the following tools:\\n\\n[\\n  {\\n    \\\"name\\\": \\\"information_extraction\\\",\\n    \\\"description\\\": \\\"Extracts the relevant information from the passage.\\\",\\n    \\\"parameters\\\": {\\n      \\\"type\\\": \\\"object\\\",\\n      \\\"properties\\\": {\\n        \\\"info\\\": {\\n          \\\"type\\\": \\\"array\\\",\\n          \\\"items\\\": {\\n            \\\"type\\\": \\\"object\\\",\\n            \\\"properties\\\": {\\n              \\\"name\\\": {\\n                \\\"title\\\": \\\"name\\\",\\n                \\\"type\\\": \\\"string\\\"\\n              },\\n              \\\"height\\\": {\\n                \\\"title\\\": \\\"height\\\",\\n                \\\"type\\\": \\\"integer\\\"\\n              },\\n              \\\"hair_color\\\": {\\n                \\\"title\\\": \\\"hair_color\\\",\\n                \\\"type\\\": \\\"string\\\"\\n              }\\n            },\\n            \\\"required\\\": [\\n              \\\"name\\\",\\n              \\\"height\\\"\\n            ]\\n          }\\n        }\\n      },\\n      \\\"required\\\": [\\n        \\\"info\\\"\\n      ]\\n    }\\n  }\\n]\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{\\n  \\\"tool\\\": <name of the selected tool>,\\n  \\\"tool_input\\\": <parameters for the selected tool, matching the tool's JSON schema>\\n}\\n\\nHuman: Extract and save the relevant entities mentioned in the following passage together with their properties.\\n\\nOnly extract the properties mentioned in the 'information_extraction' function.\\n\\nIf a property is not present and is not required in the function parameters, do not include it in the output.\\n\\nPassage:\\nAlex is 5 feet tall. Claudia is 1 feet taller than Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 3:llm:ChatOllama] [2.42s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"{\\n  \\\"tool\\\": \\\"information_extraction\\\",\\n  \\\"tool_input\\\": {\\n    \\\"info\\\": [\\n      {\\n        \\\"name\\\": \\\"Alex\\\",\\n        \\\"height\\\": 5\\n      },\\n      {\\n        \\\"name\\\": \\\"Claudia\\\",\\n        \\\"height\\\": 6\\n      }\\n    ]\\n  }\\n}\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"mistral:7b-instruct-v0.2-q6_K\",\n",
      "          \"created_at\": \"2024-03-11T05:13:13.055599642Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 2417134129,\n",
      "          \"load_duration\": 249136,\n",
      "          \"prompt_eval_count\": 385,\n",
      "          \"prompt_eval_duration\": 549029000,\n",
      "          \"eval_count\": 79,\n",
      "          \"eval_duration\": 1866173000\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"{\\n  \\\"tool\\\": \\\"information_extraction\\\",\\n  \\\"tool_input\\\": {\\n    \\\"info\\\": [\\n      {\\n        \\\"name\\\": \\\"Alex\\\",\\n        \\\"height\\\": 5\\n      },\\n      {\\n        \\\"name\\\": \\\"Claudia\\\",\\n        \\\"height\\\": 6\\n      }\\n    ]\\n  }\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LLMChain > 2:llm:OllamaFunctions] [2.42s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"function_call\": {\n",
      "                \"name\": \"information_extraction\",\n",
      "                \"arguments\": \"{\\\"info\\\": [{\\\"name\\\": \\\"Alex\\\", \\\"height\\\": 5}, {\\\"name\\\": \\\"Claudia\\\", \\\"height\\\": 6}]}\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LLMChain] [2.42s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": [\n",
      "    {\n",
      "      \"name\": \"Alex\",\n",
      "      \"height\": 5\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Claudia\",\n",
      "      \"height\": 6\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'Alex', 'height': 5}, {'name': 'Claudia', 'height': 6}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_extraction_chain\n",
    "\n",
    "# Schema\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\"},\n",
    "        \"height\": {\"type\": \"integer\"},\n",
    "        \"hair_color\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"name\", \"height\"],\n",
    "}\n",
    "\n",
    "# Input\n",
    "input = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller than Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\"\"\n",
    "\n",
    "# Run chain\n",
    "llm = OllamaFunctions(model=\"mistral:7b-instruct-v0.2-q6_K\", temperature=0)\n",
    "chain = create_extraction_chain(schema, llm)\n",
    "chain.run(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lass Dog(BaseModel):\n",
    "    '''Identifying information about a dog.'''\n",
    "\n",
    "    name: str = Field(..., description=\"The dog's name\")\n",
    "    color: str = Field(..., description=\"The dog's color\")\n",
    "    fav_food: Optional[str] = Field(None, description=\"The dog's favorite food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{'problem': \"What's the temp of sf + 5?\", 'context': ['Thet empreature of sf is 33 degrees']}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'problem': \"What's the temp of sf + 5?\", 'context': [SystemMessage(content='The following additional context is provided from other functions.    Use it to substitute into any ${#} variables or other words in the problem.    \\n\\n$Thet empreature of sf is 33 degrees\\n\\nNote that context varibles are not defined in code yet.You must extract the relevant numbers and directly put them in code.')]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "_SYSTEM_PROMPT = \"\"\"Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
    "\n",
    "Question: ${{Question with math problem.}}\n",
    "```text\n",
    "${{single line mathematical expression that solves the problem}}\n",
    "```\n",
    "...numexpr.evaluate(text)...\n",
    "```output\n",
    "${{Output of running the code}}\n",
    "```\n",
    "Answer: ${{Answer}}\n",
    "\n",
    "Begin.\n",
    "\n",
    "Question: What is 37593 * 67?\n",
    "ExecuteCode({{code: \"37593 * 67\"}})\n",
    "...numexpr.evaluate(\"37593 * 67\")...\n",
    "```output\n",
    "2518731\n",
    "```\n",
    "Answer: 2518731\n",
    "\n",
    "Question: 37593^(1/5)\n",
    "ExecuteCode({{code: \"37593**(1/5)\"}})\n",
    "...numexpr.evaluate(\"37593**(1/5)\")...\n",
    "```output\n",
    "8.222831614237718\n",
    "```\n",
    "Answer: 8.222831614237718\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ExecuteCode(BaseModel):\n",
    "    \"\"\"The input to the numexpr.evaluate() function.\"\"\"\n",
    "\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"The reasoning behind the code expression, including how context is included, if applicable.\",\n",
    "    )\n",
    "\n",
    "    code: str = Field(\n",
    "        ...,\n",
    "        description=\"The simple code expresssion to execute by numexpr.evaluate().\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"problem\": \"What's the temp of sf + 5?\",\n",
      "  \"context\": [\n",
      "    \"The tempreature of sf is 33 degrees\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"problem\": \"What's the temp of sf + 5?\",\n",
      "  \"context\": [\n",
      "    \"The tempreature of sf is 33 degrees\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:OllamaFunctions] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${Question with math problem.}\\n```text\\n${single line mathematical expression that solves the problem}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${Output of running the code}\\n```\\nAnswer: ${Answer}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\nExecuteCode({code: \\\"37593 * 67\\\"})\\n...numexpr.evaluate(\\\"37593 * 67\\\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\nExecuteCode({code: \\\"37593**(1/5)\\\"})\\n...numexpr.evaluate(\\\"37593**(1/5)\\\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nHuman: What's the temp of sf + 5?\\nHuman: The tempreature of sf is 33 degrees\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You have access to the following tools:\\n\\n[\\n  {\\n    \\\"name\\\": \\\"_OutputFormatter\\\",\\n    \\\"description\\\": \\\"Output formatter. Should always be used to format your response to the user.\\\",\\n    \\\"parameters\\\": {\\n      \\\"type\\\": \\\"object\\\",\\n      \\\"properties\\\": {\\n        \\\"output\\\": {\\n          \\\"description\\\": \\\"The input to the numexpr.evaluate() function.\\\",\\n          \\\"type\\\": \\\"object\\\",\\n          \\\"properties\\\": {\\n            \\\"reasoning\\\": {\\n              \\\"description\\\": \\\"The reasoning behind the code expression, including how context is included, if applicable.\\\",\\n              \\\"type\\\": \\\"string\\\"\\n            },\\n            \\\"code\\\": {\\n              \\\"description\\\": \\\"The simple code expresssion to execute by numexpr.evaluate().\\\",\\n              \\\"type\\\": \\\"string\\\"\\n            }\\n          },\\n          \\\"required\\\": [\\n            \\\"reasoning\\\",\\n            \\\"code\\\"\\n          ]\\n        }\\n      },\\n      \\\"required\\\": [\\n        \\\"output\\\"\\n      ]\\n    }\\n  }\\n]\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{\\n  \\\"tool\\\": <name of the selected tool>,\\n  \\\"tool_input\\\": <parameters for the selected tool, matching the tool's JSON schema>\\n}\\n\\nSystem: Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${Question with math problem.}\\n```text\\n${single line mathematical expression that solves the problem}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${Output of running the code}\\n```\\nAnswer: ${Answer}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\nExecuteCode({code: \\\"37593 * 67\\\"})\\n...numexpr.evaluate(\\\"37593 * 67\\\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\nExecuteCode({code: \\\"37593**(1/5)\\\"})\\n...numexpr.evaluate(\\\"37593**(1/5)\\\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nHuman: What's the temp of sf + 5?\\nHuman: The tempreature of sf is 33 degrees\"\n",
      "  ]\n",
      "}\n",
      "\u001b[31;1m\u001b[1;3m[llm/error]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:llm:ChatOllama] [159.59s] LLM run errored with error:\n",
      "\u001b[0m\"KeyboardInterrupt()Traceback (most recent call last):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\\\", line 257, in _generate\\n    final_chunk = self._chat_stream_with_aggregation(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\\\", line 188, in _chat_stream_with_aggregation\\n    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\\\", line 161, in _create_chat_stream\\n    yield from self._create_stream(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py\\\", line 865, in iter_lines\\n    for chunk in self.iter_content(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/utils.py\\\", line 571, in stream_decode_response_unicode\\n    for chunk in iterator:\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py\\\", line 816, in generate\\n    yield from self.raw.stream(chunk_size, decode_content=True)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\\\", line 1030, in stream\\n    yield from self.read_chunked(amt, decode_content=decode_content)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\\\", line 1170, in read_chunked\\n    self._update_chunk_length()\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\\\", line 1098, in _update_chunk_length\\n    line = self._fp.fp.readline()  # type: ignore[union-attr]\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/socket.py\\\", line 705, in readinto\\n    return self._sock.recv_into(b)\\n\\n\\nKeyboardInterrupt\"\n",
      "\u001b[31;1m\u001b[1;3m[llm/error]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:OllamaFunctions] [159.60s] LLM run errored with error:\n",
      "\u001b[0m\"KeyboardInterrupt()Traceback (most recent call last):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py\\\", line 92, in _generate\\n    response_message = self.llm.predict_messages(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 145, in warning_emitting_wrapper\\n    return wrapped(*args, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 747, in predict_messages\\n    return self(messages, stop=_stop, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 145, in warning_emitting_wrapper\\n    return wrapped(*args, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 691, in __call__\\n    generation = self.generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 408, in generate\\n    raise e\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\\\", line 257, in _generate\\n    final_chunk = self._chat_stream_with_aggregation(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\\\", line 188, in _chat_stream_with_aggregation\\n    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\\\", line 161, in _create_chat_stream\\n    yield from self._create_stream(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py\\\", line 865, in iter_lines\\n    for chunk in self.iter_content(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/utils.py\\\", line 571, in stream_decode_response_unicode\\n    for chunk in iterator:\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py\\\", line 816, in generate\\n    yield from self.raw.stream(chunk_size, decode_content=True)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\\\", line 1030, in stream\\n    yield from self.read_chunked(amt, decode_content=decode_content)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\\\", line 1170, in read_chunked\\n    self._update_chunk_length()\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\\\", line 1098, in _update_chunk_length\\n    line = self._fp.fp.readline()  # type: ignore[union-attr]\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/socket.py\\\", line 705, in readinto\\n    return self._sock.recv_into(b)\\n\\n\\nKeyboardInterrupt\"\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [159.60s] Chain run errored with error:\n",
      "\u001b[0m\"KeyboardInterrupt()Traceback (most recent call last):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 2075, in invoke\\n    input = step.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py\\\", line 4069, in invoke\\n    return self.bound.invoke(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 166, in invoke\\n    self.generate_prompt(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 544, in generate_prompt\\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 408, in generate\\n    raise e\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py\\\", line 92, in _generate\\n    response_message = self.llm.predict_messages(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 145, in warning_emitting_wrapper\\n    return wrapped(*args, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 747, in predict_messages\\n    return self(messages, stop=_stop, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 145, in warning_emitting_wrapper\\n    return wrapped(*args, **kwargs)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 691, in __call__\\n    generation = self.generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 408, in generate\\n    raise e\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 398, in generate\\n    self._generate_with_cache(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\\\", line 577, in _generate_with_cache\\n    return self._generate(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\\\", line 257, in _generate\\n    final_chunk = self._chat_stream_with_aggregation(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\\\", line 188, in _chat_stream_with_aggregation\\n    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py\\\", line 161, in _create_chat_stream\\n    yield from self._create_stream(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py\\\", line 865, in iter_lines\\n    for chunk in self.iter_content(\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/utils.py\\\", line 571, in stream_decode_response_unicode\\n    for chunk in iterator:\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py\\\", line 816, in generate\\n    yield from self.raw.stream(chunk_size, decode_content=True)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\\\", line 1030, in stream\\n    yield from self.read_chunked(amt, decode_content=decode_content)\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\\\", line 1170, in read_chunked\\n    self._update_chunk_length()\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py\\\", line 1098, in _update_chunk_length\\n    line = self._fp.fp.readline()  # type: ignore[union-attr]\\n\\n\\n  File \\\"/home/amaithi/anaconda3/envs/lang/lib/python3.10/socket.py\\\", line 705, in readinto\\n    return self._sock.recv_into(b)\\n\\n\\nKeyboardInterrupt\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 36\u001b[0m\n\u001b[1;32m     29\u001b[0m structured_llm \u001b[38;5;241m=\u001b[39m create_structured_output_runnable(ExecuteCode, ollama_func, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai-functions\u001b[39m\u001b[38;5;124m\"\u001b[39m, prompt\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# system = '''Extract information about any dogs mentioned in the user input.'''\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# prompt = ChatPromptTemplate.from_messages(\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#     [(\"system\", system), (\"human\", \"{input}\"),]\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# extractor = prompt | structured_llm\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mstructured_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproblem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms the temp of sf + 5?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe tempreature of sf is 33 degrees\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# -> Dog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:4069\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4065\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4066\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4067\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:166\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    163\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    165\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    175\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py:92\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 92\u001b[0m response_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m chat_generation_content \u001b[38;5;241m=\u001b[39m response_message\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_generation_content, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:747\u001b[0m, in \u001b[0;36mBaseChatModel.predict_messages\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    746\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[0;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:691\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1.7\u001b[39m\u001b[38;5;124m\"\u001b[39m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvoke\u001b[39m\u001b[38;5;124m\"\u001b[39m, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    690\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 691\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py:257\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    235\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    239\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    240\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m            ])\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    265\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext),\n\u001b[1;32m    266\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mgeneration_info,\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[chat_generation])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py:188\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    186\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    187\u001b[0m     final_chunk: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[1;32m    190\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _chat_stream_response_to_chat_generation_chunk(stream_resp)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py:161\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_chat_stream\u001b[39m(\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    154\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m    155\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    157\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    158\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_messages_to_ollama_messages(messages),\n\u001b[1;32m    160\u001b[0m     }\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[1;32m    162\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload, stop\u001b[38;5;241m=\u001b[39mstop, api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    163\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py:865\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(\n\u001b[1;32m    866\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, decode_unicode\u001b[38;5;241m=\u001b[39mdecode_unicode\n\u001b[1;32m    867\u001b[0m ):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    870\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m pending \u001b[38;5;241m+\u001b[39m chunk\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/requests/utils.py:571\u001b[0m, in \u001b[0;36mstream_decode_response_unicode\u001b[0;34m(iterator, r)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    570\u001b[0m decoder \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetincrementaldecoder(r\u001b[38;5;241m.\u001b[39mencoding)(errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m    572\u001b[0m     rv \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(chunk)\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rv:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py:1030\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1030\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py:1170\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1170\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1172\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/urllib3/response.py:1098\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Optional\n",
    "\n",
    "from langchain.chains import create_structured_output_runnable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "class Dog(BaseModel):\n",
    "    '''Identifying information about a dog.'''\n",
    "\n",
    "    name: str = Field(..., description=\"The dog's name\")\n",
    "    color: str = Field(..., description=\"The dog's color\")\n",
    "    fav_food: Optional[str] = Field(None, description=\"The dog's favorite food\")\n",
    "\n",
    "# llm = ChatOllama(model='mistral:7b-instruct-v0.2-q6_K', temperature=0.1)\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "ollama_func = OllamaFunctions(model='mistral:7b-instruct-v0.2-q6_K')\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", _SYSTEM_PROMPT),\n",
    "            (\"user\", \"{problem}\"),\n",
    "            MessagesPlaceholder(variable_name=\"context\", optional=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "structured_llm = create_structured_output_runnable(ExecuteCode, ollama_func, mode=\"openai-functions\", prompt=prompt)\n",
    "\n",
    "# system = '''Extract information about any dogs mentioned in the user input.'''\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [(\"system\", system), (\"human\", \"{input}\"),]\n",
    "# )\n",
    "# extractor = prompt | structured_llm\n",
    "structured_llm.invoke({\n",
    "        \"problem\": \"What's the temp of sf + 5?\",\n",
    "        \"context\": [\"The tempreature of sf is 33 degrees\"],\n",
    "    })\n",
    "# -> Dog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.11.17)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/amaithi/anaconda3/envs/lang/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=ed85b2014ae4896b034f4f5197634e87c0a96eaa75366d5eefd6153e7c3ed0ad\n",
      "  Stored in directory: /home/amaithi/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet  duckduckgo-search\n",
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# Imported from the https://github.com/langchain-ai/langgraph/tree/main/examples/plan-and-execute repo\n",
    "# from math_tools import get_math_tool\n",
    "\n",
    "\n",
    "# _get_pass(\"TAVILY_API_KEY\")\n",
    "import os \n",
    "os.environ['TAVILY_API_KEY'] = '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'\n",
    "\n",
    "# calculate = get_math_tool(llm='mistral:7b-instruct-v0.2-q6_K')\n",
    "\n",
    "search = TavilySearchResults(\n",
    "    max_results=1,\n",
    "    description='tavily_search_results_json(query=\"the search query\") - a search engine.',\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "duck = DuckDuckGoSearchRun()\n",
    "\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)\n",
    "\n",
    "class WikiInputs(BaseModel):\n",
    "    \"\"\"Inputs to the wikipedia tool.\"\"\"\n",
    "\n",
    "    query: str = Field(\n",
    "        description=\"query to look up in Wikipedia, should be 3 or less words\"\n",
    "    )\n",
    "    \n",
    "\n",
    "wiki = WikipediaQueryRun(\n",
    "    name=\"wikipedia_search\",\n",
    "    description=\"wikipedia_search - to search things in wikipedia\",\n",
    "    args_schema=WikiInputs,\n",
    "    api_wrapper=api_wrapper,\n",
    "    return_direct=True,\n",
    ")\n",
    "\n",
    "\n",
    "tools = [search, wiki]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia_search'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plannaer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following \u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m types:\n",
      "\u001b[33;1m\u001b[1;3m{tool_descriptions}\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m. join(): Collects and combines results from prior actions.\n",
      "\n",
      " - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\n",
      " - join should always be the last action in the plan, and will be called in two scenarios:\n",
      "   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\n",
      "   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\n",
      " - Each action described above contains input/output types and description.\n",
      "    - You must strictly adhere to the input and output types for each action.\n",
      "    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\n",
      " - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\n",
      " - Each action MUST have a unique ID, which is strictly increasing.\n",
      " - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\n",
      " - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\n",
      " - Ensure the plan maximizes parallelizability.\n",
      " - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\n",
      " - Never introduce new actions other than the ones provided.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{messages}\u001b[0m\n",
      "\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Remember, ONLY respond with the task list in the correct format! E.g.:\n",
      "idx. tool(arg_name=args)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "from LLMCompiler.output_parser import LLMCompilerPlanParser, Task\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "prompt = hub.pull(\"wfh/llm-compiler\")\n",
    "print(prompt.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['messages'] input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]} partial_variables={'replan': '', 'num_tools': 2, 'tool_descriptions': '0. tavily_search_results_json(query=\"the search query\") - a search engine.\\n\\n 1. wikipedia_search - to search things in wikipedia\\n'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['num_tools', 'tool_descriptions'], template=\"Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following {num_tools} types:\\n{tool_descriptions}\\n{num_tools}. join(): Collects and combines results from prior actions.\\n\\n - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\\n - join should always be the last action in the plan, and will be called in two scenarios:\\n   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\\n   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\\n - Each action described above contains input/output types and description.\\n    - You must strictly adhere to the input and output types for each action.\\n    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\\n - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\\n - Each action MUST have a unique ID, which is strictly increasing.\\n - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\\n - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\\n - Ensure the plan maximizes parallelizability.\\n - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\\n - Never introduce new actions other than the ones provided.\")), MessagesPlaceholder(variable_name='messages'), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Remember, ONLY respond with the task list in the correct format! E.g.:\\nidx. tool(arg_name=args)'))]\n"
     ]
    }
   ],
   "source": [
    "tool_descriptions = \"\\n \".join(\n",
    "        f\"{i}. {tool.description}\\n\" for i, tool in enumerate(tools)\n",
    "    )\n",
    "\n",
    "planner_prompt = prompt.partial(\n",
    "    replan=\"\",\n",
    "    num_tools=len(tools),\n",
    "    tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "\n",
    "print(planner_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_planner(\n",
    "    llm: BaseChatModel, tools: Sequence[BaseTool], base_prompt: ChatPromptTemplate\n",
    "):\n",
    "    tool_descriptions = \"\\n\".join(\n",
    "        f\"{i}. {tool.description}\\n\" for i, tool in enumerate(tools)\n",
    "    )\n",
    "    \n",
    "    planner_prompt = base_prompt.partial(\n",
    "        replan=\"\",\n",
    "        num_tools=len(tools),\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "    print(planner_prompt.pretty_print())\n",
    "    \n",
    "    replanner_prompt = base_prompt.partial(\n",
    "        replan=' - You are given \"Previous Plan\" which is the plan that the previous agent created along with the execution results '\n",
    "        \"(given as Observation) of each plan and a general thought (given as Thought) about the executed results.\"\n",
    "        'You MUST use these information to create the next plan under \"Current Plan\".\\n'\n",
    "        ' - When starting the Current Plan, you should start with \"Thought\" that outlines the strategy for the next plan.\\n'\n",
    "        \" - In the Current Plan, you should NEVER repeat the actions that are already executed in the Previous Plan.\\n\"\n",
    "        \" - You must continue the task index from the end of the previous one. Do not repeat task indices.\",\n",
    "        num_tools=len(tools),\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "\n",
    "    def should_replan(state: list):\n",
    "        # Context is passed as a system message\n",
    "        return isinstance(state[-1], SystemMessage)\n",
    "\n",
    "    def wrap_messages(state: list):\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    def wrap_and_get_last_index(state: list):\n",
    "        next_task = 0\n",
    "        for message in state[::-1]:\n",
    "            if isinstance(message, FunctionMessage):\n",
    "                next_task = message.additional_kwargs[\"idx\"] + 1\n",
    "                break\n",
    "        state[-1].content = state[-1].content + f\" - Begin counting at : {next_task}\"\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    return (\n",
    "        RunnableBranch(\n",
    "            (should_replan, wrap_and_get_last_index | replanner_prompt),\n",
    "            wrap_messages | planner_prompt,\n",
    "        )\n",
    "        | llm\n",
    "        | LLMCompilerPlanParser(tools=tools)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following \u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m types:\n",
      "\u001b[33;1m\u001b[1;3m{tool_descriptions}\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m. join(): Collects and combines results from prior actions.\n",
      "\n",
      " - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\n",
      " - join should always be the last action in the plan, and will be called in two scenarios:\n",
      "   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\n",
      "   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\n",
      " - Each action described above contains input/output types and description.\n",
      "    - You must strictly adhere to the input and output types for each action.\n",
      "    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\n",
      " - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\n",
      " - Each action MUST have a unique ID, which is strictly increasing.\n",
      " - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\n",
      " - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\n",
      " - Ensure the plan maximizes parallelizability.\n",
      " - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\n",
      " - Never introduce new actions other than the ones provided.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{messages}\u001b[0m\n",
      "\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Remember, ONLY respond with the task list in the correct format! E.g.:\n",
      "idx. tool(arg_name=args)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "lllm = ChatOllama(model='mistral:7b-instruct-v0.2-q6_K')\n",
    "\n",
    "planner = create_planner(lllm, tools, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TavilySearchResults(description='tavily_search_results_json(query=\"the search query\") - a search engine.', max_results=1),\n",
       " StructuredTool(name='math', description='math(problem: str, context: Optional[List[str]] = None, config: Optional[langchain_core.runnables.config.RunnableConfig] = None) - math(problem: str, context: Optional[list[str]]) -> float:\\n - Solves the provided math problem.\\n - `problem` can be either a simple math problem (e.g. \"1 + 3\") or a word problem (e.g. \"how many apples are there if there are 3 apples and 2 apples\").\\n - You cannot calculate multiple expressions in one call. For instance, `math(\\'1 + 3, 2 + 4\\')` does not work. If you need to calculate multiple expressions, you need to call them separately like `math(\\'1 + 3\\')` and then `math(\\'2 + 4\\')`\\n - Minimize the number of `math` actions as much as possible. For instance, instead of calling 2. math(\"what is the 10% of $1\") and then call 3. math(\"$1 + $2\"), you MUST call 2. math(\"what is the 110% of $1\") instead, which will reduce the number of math actions.\\n - You can optionally provide a list of strings as `context` to help the agent solve the problem. If there are multiple contexts you need to answer the question, you can provide them as a list of strings.\\n - `math` action will not see the output of the previous actions unless you provide it as `context`. You MUST provide the output of the previous actions as `context` if you need to do math on it.\\n - You MUST NEVER provide `search` type action\\'s outputs as a variable in the `problem` argument. This is because `search` returns a text blob that contains the information about the entity, not a number or value. Therefore, when you need to provide an output of `search` action, you MUST provide it as a `context` argument to `math` action. For example, 1. search(\"Barack Obama\") and then 2. math(\"age of $1\") is NEVER allowed. Use 2. math(\"age of Barack Obama\", context=[\"$1\"]) instead.\\n - When you ask a question about `context`, specify the units. For instance, \"what is xx in height?\" or \"what is xx in millions?\" instead of \"what is xx?\"', args_schema=<class 'pydantic.v1.main.mathSchema'>, func=<function get_math_tool.<locals>.calculate_expression at 0x7f8e4293d3a0>)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join ()\n",
      "---\n",
      "name='math' description='math(problem: str, context: Optional[List[str]] = None, config: Optional[langchain_core.runnables.config.RunnableConfig] = None) - math(problem: str, context: Optional[list[str]]) -> float:\\n - Solves the provided math problem.\\n - `problem` can be either a simple math problem (e.g. \"1 + 3\") or a word problem (e.g. \"how many apples are there if there are 3 apples and 2 apples\").\\n - You cannot calculate multiple expressions in one call. For instance, `math(\\'1 + 3, 2 + 4\\')` does not work. If you need to calculate multiple expressions, you need to call them separately like `math(\\'1 + 3\\')` and then `math(\\'2 + 4\\')`\\n - Minimize the number of `math` actions as much as possible. For instance, instead of calling 2. math(\"what is the 10% of $1\") and then call 3. math(\"$1 + $2\"), you MUST call 2. math(\"what is the 110% of $1\") instead, which will reduce the number of math actions.\\n - You can optionally provide a list of strings as `context` to help the agent solve the problem. If there are multiple contexts you need to answer the question, you can provide them as a list of strings.\\n - `math` action will not see the output of the previous actions unless you provide it as `context`. You MUST provide the output of the previous actions as `context` if you need to do math on it.\\n - You MUST NEVER provide `search` type action\\'s outputs as a variable in the `problem` argument. This is because `search` returns a text blob that contains the information about the entity, not a number or value. Therefore, when you need to provide an output of `search` action, you MUST provide it as a `context` argument to `math` action. For example, 1. search(\"Barack Obama\") and then 2. math(\"age of $1\") is NEVER allowed. Use 2. math(\"age of Barack Obama\", context=[\"$1\"]) instead.\\n - When you ask a question about `context`, specify the units. For instance, \"what is xx in height?\" or \"what is xx in millions?\" instead of \"what is xx?\"' args_schema=<class 'pydantic.v1.main.mathSchema'> func=<function get_math_tool.<locals>.calculate_expression at 0x7f8e4293d3a0> {'problem': 'extract latitude and longitude from $1', 'context': '$0'}\n",
      "---\n",
      "join ()\n",
      "---\n",
      "name='math' description='math(problem: str, context: Optional[List[str]] = None, config: Optional[langchain_core.runnables.config.RunnableConfig] = None) - math(problem: str, context: Optional[list[str]]) -> float:\\n - Solves the provided math problem.\\n - `problem` can be either a simple math problem (e.g. \"1 + 3\") or a word problem (e.g. \"how many apples are there if there are 3 apples and 2 apples\").\\n - You cannot calculate multiple expressions in one call. For instance, `math(\\'1 + 3, 2 + 4\\')` does not work. If you need to calculate multiple expressions, you need to call them separately like `math(\\'1 + 3\\')` and then `math(\\'2 + 4\\')`\\n - Minimize the number of `math` actions as much as possible. For instance, instead of calling 2. math(\"what is the 10% of $1\") and then call 3. math(\"$1 + $2\"), you MUST call 2. math(\"what is the 110% of $1\") instead, which will reduce the number of math actions.\\n - You can optionally provide a list of strings as `context` to help the agent solve the problem. If there are multiple contexts you need to answer the question, you can provide them as a list of strings.\\n - `math` action will not see the output of the previous actions unless you provide it as `context`. You MUST provide the output of the previous actions as `context` if you need to do math on it.\\n - You MUST NEVER provide `search` type action\\'s outputs as a variable in the `problem` argument. This is because `search` returns a text blob that contains the information about the entity, not a number or value. Therefore, when you need to provide an output of `search` action, you MUST provide it as a `context` argument to `math` action. For example, 1. search(\"Barack Obama\") and then 2. math(\"age of $1\") is NEVER allowed. Use 2. math(\"age of Barack Obama\", context=[\"$1\"]) instead.\\n - When you ask a question about `context`, specify the units. For instance, \"what is xx in height?\" or \"what is xx in millions?\" instead of \"what is xx?\"' args_schema=<class 'pydantic.v1.main.mathSchema'> func=<function get_math_tool.<locals>.calculate_expression at 0x7f8e4293d3a0> {'problem': 'calculate distance between current location and Pondicherry using latitudes and longitudes', 'context': '[str($2), \"current location\\'s latitude and longitude\"]'}\n",
      "---\n",
      "join ()\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "example_question = \"Where does pondicherry located?\"\n",
    "\n",
    "for task in planner.stream([HumanMessage(content=example_question)]):\n",
    "    print(task[\"tool\"], task[\"args\"])\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task fetching Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Union, Iterable, List, Tuple, Dict\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.runnables import (\n",
    "    chain as as_runnable,\n",
    ")\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "import time\n",
    "\n",
    "\n",
    "def _get_observations(messages: List[BaseMessage]) -> Dict[int, Any]:\n",
    "    # Get all previous tool responses\n",
    "    results = {}\n",
    "    for message in messages[::-1]:\n",
    "        if isinstance(message, FunctionMessage):\n",
    "            results[int(message.additional_kwargs[\"idx\"])] = message.content\n",
    "    return results\n",
    "\n",
    "\n",
    "class SchedulerInput(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    tasks: Iterable[Task]\n",
    "\n",
    "\n",
    "def _execute_task(task, observations, config):\n",
    "    tool_to_use = task[\"tool\"]\n",
    "    if isinstance(tool_to_use, str):\n",
    "        return tool_to_use\n",
    "    args = task[\"args\"]\n",
    "    try:\n",
    "        if isinstance(args, str):\n",
    "            resolved_args = _resolve_arg(args, observations)\n",
    "        elif isinstance(args, dict):\n",
    "            resolved_args = {\n",
    "                key: _resolve_arg(val, observations) for key, val in args.items()\n",
    "            }\n",
    "        else:\n",
    "            # This will likely fail\n",
    "            resolved_args = args\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.)\"\n",
    "            f\" Args could not be resolved. Error: {repr(e)}\"\n",
    "        )\n",
    "    try:\n",
    "        return tool_to_use.invoke(resolved_args, config)\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.\"\n",
    "            + f\" Args resolved to {resolved_args}. Error: {repr(e)})\"\n",
    "        )\n",
    "\n",
    "\n",
    "def _resolve_arg(arg: Union[str, Any], observations: Dict[int, Any]):\n",
    "    if isinstance(arg, str) and arg.startswith(\"$\"):\n",
    "        try:\n",
    "            stripped = arg[1:].replace(\".output\", \"\").strip(\"{}\")\n",
    "            idx = int(stripped)\n",
    "        except Exception:\n",
    "            return str(arg)\n",
    "        return str(observations[idx])\n",
    "    elif isinstance(arg, list):\n",
    "        return [_resolve_arg(a, observations) for a in arg]\n",
    "    else:\n",
    "        return str(arg)\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_task(task_inputs, config):\n",
    "    task: Task = task_inputs[\"task\"]\n",
    "    observations: Dict[int, Any] = task_inputs[\"observations\"]\n",
    "    try:\n",
    "        observation = _execute_task(task, observations, config)\n",
    "    except Exception:\n",
    "        import traceback\n",
    "\n",
    "        observation = traceback.format_exception()  # repr(e) +\n",
    "    observations[task[\"idx\"]] = observation\n",
    "\n",
    "\n",
    "def schedule_pending_task(\n",
    "    task: Task, observations: Dict[int, Any], retry_after: float = 0.2\n",
    "):\n",
    "    while True:\n",
    "        deps = task[\"dependencies\"]\n",
    "        if deps and (any([dep not in observations for dep in deps])):\n",
    "            # Dependencies not yet satisfied\n",
    "            time.sleep(retry_after)\n",
    "            continue\n",
    "        schedule_task.invoke({\"task\": task, \"observations\": observations})\n",
    "        break\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_tasks(scheduler_input: SchedulerInput) -> List[FunctionMessage]:\n",
    "    \"\"\"Group the tasks into a DAG schedule.\"\"\"\n",
    "    # For streaming, we are making a few simplifying assumption:\n",
    "    # 1. The LLM does not create cyclic dependencies\n",
    "    # 2. That the LLM will not generate tasks with future deps\n",
    "    # If this ceases to be a good assumption, you can either\n",
    "    # adjust to do a proper topological sort (not-stream)\n",
    "    # or use a more complicated data structure\n",
    "    tasks = scheduler_input[\"tasks\"]\n",
    "    messages = scheduler_input[\"messages\"]\n",
    "    # If we are re-planning, we may have calls that depend on previous\n",
    "    # plans. Start with those.\n",
    "    observations = _get_observations(messages)\n",
    "    task_names = {}\n",
    "    originals = set(observations)\n",
    "    # ^^ We assume each task inserts a different key above to\n",
    "    # avoid race conditions...\n",
    "    futures = []\n",
    "    retry_after = 0.25  # Retry every quarter second\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for task in tasks:\n",
    "            deps = task[\"dependencies\"]\n",
    "            task_names[task[\"idx\"]] = (\n",
    "                task[\"tool\"] if isinstance(task[\"tool\"], str) else task[\"tool\"].name\n",
    "            )\n",
    "            if (\n",
    "                # Depends on other tasks\n",
    "                deps\n",
    "                and (any([dep not in observations for dep in deps]))\n",
    "            ):\n",
    "                futures.append(\n",
    "                    executor.submit(\n",
    "                        schedule_pending_task, task, observations, retry_after\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # No deps or all deps satisfied\n",
    "                # can schedule now\n",
    "                schedule_task.invoke(dict(task=task, observations=observations))\n",
    "                # futures.append(executor.submit(schedule_task.invoke dict(task=task, observations=observations)))\n",
    "\n",
    "        # All tasks have been submitted or enqueued\n",
    "        # Wait for them to complete\n",
    "        wait(futures)\n",
    "    # Convert observations to new tool messages to add to the state\n",
    "    new_observations = {\n",
    "        k: (task_names[k], observations[k])\n",
    "        for k in sorted(observations.keys() - originals)\n",
    "    }\n",
    "    tool_messages = [\n",
    "        FunctionMessage(name=name, content=str(obs), additional_kwargs={\"idx\": k})\n",
    "        for k, (name, obs) in new_observations.items()\n",
    "    ]\n",
    "    return tool_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def plan_and_schedule(messages: List[BaseMessage], config):\n",
    "    tasks = planner.stream(messages, config)\n",
    "    # Begin executing the planner immediately\n",
    "    tasks = itertools.chain([next(tasks)], tasks)\n",
    "    scheduled_tasks = schedule_tasks.invoke(\n",
    "        {\n",
    "            \"messages\": messages,\n",
    "            \"tasks\": tasks,\n",
    "        },\n",
    "        config,\n",
    "    )\n",
    "    return scheduled_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_messages = plan_and_schedule.invoke([HumanMessage(content=example_question)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FunctionMessage(content='join', additional_kwargs={'idx': 1}, name='join')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "\n",
    "class FinalResponse(BaseModel):\n",
    "    \"\"\"The final response/answer.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Replan(BaseModel):\n",
    "    feedback: str = Field(\n",
    "        description=\"Analysis of the previous attempts and recommendations on what needs to be fixed.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class JoinOutputs(BaseModel):\n",
    "    \"\"\"Decide whether to replan or whether you can return the final response.\"\"\"\n",
    "\n",
    "    thought: str = Field(\n",
    "        description=\"The chain of thought reasoning for the selected action\"\n",
    "    )\n",
    "    action: Union[FinalResponse, Replan]\n",
    "\n",
    "\n",
    "joiner_prompt = hub.pull(\"wfh/llm-compiler-joiner\").partial(\n",
    "    examples=\"\"\n",
    ")  # You can optionally add examples\n",
    "# llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"mistral:7b-instruct-v0.2-q6_K\")\n",
    "llm = OllamaFunctions(llm=model)\n",
    "from langchain.chains import create_extraction_chain\n",
    "\n",
    "runnable = create_structured_output_runnable(JoinOutputs, llm, joiner_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_joiner_output(decision: JoinOutputs) -> List[BaseMessage]:\n",
    "    response = [AIMessage(content=f\"Thought: {decision.thought}\")]\n",
    "    if isinstance(decision.action, Replan):\n",
    "        return response + [\n",
    "            SystemMessage(\n",
    "                content=f\"Context from last attempt: {decision.action.feedback}\"\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        return response + [AIMessage(content=decision.action.response)]\n",
    "\n",
    "\n",
    "def select_recent_messages(messages: list) -> dict:\n",
    "    selected = []\n",
    "    for msg in messages[::-1]:\n",
    "        selected.append(msg)\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            break\n",
    "\n",
    "        c = {\"messages\": selected[::-1]}\n",
    "    return {\"messages\": selected[::-1]}\n",
    "\n",
    "\n",
    "joiner = select_recent_messages | runnable | _parse_joiner_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [HumanMessage(content=example_question)] + tool_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Where does pondicherry located?'),\n",
       " FunctionMessage(content='join', additional_kwargs={'idx': 1}, name='join')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Received unsupported message type for Ollama.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjoiner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_messages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:4069\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4065\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4066\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4067\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:166\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    163\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    165\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    175\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_experimental/llms/ollama_functions.py:92\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 92\u001b[0m response_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m chat_generation_content \u001b[38;5;241m=\u001b[39m response_message\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_generation_content, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:747\u001b[0m, in \u001b[0;36mBaseChatModel.predict_messages\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    746\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[0;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:691\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1.7\u001b[39m\u001b[38;5;124m\"\u001b[39m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvoke\u001b[39m\u001b[38;5;124m\"\u001b[39m, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    690\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 691\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:257\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    235\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    239\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    240\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m            ])\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    265\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext),\n\u001b[1;32m    266\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mgeneration_info,\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[chat_generation])\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:188\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    186\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    187\u001b[0m     final_chunk: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chat_stream_response_to_chat_generation_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:159\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_chat_stream\u001b[39m(\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    154\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m    155\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    157\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    158\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_messages_to_ollama_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    160\u001b[0m     }\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[1;32m    162\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload, stop\u001b[38;5;241m=\u001b[39mstop, api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    163\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:112\u001b[0m, in \u001b[0;36mChatOllama._convert_messages_to_ollama_messages\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m    110\u001b[0m     role \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived unsupported message type for Ollama.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m images \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: Received unsupported message type for Ollama."
     ]
    }
   ],
   "source": [
    "joiner.invoke(input_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compose using LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessageGraph, END\n",
    "from typing import Dict\n",
    "\n",
    "graph_builder = MessageGraph()\n",
    "\n",
    "# 1.  Define vertices\n",
    "# We defined plan_and_schedule above already\n",
    "# Assign each node to a state variable to update\n",
    "graph_builder.add_node(\"plan_and_schedule\", plan_and_schedule)\n",
    "graph_builder.add_node(\"join\", joiner)\n",
    "\n",
    "\n",
    "## Define edges\n",
    "graph_builder.add_edge(\"plan_and_schedule\", \"join\")\n",
    "\n",
    "### This condition determines looping logic\n",
    "\n",
    "\n",
    "def should_continue(state: List[BaseMessage]):\n",
    "    if isinstance(state[-1], AIMessage):\n",
    "        return END\n",
    "    return \"plan_and_schedule\"\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    start_key=\"join\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    condition=should_continue,\n",
    ")\n",
    "graph_builder.set_entry_point(\"plan_and_schedule\")\n",
    "chain = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan_and_schedule': [FunctionMessage(content=\"ERROR(Failed to call wikipedia_search with args {}. Args resolved to {}. Error: ValidationError(model='WikiInputs', errors=[{'loc': ('query',), 'msg': 'field required', 'type': 'value_error.missing'}]))\", additional_kwargs={'idx': 1}, name='wikipedia_search')]}\n",
      "---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Received unsupported message type for Ollama.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m chain\u001b[38;5;241m.\u001b[39mstream([HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the GDP of New York?\u001b[39m\u001b[38;5;124m\"\u001b[39m)]):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(step)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py:615\u001b[0m, in \u001b[0;36mPregel.transform\u001b[0;34m(self, input, config, output_keys, input_keys, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    614\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]]:\n\u001b[0;32m--> 615\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[1;32m    616\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[1;32m    618\u001b[0m         config,\n\u001b[1;32m    619\u001b[0m         output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m    620\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    622\u001b[0m     ):\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:1513\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1513\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m   1515\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py:355\u001b[0m, in \u001b[0;36mPregel._transform\u001b[0;34m(self, input, run_manager, config, input_keys, output_keys, interrupt)\u001b[0m\n\u001b[1;32m    348\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    349\u001b[0m     futures,\n\u001b[1;32m    350\u001b[0m     return_when\u001b[38;5;241m=\u001b[39mconcurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m    351\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m    352\u001b[0m )\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# interrupt on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m \u001b[43m_interrupt_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# apply writes to channels\u001b[39;00m\n\u001b[1;32m    358\u001b[0m _apply_writes(\n\u001b[1;32m    359\u001b[0m     checkpoint, channels, pending_writes, config, step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    360\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langgraph/pregel/__init__.py:698\u001b[0m, in \u001b[0;36m_interrupt_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m    696\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m--> 698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;66;03m# TODO this is where retry of an entire step would happen\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:4069\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4065\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4066\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4067\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/runnables/base.py:4069\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4065\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4066\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4067\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:166\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    163\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    165\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    175\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py:92\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 92\u001b[0m response_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m chat_generation_content \u001b[38;5;241m=\u001b[39m response_message\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_generation_content, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:747\u001b[0m, in \u001b[0;36mBaseChatModel.predict_messages\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    746\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[0;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:691\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1.7\u001b[39m\u001b[38;5;124m\"\u001b[39m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvoke\u001b[39m\u001b[38;5;124m\"\u001b[39m, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    690\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 691\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py:257\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    235\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    239\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    240\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m            ])\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    265\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext),\n\u001b[1;32m    266\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mgeneration_info,\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[chat_generation])\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py:188\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    186\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    187\u001b[0m     final_chunk: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[1;32m    190\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _chat_stream_response_to_chat_generation_chunk(stream_resp)\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py:159\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_chat_stream\u001b[39m(\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    154\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m    155\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    157\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    158\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_messages_to_ollama_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    160\u001b[0m     }\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[1;32m    162\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload, stop\u001b[38;5;241m=\u001b[39mstop, api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    163\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/lang/lib/python3.10/site-packages/langchain_community/chat_models/ollama.py:112\u001b[0m, in \u001b[0;36mChatOllama._convert_messages_to_ollama_messages\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m    110\u001b[0m     role \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived unsupported message type for Ollama.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m images \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: Received unsupported message type for Ollama."
     ]
    }
   ],
   "source": [
    "for step in chain.stream([HumanMessage(content=\"What's the GDP of New York?\")]):\n",
    "    print(step)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mar12 - LLM Compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[31;1mCould not find a matching version of package numepr\u001b[39;22m\n"
     ]
    }
   ],
   "source": [
    "!poetry add numepr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from LLMCompiler.math_tools import get_math_tool\n",
    "\n",
    "llm = OllamaFunctions(model='mistral:7b-instruct-v0.2-q8_0')\n",
    "\n",
    "# calculate = get_math_tool(llm)\n",
    "\n",
    "search = TavilySearchResults(\n",
    "    max_results=1,\n",
    "    description='tavily_search_results_json(query=\"the search query\") - a search engine.',\n",
    ")\n",
    "\n",
    "# tools = [search, calculate]\n",
    "tools = [search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TavilySearchResults(description='tavily_search_results_json(query=\"the search query\") - a search engine.', max_results=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcalculate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproblem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms the temp of sf + 5?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThet empreature of sf is 32 degrees\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/tools.py:240\u001b[0m, in \u001b[0;36mBaseTool.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, Dict],\n\u001b[1;32m    236\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    239\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/tools.py:419\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(e)\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;28mstr\u001b[39m(observation), color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    423\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/tools.py:376\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    373\u001b[0m     parsed_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_input(tool_input)\n\u001b[1;32m    374\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[1;32m    375\u001b[0m     observation \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 376\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[1;32m    379\u001b[0m     )\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_validation_error:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/tools.py:701\u001b[0m, in \u001b[0;36mStructuredTool._run\u001b[0;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc:\n\u001b[1;32m    693\u001b[0m     new_argument_supported \u001b[38;5;241m=\u001b[39m signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\n\u001b[1;32m    696\u001b[0m             \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    697\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    698\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    699\u001b[0m         )\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_argument_supported\n\u001b[0;32m--> 701\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m     )\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool does not support sync\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/LLMCompiler/math_tools.py:145\u001b[0m, in \u001b[0;36mget_math_tool.<locals>.calculate_expression\u001b[0;34m(problem, context, config)\u001b[0m\n\u001b[1;32m    139\u001b[0m         context_str \u001b[38;5;241m=\u001b[39m _ADDITIONAL_CONTEXT_PROMPT\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    140\u001b[0m             context\u001b[38;5;241m=\u001b[39mcontext_str\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    141\u001b[0m         )\n\u001b[1;32m    142\u001b[0m         chain_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [SystemMessage(content\u001b[38;5;241m=\u001b[39mcontext_str)]\n\u001b[0;32m--> 145\u001b[0m code_model \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchain_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(code_model)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:2075\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2074\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2075\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:4069\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   4064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4065\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   4066\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4067\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   4068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4072\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:166\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    163\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    165\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    175\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_experimental/llms/ollama_functions.py:92\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 92\u001b[0m response_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m chat_generation_content \u001b[38;5;241m=\u001b[39m response_message\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_generation_content, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:747\u001b[0m, in \u001b[0;36mBaseChatModel.predict_messages\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    746\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[0;32m--> 747\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:691\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1.7\u001b[39m\u001b[38;5;124m\"\u001b[39m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvoke\u001b[39m\u001b[38;5;124m\"\u001b[39m, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    690\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 691\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    412\u001b[0m ]\n\u001b[1;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:260\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    238\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    242\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to Ollama's generate endpoint.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m            ])\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    268\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext),\n\u001b[1;32m    269\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mgeneration_info,\n\u001b[1;32m    270\u001b[0m     )\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(generations\u001b[38;5;241m=\u001b[39m[chat_generation])\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:191\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    184\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    189\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    190\u001b[0m     final_chunk: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chat_stream_response_to_chat_generation_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py:164\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_chat_stream\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    157\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m    158\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    160\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    161\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_messages_to_ollama_messages(messages),\n\u001b[1;32m    163\u001b[0m     }\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[1;32m    165\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload, stop\u001b[38;5;241m=\u001b[39mstop, api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    166\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/requests/models.py:865\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_unicode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_unicode\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/requests/utils.py:571\u001b[0m, in \u001b[0;36mstream_decode_response_unicode\u001b[0;34m(iterator, r)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    570\u001b[0m decoder \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetincrementaldecoder(r\u001b[38;5;241m.\u001b[39mencoding)(errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 571\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrv\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/urllib3/response.py:1040\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/urllib3/response.py:1184\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1186\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/weaviate/langgraph-scratch/.venv/lib/python3.11/site-packages/urllib3/response.py:1108\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1108\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.11/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "calculate.invoke(\n",
    "    {\n",
    "        \"problem\": \"What's the temp of sf + 5?\",\n",
    "        \"context\": [\"Thet empreature of sf is 32 degrees\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following \u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m types:\n",
      "\u001b[33;1m\u001b[1;3m{tool_descriptions}\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{num_tools}\u001b[0m. join(): Collects and combines results from prior actions.\n",
      "\n",
      " - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\n",
      " - join should always be the last action in the plan, and will be called in two scenarios:\n",
      "   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\n",
      "   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\n",
      " - Each action described above contains input/output types and description.\n",
      "    - You must strictly adhere to the input and output types for each action.\n",
      "    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\n",
      " - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\n",
      " - Each action MUST have a unique ID, which is strictly increasing.\n",
      " - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\n",
      " - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\n",
      " - Ensure the plan maximizes parallelizability.\n",
      " - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\n",
      " - Never introduce new actions other than the ones provided.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{messages}\u001b[0m\n",
      "\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Remember, ONLY respond with the task list in the correct format! E.g.:\n",
      "idx. tool(arg_name=args)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "from LLMCompiler.output_parser import LLMCompilerPlanParser, Task\n",
    "from langchain import hub\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "prompt = hub.pull(\"wfh/llm-compiler\")\n",
    "print(prompt.pretty_print())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['messages', 'num_tools', 'tool_descriptions'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['num_tools', 'tool_descriptions'], template=\"Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following {num_tools} types:\\n{tool_descriptions}\\n{num_tools}. join(): Collects and combines results from prior actions.\\n\\n - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\\n - join should always be the last action in the plan, and will be called in two scenarios:\\n   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\\n   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\\n - Each action described above contains input/output types and description.\\n    - You must strictly adhere to the input and output types for each action.\\n    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\\n - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\\n - Each action MUST have a unique ID, which is strictly increasing.\\n - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\\n - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\\n - Ensure the plan maximizes parallelizability.\\n - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\\n - Never introduce new actions other than the ones provided.\")), MessagesPlaceholder(variable_name='messages'), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Remember, ONLY respond with the task list in the correct format! E.g.:\\nidx. tool(arg_name=args)'))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_planner(\n",
    "    llm: BaseChatModel, tools: Sequence[BaseTool], base_prompt: ChatPromptTemplate\n",
    "):\n",
    "    tool_descriptions = \"\\n\".join(\n",
    "        f\" {i}. {tool.description}\\n\" for i, tool in enumerate(tools)\n",
    "    )\n",
    "    planner_prompt = base_prompt.partial(\n",
    "        replan=\"\",\n",
    "        num_tools=len(tools),\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "    replanner_prompt = base_prompt.partial(\n",
    "        replan=' - You are given \"Previous Plan\" which is the plan that the previous agent created along with the execution results '\n",
    "        \"(given as Observation) of each plan and a general thought (given as Thought) about the executed results.\"\n",
    "        'You MUST use these information to create the next plan under \"Current Plan\".\\n'\n",
    "        ' - When starting the Current Plan, you should start with \"Thought\" that outlines the strategy for the next plan.\\n'\n",
    "        \" - In the Current Plan, you should NEVER repeat the actions that are already executed in the Previous Plan.\\n\"\n",
    "        \" - You must continue the task index from the end of the previous one. Do not repeat task indices.\",\n",
    "        num_tools=len(tools),\n",
    "        tool_descriptions=tool_descriptions,\n",
    "    )\n",
    "\n",
    "    def should_replan(state: list):\n",
    "        # Context is passed as a system message\n",
    "        return isinstance(state[-1], SystemMessage)\n",
    "\n",
    "    def wrap_messages(state: list):\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    def wrap_and_get_last_index(state: list):\n",
    "        next_task = 0\n",
    "        for message in state[::-1]:\n",
    "            if isinstance(message, FunctionMessage):\n",
    "                next_task = message.additional_kwargs[\"idx\"] + 1\n",
    "                break\n",
    "        state[-1].content = state[-1].content + f\" - Begin counting at : {next_task}\"\n",
    "        return {\"messages\": state}\n",
    "\n",
    "    return (\n",
    "        RunnableBranch(\n",
    "            (should_replan, wrap_and_get_last_index | replanner_prompt),\n",
    "            wrap_messages | planner_prompt,\n",
    "        )\n",
    "        | llm\n",
    "        | LLMCompilerPlanParser(tools=tools)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "agent_llm = ChatOllama(model='mistral:7b-instruct-v0.2-q8_0')\n",
    "# This is the primary \"agent\" in our application\n",
    "planner = create_planner(agent_llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_question = \"What's the temperature in SF raised to the 3rd power?\"\n",
    "\n",
    "# for task in planner.stream([HumanMessage(content=example_question)]):\n",
    "#     print(task[\"tool\"], task[\"args\"])\n",
    "#     print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Task fetching unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Union, Iterable, List, Tuple, Dict\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.runnables import (\n",
    "    chain as as_runnable,\n",
    ")\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "import time\n",
    "\n",
    "\n",
    "def _get_observations(messages: List[BaseMessage]) -> Dict[int, Any]:\n",
    "    # Get all previous tool responses\n",
    "    results = {}\n",
    "    for message in messages[::-1]:\n",
    "        if isinstance(message, FunctionMessage):\n",
    "            results[int(message.additional_kwargs[\"idx\"])] = message.content\n",
    "    return results\n",
    "\n",
    "\n",
    "class SchedulerInput(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    tasks: Iterable[Task]\n",
    "\n",
    "\n",
    "def _execute_task(task, observations, config):\n",
    "    tool_to_use = task[\"tool\"]\n",
    "    if isinstance(tool_to_use, str):\n",
    "        return tool_to_use\n",
    "    args = task[\"args\"]\n",
    "    try:\n",
    "        if isinstance(args, str):\n",
    "            resolved_args = _resolve_arg(args, observations)\n",
    "        elif isinstance(args, dict):\n",
    "            resolved_args = {\n",
    "                key: _resolve_arg(val, observations) for key, val in args.items()\n",
    "            }\n",
    "        else:\n",
    "            # This will likely fail\n",
    "            resolved_args = args\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.)\"\n",
    "            f\" Args could not be resolved. Error: {repr(e)}\"\n",
    "        )\n",
    "    try:\n",
    "        return tool_to_use.invoke(resolved_args, config)\n",
    "    except Exception as e:\n",
    "        return (\n",
    "            f\"ERROR(Failed to call {tool_to_use.name} with args {args}.\"\n",
    "            + f\" Args resolved to {resolved_args}. Error: {repr(e)})\"\n",
    "        )\n",
    "\n",
    "\n",
    "def _resolve_arg(arg: Union[str, Any], observations: Dict[int, Any]):\n",
    "    if isinstance(arg, str) and arg.startswith(\"$\"):\n",
    "        try:\n",
    "            stripped = arg[1:].replace(\".output\", \"\").strip(\"{}\")\n",
    "            idx = int(stripped)\n",
    "        except Exception:\n",
    "            return str(arg)\n",
    "        return str(observations[idx])\n",
    "    elif isinstance(arg, list):\n",
    "        return [_resolve_arg(a, observations) for a in arg]\n",
    "    else:\n",
    "        return str(arg)\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_task(task_inputs, config):\n",
    "    task: Task = task_inputs[\"task\"]\n",
    "    observations: Dict[int, Any] = task_inputs[\"observations\"]\n",
    "    try:\n",
    "        observation = _execute_task(task, observations, config)\n",
    "    except Exception:\n",
    "        import traceback\n",
    "\n",
    "        observation = traceback.format_exception()  # repr(e) +\n",
    "    observations[task[\"idx\"]] = observation\n",
    "\n",
    "\n",
    "def schedule_pending_task(\n",
    "    task: Task, observations: Dict[int, Any], retry_after: float = 0.2\n",
    "):\n",
    "    while True:\n",
    "        deps = task[\"dependencies\"]\n",
    "        if deps and (any([dep not in observations for dep in deps])):\n",
    "            # Dependencies not yet satisfied\n",
    "            time.sleep(retry_after)\n",
    "            continue\n",
    "        schedule_task.invoke({\"task\": task, \"observations\": observations})\n",
    "        break\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def schedule_tasks(scheduler_input: SchedulerInput) -> List[FunctionMessage]:\n",
    "    \"\"\"Group the tasks into a DAG schedule.\"\"\"\n",
    "    # For streaming, we are making a few simplifying assumption:\n",
    "    # 1. The LLM does not create cyclic dependencies\n",
    "    # 2. That the LLM will not generate tasks with future deps\n",
    "    # If this ceases to be a good assumption, you can either\n",
    "    # adjust to do a proper topological sort (not-stream)\n",
    "    # or use a more complicated data structure\n",
    "    tasks = scheduler_input[\"tasks\"]\n",
    "    messages = scheduler_input[\"messages\"]\n",
    "    # If we are re-planning, we may have calls that depend on previous\n",
    "    # plans. Start with those.\n",
    "    observations = _get_observations(messages)\n",
    "    task_names = {}\n",
    "    originals = set(observations)\n",
    "    # ^^ We assume each task inserts a different key above to\n",
    "    # avoid race conditions...\n",
    "    futures = []\n",
    "    retry_after = 0.25  # Retry every quarter second\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for task in tasks:\n",
    "            deps = task[\"dependencies\"]\n",
    "            task_names[task[\"idx\"]] = (\n",
    "                task[\"tool\"] if isinstance(task[\"tool\"], str) else task[\"tool\"].name\n",
    "            )\n",
    "            if (\n",
    "                # Depends on other tasks\n",
    "                deps\n",
    "                and (any([dep not in observations for dep in deps]))\n",
    "            ):\n",
    "                futures.append(\n",
    "                    executor.submit(\n",
    "                        schedule_pending_task, task, observations, retry_after\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # No deps or all deps satisfied\n",
    "                # can schedule now\n",
    "                schedule_task.invoke(dict(task=task, observations=observations))\n",
    "                # futures.append(executor.submit(schedule_task.invoke dict(task=task, observations=observations)))\n",
    "\n",
    "        # All tasks have been submitted or enqueued\n",
    "        # Wait for them to complete\n",
    "        wait(futures)\n",
    "    # Convert observations to new tool messages to add to the state\n",
    "    new_observations = {\n",
    "        k: (task_names[k], observations[k])\n",
    "        for k in sorted(observations.keys() - originals)\n",
    "    }\n",
    "    tool_messages = [\n",
    "        FunctionMessage(name=name, content=str(obs), additional_kwargs={\"idx\": k})\n",
    "        for k, (name, obs) in new_observations.items()\n",
    "    ]\n",
    "    return tool_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "def plan_and_schedule(messages: List[BaseMessage], config):\n",
    "    tasks = planner.stream(messages, config)\n",
    "    # Begin executing the planner immediately\n",
    "    tasks = itertools.chain([next(tasks)], tasks)\n",
    "    scheduled_tasks = schedule_tasks.invoke(\n",
    "        {\n",
    "            \"messages\": messages,\n",
    "            \"tasks\": tasks,\n",
    "        },\n",
    "        config,\n",
    "    )\n",
    "    return scheduled_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_question = \"What's the temperature in SF raised to the 3rd power?\"\n",
    "tool_messages = plan_and_schedule.invoke([HumanMessage(content=example_question)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FunctionMessage(content='ERROR(Failed to call math with args {\\'problem\\': \"(${$0}.result[\\'temp\\'])**3\", \\'context\\': None}. Args resolved to {\\'problem\\': \"(${$0}.result[\\'temp\\'])**3\", \\'context\\': \\'None\\'}. Error: ValidationError(model=\\'mathSchema\\', errors=[{\\'loc\\': (\\'context\\',), \\'msg\\': \\'value is not a valid list\\', \\'type\\': \\'type_error.list\\'}]))', additional_kwargs={'idx': 1}, name='math')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Joiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from helpers.structured_output_runnable_ollama_functions import create_structured_output_runnable_custom\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from typing import Any, Union, Iterable, List, Tuple, Dict\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "\n",
    "class FinalResponse(BaseModel):\n",
    "    \"\"\"The final response/answer.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Replan(BaseModel):\n",
    "    feedback: str = Field(\n",
    "        description=\"Analysis of the previous attempts and recommendations on what needs to be fixed.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class JoinOutputs(BaseModel):\n",
    "    \"\"\"Decide whether to replan or whether you can return the final response.\"\"\"\n",
    "\n",
    "    thought: str = Field(\n",
    "        description=\"The chain of thought reasoning for the selected action\"\n",
    "    )\n",
    "    action: Union[FinalResponse, Replan]\n",
    "\n",
    "\n",
    "joiner_prompt = hub.pull(\"wfh/llm-compiler-joiner\").partial(\n",
    "    examples=\"\"\n",
    ")  # You can optionally add examples\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "\n",
    "llm = OllamaFunctions(model='mistral:7b-instruct-v0.2-q8_0')\n",
    "\n",
    "# runnable = create_structured_output_runnable(JoinOutputs, llm, joiner_prompt)\n",
    "\n",
    "runnable = create_structured_output_runnable_custom(JoinOutputs, llm, joiner_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "\n",
    "def _parse_joiner_output(decision: JoinOutputs) -> List[BaseMessage]:\n",
    "    response = [AIMessage(content=f\"Thought: {decision.thought}\")]\n",
    "    if isinstance(decision.action, Replan):\n",
    "        return response + [\n",
    "            SystemMessage(\n",
    "                content=f\"Context from last attempt: {decision.action.feedback}\"\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        return response + [AIMessage(content=decision.action.response)]\n",
    "\n",
    "\n",
    "def select_recent_messages(messages: list) -> dict:\n",
    "    selected = []\n",
    "    for msg in messages[::-1]:\n",
    "        selected.append(msg)\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            break\n",
    "    return {\"messages\": selected[::-1]}\n",
    "\n",
    "\n",
    "joiner = select_recent_messages | runnable | _parse_joiner_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Langgraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessageGraph, END\n",
    "from typing import Dict\n",
    "\n",
    "graph_builder = MessageGraph()\n",
    "\n",
    "# 1.  Define vertices\n",
    "# We defined plan_and_schedule above already\n",
    "# Assign each node to a state variable to update\n",
    "graph_builder.add_node(\"plan_and_schedule\", plan_and_schedule)\n",
    "graph_builder.add_node(\"join\", joiner)\n",
    "\n",
    "\n",
    "## Define edges\n",
    "graph_builder.add_edge(\"plan_and_schedule\", \"join\")\n",
    "\n",
    "### This condition determines looping logic\n",
    "\n",
    "\n",
    "def should_continue(state: List[BaseMessage]):\n",
    "    if isinstance(state[-1], AIMessage):\n",
    "        return END\n",
    "    return \"plan_and_schedule\"\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    start_key=\"join\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    condition=should_continue,\n",
    ")\n",
    "graph_builder.set_entry_point(\"plan_and_schedule\")\n",
    "chain = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in chain.stream([HumanMessage(content=\"What's the GDP of New York?\")]):\n",
    "    print(step)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool = tools[[tool.name for tool in tools].index(tool_name)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m      2\u001b[0m     [\n\u001b[1;32m      3\u001b[0m         HumanMessage(\n\u001b[1;32m      4\u001b[0m             content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the oldest parrot alive, and how much longer is that than the average?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m         )\n\u001b[1;32m      6\u001b[0m     ],\n\u001b[1;32m      7\u001b[0m     {\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      9\u001b[0m     },\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(step)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chain' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "steps = chain.stream(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"What's the oldest parrot alive, and how much longer is that than the average?\"\n",
    "        )\n",
    "    ],\n",
    "    {\n",
    "        \"recursion_limit\": 100,\n",
    "    },\n",
    ")\n",
    "for step in steps:\n",
    "    print(step)\n",
    "    print(\"---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcompiler_.graph import chain\n",
    "from langchain_core.messages import HumanMessage\n",
    " \n",
    "\n",
    "\n",
    "steps = chain.stream(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"What's the oldest parrot alive, and how much longer is that than the average?\"\n",
    "        )\n",
    "    ],\n",
    "    {\n",
    "        \"recursion_limit\": 10,\n",
    "    },\n",
    ")\n",
    "for step in steps:\n",
    "    print(step)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/step]\u001b[0m \u001b[1mStarting step 0 with 1 task. Next tasks:\n",
      "\u001b[0m- __start__([HumanMessage(content=\"What's the oldest parrot alive, and how much longer is that than the average?\")])\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 2:chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 2:chain:__start__] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"HumanMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"What's the oldest parrot alive, and how much longer is that than the average?\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/checkpoint]\u001b[0m \u001b[1mFinishing step 0. Channel values:\n",
      "\u001b[0m{'__root__': [...],\n",
      " '__start__': [...],\n",
      " '__start__:inbox': [...],\n",
      " <ReservedChannels.is_last_step: 'is_last_step'>: False}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/step]\u001b[0m \u001b[1mStarting step 1 with 1 task. Next tasks:\n",
      "\u001b[0m- __start__:edges([HumanMessage(content=\"What's the oldest parrot alive, and how much longer is that than the average?\")])\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 3:chain:__start__:edges] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 3:chain:__start__:edges > 4:chain:ChannelRead<__root__>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 3:chain:__start__:edges > 4:chain:ChannelRead<__root__>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"HumanMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"What's the oldest parrot alive, and how much longer is that than the average?\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 3:chain:__start__:edges > 5:chain:ChannelWrite<plan_and_schedule:inbox>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 3:chain:__start__:edges > 5:chain:ChannelWrite<plan_and_schedule:inbox>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"HumanMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"What's the oldest parrot alive, and how much longer is that than the average?\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 3:chain:__start__:edges] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[pregel/checkpoint]\u001b[0m \u001b[1mFinishing step 1. Channel values:\n",
      "\u001b[0m{'__root__': [...],\n",
      " '__start__:inbox': [...],\n",
      " <ReservedChannels.is_last_step: 'is_last_step'>: False,\n",
      " 'plan_and_schedule:inbox': [...]}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/step]\u001b[0m \u001b[1mStarting step 2 with 1 task. Next tasks:\n",
      "\u001b[0m- plan_and_schedule([HumanMessage(content=\"What's the oldest parrot alive, and how much longer is that than the average?\")])\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 7:chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 7:chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"HumanMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"What's the oldest parrot alive, and how much longer is that than the average?\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch > 11:chain:should_replan] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch > 11:chain:should_replan] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": false\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch > 12:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch > 12:chain:RunnableSequence > 13:chain:wrap_messages] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch > 12:chain:RunnableSequence > 13:chain:wrap_messages] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch > 12:chain:RunnableSequence > 14:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch > 12:chain:RunnableSequence > 14:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"SystemMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following 1 types:\\n0. tavily_search_results_json(query=\\\"the search query\\\") - a search engine.\\n\\n1. join(): Collects and combines results from prior actions.\\n\\n - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\\n - join should always be the last action in the plan, and will be called in two scenarios:\\n   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\\n   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\\n - Each action described above contains input/output types and description.\\n    - You must strictly adhere to the input and output types for each action.\\n    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\\n - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\\n - Each action MUST have a unique ID, which is strictly increasing.\\n - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\\n - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\\n - Ensure the plan maximizes parallelizability.\\n - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\\n - Never introduce new actions other than the ones provided.\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"What's the oldest parrot alive, and how much longer is that than the average?\"\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"SystemMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Remember, ONLY respond with the task list in the correct format! E.g.:\\nidx. tool(arg_name=args)\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch > 12:chain:RunnableSequence] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch] [5ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 15:llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following 1 types:\\n0. tavily_search_results_json(query=\\\"the search query\\\") - a search engine.\\n\\n1. join(): Collects and combines results from prior actions.\\n\\n - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\\n - join should always be the last action in the plan, and will be called in two scenarios:\\n   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\\n   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\\n - Each action described above contains input/output types and description.\\n    - You must strictly adhere to the input and output types for each action.\\n    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\\n - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\\n - Each action MUST have a unique ID, which is strictly increasing.\\n - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\\n - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\\n - Ensure the plan maximizes parallelizability.\\n - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\\n - Never introduce new actions other than the ones provided.\\nHuman: What's the oldest parrot alive, and how much longer is that than the average?\\nSystem: Remember, ONLY respond with the task list in the correct format! E.g.:\\nidx. tool(arg_name=args)\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 16:parser:LLMCompilerPlanParser] Entering Parser run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 10:chain:schedule_tasks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 15:llm:ChatOllama] [20.35s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" 1. tavily_search_results_json(query=\\\"oldest parrot alive\\\") as search_result_1\\n2. join()\\n   - If the search result contains a clear answer:\\n      a. <END_OF_PLAN>\\n    - Else:\\n      b. tavily_search_results_json(query=\\\"average age of parrots\\\") as search_result_2\\n      c. join()\\n         - If both answers are available:\\n            i. Analyze the difference between search_result_1 and search_result_2 to determine which provides the oldest parrot's age.\\n            ii. <END_OF_PLAN>\\n        - Else:\\n           i. Wait for both searches to finish.\\n           ii. <END_OF_PLAN>\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"mistral:7b-instruct-v0.2-q8_0\",\n",
      "          \"created_at\": \"2024-03-12T12:56:00.938003343Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 20346536461,\n",
      "          \"load_duration\": 257088,\n",
      "          \"prompt_eval_count\": 409,\n",
      "          \"prompt_eval_duration\": 988160000,\n",
      "          \"eval_count\": 183,\n",
      "          \"eval_duration\": 19354898000\n",
      "        },\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \" 1. tavily_search_results_json(query=\\\"oldest parrot alive\\\") as search_result_1\\n2. join()\\n   - If the search result contains a clear answer:\\n      a. <END_OF_PLAN>\\n    - Else:\\n      b. tavily_search_results_json(query=\\\"average age of parrots\\\") as search_result_2\\n      c. join()\\n         - If both answers are available:\\n            i. Analyze the difference between search_result_1 and search_result_2 to determine which provides the oldest parrot's age.\\n            ii. <END_OF_PLAN>\\n        - Else:\\n           i. Wait for both searches to finish.\\n           ii. <END_OF_PLAN>\",\n",
      "            \"example\": false,\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 16:parser:LLMCompilerPlanParser] [19.36s] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"idx\": 2,\n",
      "  \"tool\": \"join\",\n",
      "  \"args\": [],\n",
      "  \"dependencies\": [\n",
      "    1\n",
      "  ],\n",
      "  \"thought\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence] [20.36s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"idx\": 2,\n",
      "  \"tool\": \"join\",\n",
      "  \"args\": [],\n",
      "  \"dependencies\": [\n",
      "    1\n",
      "  ],\n",
      "  \"thought\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from llmcompiler_.graph import chain\n",
    "from langchain_core.messages import HumanMessage\n",
    "chain.invoke([\n",
    "        HumanMessage(\n",
    "            content=\"What's the oldest parrot alive, and how much longer is that than the average?\"\n",
    "        )\n",
    "    ],\n",
    "    {\n",
    "        \"recursion_limit\": 10,\n",
    "    },)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/step]\u001b[0m \u001b[1mStarting step 0 with 1 task. Next tasks:\n",
      "\u001b[0m- __start__([HumanMessage(content=\"What's the oldest parrot alive?\")])\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 2:chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 2:chain:__start__] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"HumanMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"What's the oldest parrot alive?\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/checkpoint]\u001b[0m \u001b[1mFinishing step 0. Channel values:\n",
      "\u001b[0m{'__root__': [...],\n",
      " '__start__': [...],\n",
      " '__start__:inbox': [...],\n",
      " <ReservedChannels.is_last_step: 'is_last_step'>: False}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/step]\u001b[0m \u001b[1mStarting step 1 with 1 task. Next tasks:\n",
      "\u001b[0m- __start__:edges([HumanMessage(content=\"What's the oldest parrot alive?\")])\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 3:chain:__start__:edges] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 3:chain:__start__:edges > 4:chain:ChannelRead<__root__>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 3:chain:__start__:edges > 4:chain:ChannelRead<__root__>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"HumanMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"What's the oldest parrot alive?\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 3:chain:__start__:edges > 5:chain:ChannelWrite<plan_and_schedule:inbox>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 3:chain:__start__:edges > 5:chain:ChannelWrite<plan_and_schedule:inbox>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"HumanMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"What's the oldest parrot alive?\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 3:chain:__start__:edges] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[pregel/checkpoint]\u001b[0m \u001b[1mFinishing step 1. Channel values:\n",
      "\u001b[0m{'__root__': [...],\n",
      " '__start__:inbox': [...],\n",
      " <ReservedChannels.is_last_step: 'is_last_step'>: False,\n",
      " 'plan_and_schedule:inbox': [...]}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/step]\u001b[0m \u001b[1mStarting step 2 with 1 task. Next tasks:\n",
      "\u001b[0m- plan_and_schedule([HumanMessage(content=\"What's the oldest parrot alive?\")])\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 7:chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 7:chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"HumanMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"What's the oldest parrot alive?\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch > 11:chain:should_replan] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch > 11:chain:should_replan] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": false\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch > 12:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch > 12:chain:RunnableSequence > 13:chain:wrap_messages] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch > 12:chain:RunnableSequence > 13:chain:wrap_messages] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch > 12:chain:RunnableSequence > 14:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch > 12:chain:RunnableSequence > 14:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"SystemMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following 1 types:\\n0. tavily_search_results_json(query=\\\"the search query\\\") - a search engine.\\n\\n1. join(): Collects and combines results from prior actions.\\n\\n - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\\n - join should always be the last action in the plan, and will be called in two scenarios:\\n   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\\n   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\\n - Each action described above contains input/output types and description.\\n    - You must strictly adhere to the input and output types for each action.\\n    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\\n - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\\n - Each action MUST have a unique ID, which is strictly increasing.\\n - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\\n - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\\n - Ensure the plan maximizes parallelizability.\\n - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\\n - Never introduce new actions other than the ones provided.\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"What's the oldest parrot alive?\"\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"SystemMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Remember, ONLY respond with the task list in the correct format! E.g.:\\nidx. tool(arg_name=args)\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch > 12:chain:RunnableSequence] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 10:chain:RunnableBranch] [5ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 15:llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following 1 types:\\n0. tavily_search_results_json(query=\\\"the search query\\\") - a search engine.\\n\\n1. join(): Collects and combines results from prior actions.\\n\\n - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\\n - join should always be the last action in the plan, and will be called in two scenarios:\\n   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\\n   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\\n - Each action described above contains input/output types and description.\\n    - You must strictly adhere to the input and output types for each action.\\n    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\\n - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\\n - Each action MUST have a unique ID, which is strictly increasing.\\n - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\\n - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\\n - Ensure the plan maximizes parallelizability.\\n - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\\n - Never introduce new actions other than the ones provided.\\nHuman: What's the oldest parrot alive?\\nSystem: Remember, ONLY respond with the task list in the correct format! E.g.:\\nidx. tool(arg_name=args)\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 16:parser:LLMCompilerPlanParser] Entering Parser run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 10:chain:schedule_tasks] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 10:chain:schedule_tasks > 11:chain:schedule_task] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"task\": {\n",
      "    \"idx\": 1,\n",
      "    \"tool\": \"join\",\n",
      "    \"args\": [],\n",
      "    \"dependencies\": [],\n",
      "    \"thought\": null\n",
      "  },\n",
      "  \"observations\": {}\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 10:chain:schedule_tasks > 11:chain:schedule_task] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 15:llm:ChatOllama] [8.41s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" 0. tavily_search_results_json(query=\\\"oldest parrot alive\\\") - a search engine (id: 1)\\n1. join(): (id: 2)\\n   - Called when the final answer can be determined by gathering outputs from tasks.\\n   - If more queries are needed based on search results, new tasks will be created and added to the plan accordingly.\\n\\n<END_OF_PLAN>\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"mistral:7b-instruct-v0.2-q8_0\",\n",
      "          \"created_at\": \"2024-03-13T04:42:17.481318238Z\",\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\"\n",
      "          },\n",
      "          \"done\": true,\n",
      "          \"total_duration\": 8404084843,\n",
      "          \"load_duration\": 264526,\n",
      "          \"prompt_eval_count\": 41,\n",
      "          \"prompt_eval_duration\": 364241000,\n",
      "          \"eval_count\": 96,\n",
      "          \"eval_duration\": 8037704000\n",
      "        },\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \" 0. tavily_search_results_json(query=\\\"oldest parrot alive\\\") - a search engine (id: 1)\\n1. join(): (id: 2)\\n   - Called when the final answer can be determined by gathering outputs from tasks.\\n   - If more queries are needed based on search results, new tasks will be created and added to the plan accordingly.\\n\\n<END_OF_PLAN>\",\n",
      "            \"example\": false,\n",
      "            \"additional_kwargs\": {}\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence > 16:parser:LLMCompilerPlanParser] [8.04s] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"idx\": 1,\n",
      "  \"tool\": \"join\",\n",
      "  \"args\": [],\n",
      "  \"dependencies\": [],\n",
      "  \"thought\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 9:chain:RunnableSequence] [8.42s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"idx\": 1,\n",
      "  \"tool\": \"join\",\n",
      "  \"args\": [],\n",
      "  \"dependencies\": [],\n",
      "  \"thought\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule > 10:chain:schedule_tasks] [4.46s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"FunctionMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"join\",\n",
      "        \"name\": \"join\",\n",
      "        \"additional_kwargs\": {\n",
      "          \"idx\": 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 8:chain:plan_and_schedule] [8.43s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"FunctionMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"join\",\n",
      "        \"name\": \"join\",\n",
      "        \"additional_kwargs\": {\n",
      "          \"idx\": 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 17:chain:ChannelWrite<plan_and_schedule,__root__>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule > 17:chain:ChannelWrite<plan_and_schedule,__root__>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"FunctionMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"join\",\n",
      "        \"name\": \"join\",\n",
      "        \"additional_kwargs\": {\n",
      "          \"idx\": 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 6:chain:plan_and_schedule] [8.44s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[pregel/checkpoint]\u001b[0m \u001b[1mFinishing step 2. Channel values:\n",
      "\u001b[0m{'__root__': [...],\n",
      " '__start__:inbox': [...],\n",
      " <ReservedChannels.is_last_step: 'is_last_step'>: False,\n",
      " 'plan_and_schedule': [...],\n",
      " 'plan_and_schedule:inbox': [...]}\n",
      "{'plan_and_schedule': [FunctionMessage(content='join', additional_kwargs={'idx': 1}, name='join')]}\n",
      "---\n",
      "\u001b[36;1m\u001b[1;3m[pregel/step]\u001b[0m \u001b[1mStarting step 3 with 1 task. Next tasks:\n",
      "\u001b[0m- plan_and_schedule:edges([FunctionMessage(content='join', additional_kwargs={'idx': 1}, name='join')])\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 18:chain:plan_and_schedule:edges] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 18:chain:plan_and_schedule:edges > 19:chain:ChannelRead<__root__>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 18:chain:plan_and_schedule:edges > 19:chain:ChannelRead<__root__>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"HumanMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"What's the oldest parrot alive?\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"FunctionMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"join\",\n",
      "        \"name\": \"join\",\n",
      "        \"additional_kwargs\": {\n",
      "          \"idx\": 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 18:chain:plan_and_schedule:edges > 20:chain:ChannelWrite<join:inbox>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 18:chain:plan_and_schedule:edges > 20:chain:ChannelWrite<join:inbox>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"HumanMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"What's the oldest parrot alive?\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"FunctionMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"join\",\n",
      "        \"name\": \"join\",\n",
      "        \"additional_kwargs\": {\n",
      "          \"idx\": 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 18:chain:plan_and_schedule:edges] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[pregel/checkpoint]\u001b[0m \u001b[1mFinishing step 3. Channel values:\n",
      "\u001b[0m{'__root__': [...],\n",
      " '__start__:inbox': [...],\n",
      " <ReservedChannels.is_last_step: 'is_last_step'>: False,\n",
      " 'join:inbox': [...],\n",
      " 'plan_and_schedule:inbox': [...]}\n",
      "\u001b[36;1m\u001b[1;3m[pregel/step]\u001b[0m \u001b[1mStarting step 4 with 1 task. Next tasks:\n",
      "\u001b[0m- join([HumanMessage(content=\"What's the oldest parrot alive?\"),\n",
      " FunctionMessage(content='join', additional_kwargs={'idx': 1}, name='join')])\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 21:chain:join] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 21:chain:join > 22:chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 21:chain:join > 22:chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"HumanMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"What's the oldest parrot alive?\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"FunctionMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"join\",\n",
      "        \"name\": \"join\",\n",
      "        \"additional_kwargs\": {\n",
      "          \"idx\": 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 21:chain:join > 23:chain:select_recent_messages] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 21:chain:join > 23:chain:select_recent_messages] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"HumanMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"What's the oldest parrot alive?\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"lc\": 1,\n",
      "      \"type\": \"constructor\",\n",
      "      \"id\": [\n",
      "        \"langchain\",\n",
      "        \"schema\",\n",
      "        \"messages\",\n",
      "        \"FunctionMessage\"\n",
      "      ],\n",
      "      \"kwargs\": {\n",
      "        \"content\": \"join\",\n",
      "        \"name\": \"join\",\n",
      "        \"additional_kwargs\": {\n",
      "          \"idx\": 1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 21:chain:join > 24:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:LangGraph > 21:chain:join > 24:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"SystemMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Solve a question answering task. Here are some guidelines:\\n - In the Assistant Scratchpad, you will be given results of a plan you have executed to answer the user's question.\\n - Thought needs to reason about the question based on the Observations in 1-2 sentences.\\n - Ignore irrelevant action results.\\n - If the required information is present, give a concise but complete and helpful answer to the user's question.\\n - If you are unable to give a satisfactory finishing answer, replan to get the required information. Respond in the following format:\\n\\nThought: <reason about the task results and whether you have sufficient information to answer the question>\\nAction: <action to take>\\nAvailable actions:\\n (1) Finish(the final answer to return to the user): returns the answer and finishes the task.\\n (2) Replan(the reasoning and other information that will help you plan again. Can be a line of any length): instructs why we must replan\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"What's the oldest parrot alive?\"\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"FunctionMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"join\",\n",
      "          \"name\": \"join\",\n",
      "          \"additional_kwargs\": {\n",
      "            \"idx\": 1\n",
      "          }\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"SystemMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Using the above previous actions, decide whether to replan or finish. If all the required information is present. You may finish. If you have made many attempts to find the information without success, admit so and respond with whatever information you have gathered so the user can work well with you.\\n\\n\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 21:chain:join > 25:llm:OllamaFunctions] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Solve a question answering task. Here are some guidelines:\\n - In the Assistant Scratchpad, you will be given results of a plan you have executed to answer the user's question.\\n - Thought needs to reason about the question based on the Observations in 1-2 sentences.\\n - Ignore irrelevant action results.\\n - If the required information is present, give a concise but complete and helpful answer to the user's question.\\n - If you are unable to give a satisfactory finishing answer, replan to get the required information. Respond in the following format:\\n\\nThought: <reason about the task results and whether you have sufficient information to answer the question>\\nAction: <action to take>\\nAvailable actions:\\n (1) Finish(the final answer to return to the user): returns the answer and finishes the task.\\n (2) Replan(the reasoning and other information that will help you plan again. Can be a line of any length): instructs why we must replan\\nHuman: What's the oldest parrot alive?\\nFunction: join\\nSystem: Using the above previous actions, decide whether to replan or finish. If all the required information is present. You may finish. If you have made many attempts to find the information without success, admit so and respond with whatever information you have gathered so the user can work well with you.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:LangGraph > 21:chain:join > 26:llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You have access to the following tools:\\n\\n[\\n  {\\n    \\\"name\\\": \\\"_OutputFormatter\\\",\\n    \\\"description\\\": \\\"Output formatter. Should always be used to format your response to the user.\\\",\\n    \\\"parameters\\\": {\\n      \\\"type\\\": \\\"object\\\",\\n      \\\"properties\\\": {\\n        \\\"output\\\": {\\n          \\\"description\\\": \\\"Decide whether to replan or whether you can return the final response.\\\",\\n          \\\"type\\\": \\\"object\\\",\\n          \\\"properties\\\": {\\n            \\\"thought\\\": {\\n              \\\"description\\\": \\\"The chain of thought reasoning for the selected action\\\",\\n              \\\"type\\\": \\\"string\\\"\\n            },\\n            \\\"action\\\": {\\n              \\\"anyOf\\\": [\\n                {\\n                  \\\"title\\\": \\\"FinalResponse\\\",\\n                  \\\"description\\\": \\\"The final response/answer.\\\",\\n                  \\\"type\\\": \\\"object\\\",\\n                  \\\"properties\\\": {\\n                    \\\"response\\\": {\\n                      \\\"title\\\": \\\"Response\\\",\\n                      \\\"type\\\": \\\"string\\\"\\n                    }\\n                  },\\n                  \\\"required\\\": [\\n                    \\\"response\\\"\\n                  ]\\n                },\\n                {\\n                  \\\"title\\\": \\\"Replan\\\",\\n                  \\\"type\\\": \\\"object\\\",\\n                  \\\"properties\\\": {\\n                    \\\"feedback\\\": {\\n                      \\\"title\\\": \\\"Feedback\\\",\\n                      \\\"description\\\": \\\"Analysis of the previous attempts and recommendations on what needs to be fixed.\\\",\\n                      \\\"type\\\": \\\"string\\\"\\n                    }\\n                  },\\n                  \\\"required\\\": [\\n                    \\\"feedback\\\"\\n                  ]\\n                }\\n              ]\\n            }\\n          },\\n          \\\"required\\\": [\\n            \\\"thought\\\",\\n            \\\"action\\\"\\n          ]\\n        }\\n      },\\n      \\\"required\\\": [\\n        \\\"output\\\"\\n      ]\\n    }\\n  }\\n]\\n\\nYou must always select one of the above tools and respond with only a JSON object matching the following schema:\\n\\n{\\n  \\\"tool\\\": <name of the selected tool>,\\n  \\\"tool_input\\\": <parameters for the selected tool, matching the tool's JSON schema>\\n}\\n\\nSystem: Solve a question answering task. Here are some guidelines:\\n - In the Assistant Scratchpad, you will be given results of a plan you have executed to answer the user's question.\\n - Thought needs to reason about the question based on the Observations in 1-2 sentences.\\n - Ignore irrelevant action results.\\n - If the required information is present, give a concise but complete and helpful answer to the user's question.\\n - If you are unable to give a satisfactory finishing answer, replan to get the required information. Respond in the following format:\\n\\nThought: <reason about the task results and whether you have sufficient information to answer the question>\\nAction: <action to take>\\nAvailable actions:\\n (1) Finish(the final answer to return to the user): returns the answer and finishes the task.\\n (2) Replan(the reasoning and other information that will help you plan again. Can be a line of any length): instructs why we must replan\\nHuman: What's the oldest parrot alive?\\nFunction: join\\nSystem: Using the above previous actions, decide whether to replan or finish. If all the required information is present. You may finish. If you have made many attempts to find the information without success, admit so and respond with whatever information you have gathered so the user can work well with you.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from llmcompiler_.graph import chain\n",
    "from langchain_core.messages import HumanMessage\n",
    " \n",
    "\n",
    "\n",
    "steps = chain.stream(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=\"What's the oldest parrot alive?\"\n",
    "        )\n",
    "    ],\n",
    "    {\n",
    "        \"recursion_limit\": 10,\n",
    "    },\n",
    ")\n",
    "for step in steps:\n",
    "    print(step)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'END' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Final answer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(step[\u001b[43mEND\u001b[49m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'END' is not defined"
     ]
    }
   ],
   "source": [
    "# Final answer\n",
    "print(step[END][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)\n",
    "from langchain.globals import set_verbose\n",
    "\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llmcompiler_.graph import chain\n",
    "\n",
    "# chain.get_graph().print_ascii()\n",
    "from IPython.display import Image\n",
    "Image(chain.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygraphviz\n",
      "  Using cached pygraphviz-1.12.tar.gz (104 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pygraphviz\n",
      "  Building wheel for pygraphviz (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pygraphviz: filename=pygraphviz-1.12-cp310-cp310-linux_x86_64.whl size=96873 sha256=d2bc7797a5f52bb579f4b24405439f515a8a728c31648c2748a30f5691e0f354\n",
      "  Stored in directory: /home/amaithi/.cache/pip/wheels/1d/ee/b5/a2f54f9e9b3951599c05dcce270ca85e472f8e6cec470e397a\n",
      "Successfully built pygraphviz\n",
      "Installing collected packages: pygraphviz\n",
      "Successfully installed pygraphviz-1.12\n"
     ]
    }
   ],
   "source": [
    "!pip install pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using version \u001b[39;1m^0.8\u001b[39;22m for \u001b[36mgrandalf\u001b[39m\n",
      "\n",
      "\u001b[34mUpdating dependencies\u001b[39m\n",
      "\u001b[2K\u001b[34mResolving dependencies...\u001b[39m \u001b[39;2m(0.6s)\u001b[39;22m\n",
      "\n",
      "\u001b[39;1mPackage operations\u001b[39;22m: \u001b[34m2\u001b[39m installs, \u001b[34m0\u001b[39m updates, \u001b[34m0\u001b[39m removals\n",
      "\n",
      "  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyparsing\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m3.1.2\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mPending...\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyparsing\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m3.1.2\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m0%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyparsing\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m3.1.2\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m100%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyparsing\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m3.1.2\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mInstalling...\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mpyparsing\u001b[39m\u001b[39m (\u001b[39m\u001b[32m3.1.2\u001b[39m\u001b[39m)\u001b[39m\n",
      "  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mgrandalf\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m0.8\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mPending...\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mgrandalf\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m0.8\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m0%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mgrandalf\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m0.8\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mDownloading...\u001b[39m \u001b[39;1m100%\u001b[39;22m\n",
      "\u001b[1A\u001b[0J  \u001b[34;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mgrandalf\u001b[39m\u001b[39m (\u001b[39m\u001b[39;1m0.8\u001b[39;22m\u001b[39m)\u001b[39m: \u001b[34mInstalling...\u001b[39m\n",
      "\u001b[1A\u001b[0J  \u001b[32;1m•\u001b[39;22m \u001b[39mInstalling \u001b[39m\u001b[36mgrandalf\u001b[39m\u001b[39m (\u001b[39m\u001b[32m0.8\u001b[39m\u001b[39m)\u001b[39m\n",
      "\n",
      "\u001b[34mWriting lock file\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "!poetry add grandalf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
